[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 412: Bayesian Statistics",
    "section": "",
    "text": "Welcome to the website for Middlebury College’s Fall 2025 STAT 412. On this website you will find the course syllabus, schedule, and assignments. The website is frequently updated throughout the semester, so please make a habit of refreshing the page."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#announcements",
    "href": "index.html#announcements",
    "title": "STAT 412: Bayesian Statistics",
    "section": "Announcements",
    "text": "Announcements\n\nPlease ensure R Studio functions on your laptop"
  },
  {
    "objectID": "index.html#course-details",
    "href": "index.html#course-details",
    "title": "STAT 412: Bayesian Statistics",
    "section": "Course details",
    "text": "Course details\nInstructor: Becky Tang (she/her)\n\nOffice: WNS 214\nEmail: btang@middlebury.edu\n\nMeeting times and location: Mondays and Wednesdays 9:45-11am in WNS 011\nOffice hours: Monday 3-4pm, Tuesday 3:30-4:30pm, Friday 11am-12pm\nSyllabus (most recent update 9/1/2025): our syllabus outlines our course policies and a rough schedule. Once classes begin, you should follow the schedule and due dates found on this website."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This page is frequently updated!\n\n\n\nWeek\nDate\nTopic\nAssignments\n\n\n\n\n1\nM 9/08\n\nWelcome!\nWhat is a statistical model?\n\n\nProblem Set 0 (due next class 9/10)\n\n\n\n\nW 9/10\n\nBayes’ rule, Bayes modeling\n\nLikelihood, prior, posterior\nCode and worksheet\n\n\n\nProblem Set 1 (due next class 9/15)\n\n\n\n2\nM 9/15\n\nBeta-Binomial model\n\nProportionality\n\nPosterior predictive distribution\n\n\nProblem Set 2 (due 9/24)\n\n\n\n\nW 9/17\n\nBayes estimators/estimates\n\nSome proofs\n\n\n\nProblem Set 2 (due 9/24)\n\n\n\n3\nM 9/22\n\nConfidence regions\nPosterior quantities and posterior predictive checks via simulation\n\nCode\n\n\n\n\n\n\nW 9/24\n\nConjugacy\nNormal sampling model\n\n\nProblem Set 3 (complete!)\nCode for HW 3\n\n\n\n4\nM 9/29\n\nMarkov chain basics\n\n\n\n\n\nW 10/01\n\nNormal model (cont.) with Gibbs sampling!\n\nData under Handouts\nCode from class\n\n\n\nFor Monday, read the article Explaining the Gibbs Sampler under Handouts and prepare to discuss!\nProblem Set 4\n\n\n\n5\nM 10/06\n\nDiscuss Explaining the Gibbs Sampler\nMCMC diagnostics\n\n\n\n\n\nW 10/06\n\nIntroduce Case Study 1\nMSE\n\n\nWork on Case Study 1"
  },
  {
    "objectID": "handouts.html",
    "href": "handouts.html",
    "title": "Handouts",
    "section": "",
    "text": "Distribution sheet\nExplaining the Gibbs Sampler (1992)\n\nDiscussion questions posted here!\n\nConstrained normal sampling code\nData\n\n prussianHorses1 \n prussianHorses2 \n school1  Load code in as readRDS(\"filepath/school1.Rda\")\n divorce  Load code in as readRDS(\"filepath/divorce.Rda\")"
  },
  {
    "objectID": "code/binom_discrete_prior.html",
    "href": "code/binom_discrete_prior.html",
    "title": "Learning about a Binomial Probability",
    "section": "",
    "text": "library(tidyverse)\nknitr::opts_chunk$set(fig.width = 6, fig.height = 3)"
  },
  {
    "objectID": "code/binom_discrete_prior.html#tidyverse-code",
    "href": "code/binom_discrete_prior.html#tidyverse-code",
    "title": "Learning about a Binomial Probability",
    "section": "Tidyverse code",
    "text": "Tidyverse code\n\n# PRIOR\ntheta &lt;- seq(0.1, 0.9, by = 0.1)\nprior1 &lt;- rep(1/9, 9)\nprior2 &lt;- c(0.05, 0.05, 0.05, 0.2, 0.3, 0.2, 0.05, 0.05, 0.05)\n\n# create data frame for visualization and wrangling\nbayes_table &lt;- data.frame(theta, prior1, prior2) \n\n# visualize different priors\nbayes_table |&gt;\n  pivot_longer(cols = 2:3, names_to = \"prior\", values_to = \"prior_probs\") |&gt;\n  mutate(theta = factor(theta)) |&gt; # for prettier visualization\n  ggplot(aes(x = theta, y = prior_probs)) + \n  geom_col() +\n  facet_wrap(~ prior) +\n  labs(x = expression(theta),\n      y = expression(f(theta)),\n      title = \"Prior\")\n\n\n\n\n\n\n\n# LIKELIHOOD\ny &lt;- 4\n## since R is vectorized:\nlike &lt;- dbinom(y, size = 11, prob = theta)\n## equivalently\n## like &lt;- choose(11, y) * theta^y * (1-theta)^(11-y)\nbayes_table &lt;- bayes_table |&gt;\n  add_column(likelihood = like) \n\n\n# MARGINAL LIKELIHOOD\nbayes_table &lt;- bayes_table |&gt;\n  mutate(product = prior2 * likelihood)\nmarg_like &lt;- sum(bayes_table$product)\n\n# POSTERIOR\nbayes_table &lt;- bayes_table |&gt;\n  mutate(posterior = product / marg_like)\nbayes_table |&gt;\n  select(theta, posterior)\n\n  theta      posterior\n1   0.1 0.006168515787\n2   0.2 0.043274594401\n3   0.3 0.086030889543\n4   0.4 0.369693507640\n5   0.5 0.377836937562\n6   0.6 0.109538817078\n7   0.7 0.006772110839\n8   0.8 0.000676165538\n9   0.9 0.000008461613\n\n# visualize prior vs posterior\n# note: could also plot in two separate plots without pivoting\nbayes_table |&gt;\n  pivot_longer(cols = c(\"prior1\", \"posterior\"), names_to = \"dist\", values_to = \"probs\") |&gt;\n  mutate(theta = factor(theta),\n         dist = factor(dist, levels = c(\"prior1\", \"posterior\"))) |&gt; # for prettier visualization\n  ggplot(aes(x = theta, y = probs)) + \n  geom_col() +\n  facet_wrap(~dist) +\n  labs(x = expression(theta),\n      y = \"Probability\",\n      title = expression(\"Comparison of \" * f(theta) * \" and \" * f(theta * \"| y\")))"
  },
  {
    "objectID": "code/binom_discrete_prior.html#base-r-code",
    "href": "code/binom_discrete_prior.html#base-r-code",
    "title": "Learning about a Binomial Probability",
    "section": "Base R code",
    "text": "Base R code\n\n## PRIOR\ntheta &lt;- seq(0.1, 0.9, by = 0.1)\nprior1 &lt;- rep(1/9, 9)\nprior2 &lt;- c(0.05, 0.05, 0.05, 0.2, 0.3, 0.2, 0.05, 0.05, 0.05)\n\nbayes_table &lt;- data.frame(theta, prior1, prior2)\n\n# Visualize prior 1\nbarplot(bayes_table$prior1, names.arg = bayes_table$theta,\n        xlab = expression(theta),\n        ylab = expression(f(theta)),\n        ylim = c(0, max(prior1) + 0.05),\n        main = \"Prior\")\n\n\n\n\n\n\n\n# LIKELIHOOD\ny &lt;- 4\nlike &lt;- dbinom(y, size = 11, prob = theta)\nbayes_table$likelihood &lt;- like\n\n# MARGINAL LIKELIHOOD\nbayes_table$product &lt;- prior1 * like\nmarg_like &lt;- sum(bayes_table$product)\n\n# POSTERIOR\nbayes_table$posterior &lt;-  bayes_table$product / marg_like\n\n# vizualize prior and posterior\npar(mfrow = c(1,2))\ny_max &lt;- max(c(prior1, bayes_table$posterior))\nbarplot(bayes_table$prior1, names.arg = bayes_table$theta,\n        xlab = expression(theta),\n        ylab = expression(f(theta)),\n        ylim = c(0, y_max + 0.05),\n        main = \"Prior\")\nbarplot(bayes_table$posterior, \n        names.arg = theta,\n        beside = T,\n        xlab = expression(theta),\n        ylab = expression(f(theta * \"| y\")),\n        ylim = c(0, y_max + 0.05),\n        main = \"Posterior\")"
  },
  {
    "objectID": "code/binom_discrete_prior.html#if-you-didnt-want-to-plot",
    "href": "code/binom_discrete_prior.html#if-you-didnt-want-to-plot",
    "title": "Learning about a Binomial Probability",
    "section": "If you didn’t want to plot",
    "text": "If you didn’t want to plot\n\nmarg_like &lt;- sum(prior1 * like)\npost &lt;- (prior1 * like)/marg_like\ncbind(theta, post)\n\n      theta          post\n [1,]   0.1 0.01893857939\n [2,]   0.2 0.13286167531\n [3,]   0.3 0.26413206805\n [4,]   0.4 0.28375828506\n [5,]   0.5 0.19333918362\n [6,]   0.6 0.08407652891\n [7,]   0.7 0.02079173713\n [8,]   0.8 0.00207596368\n [9,]   0.9 0.00002597885"
  },
  {
    "objectID": "code/proportionality.html",
    "href": "code/proportionality.html",
    "title": "Prior, likelihood, posterior, proportionality",
    "section": "",
    "text": "We often say the posterior distribution is “proportional to likelihood times prior”, where proportionality is with respect to \\(\\theta\\):\n\\[\\begin{align*}\nf_{\\theta | y}(\\theta | y) &= \\frac{f_{y | \\theta} (y | \\theta) f_{\\theta}(\\theta)}{f_{Y}(y)} \\\\\n& \\overset{\\theta}{\\propto} f_{y | \\theta} (y | \\theta) f_{\\theta}(\\theta)\n\\end{align*}\\]\nThat is, the posterior distribution is a function of \\(\\theta\\), so the marginal likelihood \\(f_{Y}(y)\\) is a constant with respect to \\(\\theta\\). It is just a scaling factor, as illustrated here:"
  },
  {
    "objectID": "handouts/bayes_est.html#theorem",
    "href": "handouts/bayes_est.html#theorem",
    "title": "Bayes estimator proofs",
    "section": "Theorem",
    "text": "Theorem\nUnder squared loss \\(L(\\theta, a) = (\\theta - a)^2\\), the Bayes estimator for \\(\\theta\\) is the posterior mean of \\(\\theta\\), \\(\\mathbb{E}[\\theta | \\mathbf{y}]\\).\n\nWant to show that \\(\\mathbb{E}[\\theta | \\mathbf{y}] = \\underset{a}{\\text{argmin}} \\ \\mathbb{E}[(\\theta - a)^2 | \\mathbf{y}]\\)\nRecall: \\(\\mathbb{E}[\\theta | \\mathbf{y}] = \\int_{\\Theta} \\theta f(\\theta | \\mathbf{y}) d\\theta\\) is a function of \\(\\mathbf{y}\\)! We’ve integrated out \\(\\theta\\)!\nAlso, for any function \\(g\\), \\(\\mathbb{E}[g(\\mathbf{y}) | \\mathbf{y}] = g(\\mathbf{y})\\).\n\nThat is, since we condition on \\(\\mathbf{y}\\) (not random), the expectation of any function of \\(\\mathbf{y}\\) is itself"
  },
  {
    "objectID": "handouts/bayes_est.html#proof-set-up",
    "href": "handouts/bayes_est.html#proof-set-up",
    "title": "Bayes estimator proofs",
    "section": "Proof set-up",
    "text": "Proof set-up\n\nWant to show that \\(m = \\underset{a}{\\text{argmin}} \\ \\mathbb{E}[ |\\theta - a| | \\mathbf{y}]\\)\nNote that the absolute value function is not differentiable, so proving a maximum/minimum cannot rely on derivatives!\nAssume that \\(\\theta\\) is continuous, so that if \\(m\\) is the posterior median, then \\(\\text{Pr}(\\theta \\geq m | \\mathbf{y}) = \\frac{1}{2} = \\text{Pr}(\\theta \\leq m | \\mathbf{y})\\).\nLet \\(a\\) be any other estimator of \\(\\theta\\).\nWe will show that\n\\[\n\\mathbb{E}[L(\\theta , a) | \\mathbf{y}] - \\mathbb{E}[L(\\theta, m) | \\mathbf{y}]  \\geq 0\n\\]\nthus demonstrating that \\(m\\) minimizes the expected loss.\nSome recap from probability: if \\(Y\\) continuous with pdf \\(f\\),\n\n\\(\\text{Pr}(Y \\leq a) = \\int_{-\\infty}^{a} f(y) dy\\)\n\\(\\text{Pr}(a \\leq Y \\leq b) = \\text{Pr}(Y\\leq b) - \\text{Pr}(Y \\leq a)\\) and these inequalities could be \\(\\leq\\) or \\(&lt;\\)"
  },
  {
    "objectID": "handouts/bayes_est.html#proof",
    "href": "handouts/bayes_est.html#proof",
    "title": "Bayes estimator proofs",
    "section": "Proof",
    "text": "Proof\nLet \\(m\\) be the posterior median, and suppose \\(a &lt; m\\) is any other estimator. Remember, \\(a\\) and \\(m\\) are constant w.r.t. \\(\\theta\\).\n\\[\n\\begin{align}\n\\mathbb{E}[L(\\theta , a) | \\mathbf{y}] & - \\mathbb{E}[L(\\theta, m) | \\mathbf{y}] = \\int_{\\Theta} |\\theta - a| f(\\theta | \\mathbf{y}) d\\theta - \\int_{\\Theta} |\\theta - m| f(\\theta | \\mathbf{y})d\\theta \\\\\n&\\class{fragment}{= \\int_{\\Theta} \\left(|\\theta - a| - |\\theta - m|\\right) f(\\theta | \\mathbf{y})d\\theta } \\\\\n&\\class{fragment}{= \\int_{-\\infty}^{a} \\left(|\\theta - a| - |\\theta - m|\\right) f(\\theta | \\mathbf{y})d\\theta + \\int_{a}^{m} \\left(|\\theta - a| - |\\theta - m|\\right) f(\\theta | \\mathbf{y})d\\theta + \\int_{m}^{\\infty} \\left(|\\theta - a| - |\\theta - m|\\right) f(\\theta | \\mathbf{y})d\\theta }\\\\\n& \\class{fragment}{= \\int_{-\\infty}^{a} ((a-\\theta) - ( m - \\theta)) f(\\theta | \\mathbf{y}) d\\theta + \\int_{a}^{m} ((\\theta - a) - (m- \\theta)) f(\\theta | \\mathbf{y}) d\\theta  +\n\\int_{m}^{\\infty} ((\\theta - a) - (\\theta - m)) f(\\theta | \\mathbf{y}) d\\theta} \\\\\n&\\class{fragment}{= \\int_{-\\infty}^{a} (a - m)  f(\\theta | \\mathbf{y}) d\\theta + \\int_{a}^{m} (2\\theta - a- m)  f(\\theta | \\mathbf{y}) d\\theta + \\int_{m}^{\\infty} (m-a) f(\\theta | \\mathbf{y}) d\\theta} \\\\\n&\\class{fragment}{\\color{orange}{\\geq}  \\int_{-\\infty}^{a} (a - m)  f(\\theta | \\mathbf{y}) d\\theta + \\int_{a}^{m} (2\\color{orange}{a} - a- m)  f(\\theta | \\mathbf{y}) d\\theta + \\int_{m}^{\\infty} (m-a) f(\\theta | \\mathbf{y}) d\\theta} \\\\\n&\\class{fragment}{= (a-m)\\text{Pr}(\\theta \\leq a | \\mathbf{y}) + \\color{purple}{(a-m)\\text{Pr}(a  &lt; \\theta \\leq m | \\mathbf{y}) }+ (m-a) \\text{Pr}(\\theta \\geq m | \\mathbf{y})} \\\\\n&\\class{fragment}{ = (a-m)\\text{Pr}(\\theta \\leq a | \\mathbf{y}) + \\color{purple}{(a-m) \\text{Pr}(\\theta \\leq m | \\mathbf{y})  - (a-m) \\text{Pr}(\\theta \\leq a | \\mathbf{y})} + (m-a) \\text{Pr}(\\theta \\geq m | \\mathbf{y}) } \\\\\n&\\class{fragment}{= (a-m) \\text{Pr}(\\theta \\leq m | \\mathbf{y}) +  (m-a) \\text{Pr}(\\theta \\geq m | \\mathbf{y})} \\\\\n&\\class{fragment}{= (a-m)\\left(\\frac{1}{2}\\right) + (m-a)\\left(\\frac{1}{2}\\right)} \\\\\n&\\class{fragment}{= (a-m)\\left(\\frac{1}{2}\\right) - (a-m)\\left(\\frac{1}{2}\\right)}\\\\\n&\\class{fragment}{= 0 \\qquad \\tiny{\\square} }\n\\end{align}\n\\]"
  },
  {
    "objectID": "handouts/bayes_est.html#proof-2",
    "href": "handouts/bayes_est.html#proof-2",
    "title": "Bayes estimator proofs",
    "section": "Proof 2",
    "text": "Proof 2\nApologies; you’ll have to do several “right clicks” until you reach the next slide.\nLet’s begin by manipulating the term we’d like to minimize: \\[\\begin{align}\n\\mathbb{E}[(\\theta - a)^2 | \\mathbf{y}] &= \\mathbb{E}[( \\color{red}{(} \\theta - \\mathbb{E}[\\theta | \\mathbf{y}]\\color{red}{)} + \\color{red}{(}\\mathbb{E}[\\theta | \\mathbf{y}]  - a\\color{red}{)})^2 | \\mathbf{y}] \\\\\n&\\class{fragment}{\\overset{\\text{FOIL}}{=} \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}] + 2\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])(\\mathbb{E}[\\theta | \\mathbf{y}]-a) | \\mathbf{y}] + \\mathbb{E}[(\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2 | \\mathbf{y}]} \\\\\n&\\class{fragment}{=\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}] + 2\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])\\color{orange}{(\\mathbb{E}[\\theta | \\mathbf{y}]-a)} | \\mathbf{y}] + \\mathbb{E}[\\color{orange}{(\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2} | \\mathbf{y}]} \\\\\n& \\class{fragment}{\\text{Note: }  \\color{orange}{\\mathbb{E}[\\theta | \\mathbf{y}]} \\text{ and } \\color{orange}{a} \\text{ are constant w.r.t } \\mathbf{y}}  \\\\\n&\\class{fragment}{=\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}]  + 2 \\color{orange}{(\\mathbb{E}[\\theta | \\mathbf{y}]-a)}\\color{purple}{\\mathbb{E}[\\theta - \\mathbb{E}[\\theta | \\mathbf{y}] | \\mathbf{y}]}+ \\color{orange}{(\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2}} \\\\\n&\\class{fragment}{=\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}]  + 2 (\\mathbb{E}[\\theta | \\mathbf{y}]-a) \\color{purple}{( \\mathbb{E}[\\theta | \\mathbf{y}] - \\mathbb{E}[\\theta | \\mathbf{y}])} + (\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2} \\\\\n&\\class{fragment}{=\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}]  + (\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2}\n\\end{align}\n\\]"
  },
  {
    "objectID": "handouts/bayes_est.html#theorem-1",
    "href": "handouts/bayes_est.html#theorem-1",
    "title": "Bayes estimator proofs",
    "section": "Theorem",
    "text": "Theorem\nUnder absolute loss \\(L(\\theta, a) = |\\theta - a|\\), a Bayes estimator for \\(\\theta\\) is any posterior median of \\(\\theta\\).\n\nThat is, a Bayes estimator is a function \\(\\delta(\\mathbf{y}) \\equiv m\\) such that \\(\\text{Pr}(\\theta \\leq m | \\mathbf{y}) \\geq \\frac{1}{2}\\) and \\(\\text{Pr}(\\theta \\geq m | \\mathbf{y}) \\geq \\frac{1}{2}\\).\nNote that when \\(\\theta\\) is continuous, there exists a single median."
  },
  {
    "objectID": "handouts/bayes_est.html#proof-2-cont.",
    "href": "handouts/bayes_est.html#proof-2-cont.",
    "title": "Bayes estimator proofs",
    "section": "Proof 2 (cont.)",
    "text": "Proof 2 (cont.)\nThus, \\[\n\\underset{a}{\\text{argmin}} \\ \\mathbb{E}[(\\theta - a)^2 | \\mathbf{y}] = \\underset{a}{\\text{argmin}} \\ \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}]  + (\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2\n\\]\n\nThe first term does not depend on \\(a\\), so cannot do anything about minimizing w.r.t \\(a\\)\nThus, focus on minimizing the second term. Minimized when \\(a = \\mathbb{E}[\\theta | \\mathbf{y}]\\).\nThus, the Bayes estimate under squared loss is the posterior mean. \\(\\tiny{\\square}\\)"
  },
  {
    "objectID": "handouts/bayes_est.html#temp",
    "href": "handouts/bayes_est.html#temp",
    "title": "Bayes estimator proofs",
    "section": "temp",
    "text": "temp"
  },
  {
    "objectID": "handouts/bayes_est.html#incremental-derivation",
    "href": "handouts/bayes_est.html#incremental-derivation",
    "title": "Bayes estimator proofs",
    "section": "Incremental Derivation",
    "text": "Incremental Derivation\n\\[\n\\begin{align}\n\\mathbb{E}[(\\theta - a)^2 \\mid \\mathbf{y}] &= \\mathbb{E}[\\big( \\color{red}{(} \\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}]\\color{red}{)} + \\color{red}{(}\\mathbb{E}[\\theta \\mid \\mathbf{y}]  - a\\color{red}{)}\\big)^2 \\mid \\mathbf{y}]\n\\end{align}\n\\]\n.fragment[ \\[\n\\begin{align}\n&= \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])^2 \\mid \\mathbf{y}] \\\\\n&\\quad + 2\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a) \\mid \\mathbf{y}] \\\\\n&\\quad + \\mathbb{E}[(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)^2 \\mid \\mathbf{y}]\n\\end{align}\n\\]]\n.fragment[ \\[\n\\begin{align}\n&= \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])^2 \\mid \\mathbf{y}] \\\\\n&\\quad + 2\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])\\color{orange}{(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)} \\mid \\mathbf{y}] \\\\\n&\\quad + \\mathbb{E}[\\color{orange}{(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)^2} \\mid \\mathbf{y}]\n\\end{align}\n\\]]\n.fragment[ &gt; Note: () and () are constant with respect to ()]\n.fragment[ \\[\n\\begin{align}\n&= \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])^2 \\mid \\mathbf{y}] \\\\\n&\\quad + 2 \\color{orange}{(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)} \\color{purple}{\\mathbb{E}[\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}] \\mid \\mathbf{y}]} \\\\\n&\\quad + \\color{orange}{(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)^2}\n\\end{align}\n\\]]\n.fragment[ \\[\n\\begin{align}\n&= \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])^2 \\mid \\mathbf{y}] \\\\\n&\\quad + 2 (\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a) \\color{purple}{( \\mathbb{E}[\\theta \\mid \\mathbf{y}] - \\mathbb{E}[\\theta \\mid \\mathbf{y}])} \\\\\n&\\quad + (\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)^2\n\\end{align}\n\\]]\n.fragment[ \\[\n\\begin{align}\n&= \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])^2 \\mid \\mathbf{y}] + (\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)^2\n\\end{align}\n\\]]"
  },
  {
    "objectID": "code/post_pred_checks.html",
    "href": "code/post_pred_checks.html",
    "title": "Simulating posterior and posterior predictive distributions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(MASS)\ndata(\"Aids2\")\nFrom the Aids2 dataset from MASS, we have data on patients diagnosed with AIDS in Australia before 1 July 1991. In particular, we are interested in the outcome status of each individual at the end of the observation (alive A or dead D). We also have data on state of origin:\n# A tibble: 4 × 5\n  state     A     D     n death_rate\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n1 NSW     664  1116  1780      0.627\n2 Other   107   142   249      0.570\n3 QLD      78   148   226      0.655\n4 VIC     233   355   588      0.604"
  },
  {
    "objectID": "code/post_pred_checks.html#posterior-predictive-check",
    "href": "code/post_pred_checks.html#posterior-predictive-check",
    "title": "Simulating posterior and posterior predictive distributions",
    "section": "Posterior predictive check",
    "text": "Posterior predictive check\nNow, let’s do some posterior predictive checks (PPC). One thing I might be wondering is if my assumption of iid across the 4 states is reasonble.\nFirst, let me simulate \\(R = 1000\\) new datasets of size 2843 from the PPD. For each dataset \\(r\\), I can assign state values according the observed data because I assumed state doesn’t matter (i.e. state and death status are independent).\n\nset.seed(412)\n# posterior predictive check\nR &lt;- 1000\ndf_ls &lt;- list()\nfor(r in 1:R){\n  theta_samp &lt;- rbeta(1, a + sum_y, b + n - sum_y)\n  ystar_samps &lt;- rbinom(n, size = 1, prob = theta_samp)\n  df_ls[[r]] &lt;- data.frame(status = ystar_samps) |&gt;\n    add_column(state = Aids2$state)\n}\n# Random sample of three rows in first simulated dataset:\ndf_ls[[1]] |&gt;\n  sample_n(3)\n\n  status state\n1      0   NSW\n2      1   NSW\n3      0   NSW\n\n\nVisually, we can look at how the distribution of deaths varies across the states in some of the simulated datasets:\n\n\n\n\n\n\n\n\n\nLooks pretty similar to the plot of the original data above, but humans like to see what they want to see.\nFor a more robust PPC, let’s consider the following test function \\(T(\\mathbf{y})\\): the ratio of the highest death rate and the lowest death rate among the four states.\nIn the observed data, this is 0.655/0.57 = 1.148. Let’s now obtain the values of the test function for the 1000 simualated datasets:\n\nppd_T &lt;- rep(NA, R)\nfor(r in 1:R){\n  temp &lt;- df_ls[[r]] |&gt;\n    group_by(state) |&gt;\n    summarise(death_rate = mean(status))\n  ppd_T[r] &lt;- max(temp$death_rate)/min(temp$death_rate)\n}\n\n\n\n\n\n\n\n\n\n\nThe posterior predictive probability of a ratio as or more extreme than the observed \\(T(\\mathbf{y}) = 1.148\\) is:\n\n# monte carlo approximation\npost_prob_T &lt;- mean(ppd_T &gt;= obs_T)\n\n\\[\\text{Pr}(T(\\mathbf{y}^{*} \\geq T(\\mathbf{y})) | \\mathbf{y}) = 0.088\\] This says that in 88 of the 1000 simulated datasets, I saw a test ratio value as or more extreme that what I observed in real life. To me, that seems quite low. An ideal scenario would have this posterior predictive probability be close to 0.50. Thus, it seems like maybe I should not treat the data as coming from one giant state, and instead allow for variation on the four states. That’s coming in a few weeks…"
  },
  {
    "objectID": "homework/412_hw2_r.html",
    "href": "homework/412_hw2_r.html",
    "title": "STAT 412: Problem Set 2 (R)",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)\nprussian1 &lt;- read_csv(\"../handouts/prussianHorses1.csv\")\nprussian2 &lt;- read_csv(\"../handouts/prussianHorses2.csv\")\n\n\na &lt;- 1\nb &lt;- 2\nqgamma(0.95, a,b)\n\n[1] 1.497866\n\ny &lt;- prussian1$deaths\nn &lt;- length(prussian1$deaths)\ntheta_seq &lt;- seq(0.1, 5, 0.05)\nprior &lt;- dgamma(theta_seq, a, b)\na_post &lt;- a + sum(y)\nb_post &lt;- b + n\npost &lt;- dgamma(theta_seq, a_post, b_post)\nbayes_df &lt;- data.frame(theta = theta_seq, prior = prior, post = post) |&gt;\n  pivot_longer(cols= 2:3, names_to = \"distribution\", values_to = \"density\")\nbayes_df |&gt;\n  ggplot(aes(x = theta, y = density, col = distribution)) + \n  geom_line()\n\n\n\n\n\n\n\npost_prob &lt;-pgamma(0.5, a_post, b_post)\n\nThe posterior probability \\(\\text{Pr}(\\theta  &lt; 0.5 = \\mathbf{y}) = 0.0487284\\). Since this is relatively small, I would say that there is not strong evidence to suggest that the average rate of deaths by horsekick in a given year and cavalary is less than 0.5.\n\nset.seed(2)\ny_new &lt;- rnbinom(220, size = a_post, prob = (b_post)/(b_post + 1))\np1 &lt;- data.frame(y_new) |&gt;\n  ggplot(aes(x = y_new)) + \n  geom_bar() +\n  labs(x = \"Deaths\", title = \"Draws from posterior predictive distribution\")\n\np2 &lt;- prussian2 |&gt;\n  ggplot(aes(x = deaths)) +\n  geom_bar() +\n  labs(x = \"Deaths\", title = \"Held-out observations\")\n\np1 + p2"
  },
  {
    "objectID": "code/post_pred_checks.html#data",
    "href": "code/post_pred_checks.html#data",
    "title": "Simulating posterior and posterior predictive distributions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(MASS)\ndata(\"Aids2\")\n\nFrom the Aids2 dataset from MASS, we have data on patients diagnosed with AIDS in Australia before 1 July 1991. In particular, we are interested in the outcome status of each individual at the end of the observation (alive A or dead D). We also have data on state of origin:\n\n\n# A tibble: 4 × 5\n  state     A     D     n death_rate\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n1 NSW     664  1116  1780      0.627\n2 Other   107   142   249      0.570\n3 QLD      78   148   226      0.655\n4 VIC     233   355   588      0.604"
  },
  {
    "objectID": "code/post_pred_checks.html#model",
    "href": "code/post_pred_checks.html#model",
    "title": "Simulating posterior and posterior predictive distributions",
    "section": "Model",
    "text": "Model\nThe response is binary, so a Bernoulli sampling model could work here. While morbid, let us consider death as success. I’ll assume that individuals are conditionally iid given the probability of death, no matter the state:\n\\[Y_{i} | \\theta \\overset{iid}{\\sim} \\text{Bern}(\\theta) \\qquad i = 1,\\ldots, 2843\\] For my prior, I don’t know much about death rates due to AIDS, so I will use a rather uninformative prior:\n\\[\\theta \\sim \\text{Beta}(2,2)\\] Because \\(n\\) is so large, I expect my posterior to be guided almost fully by the observed data.\nUnder this sampling model and prior, we know the posterior distribution of \\(\\theta\\) and the PPD for a new data point \\(Y^{*}\\) exactly. However, we will obtain samples via simulation from the posterior and PPD instead."
  },
  {
    "objectID": "code/post_pred_checks.html#simulate-posterior-and-posterior-predictive-distributions",
    "href": "code/post_pred_checks.html#simulate-posterior-and-posterior-predictive-distributions",
    "title": "Simulating posterior and posterior predictive distributions",
    "section": "Simulate posterior and posterior predictive distributions",
    "text": "Simulate posterior and posterior predictive distributions\nIn particular, I will obtain \\(S = 5000\\) samples from each distribution:\n\nset.seed(412)\nsum_y &lt;- sum(Aids2$status == \"D\")\nn &lt;- nrow(Aids2)\na &lt;- 2\nb &lt;- 2\n\nS &lt;- 5000\na_star &lt;- a + sum_y\nb_star &lt;- b + n - sum_y\n\n### OPTION 1: for loop\ntheta_samps &lt;- rep(NA, S) \nystar_samps &lt;- rep(NA, S)\nfor(s in 1:S){\n  # sample theta from posterior\n  theta &lt;- rbeta(1, a_star, b_star)\n  \n  # sample y* from data model\n  ystar &lt;- rbinom(1, size = 1, prob = theta)\n  \n  # store\n  theta_samps[s] &lt;- theta\n  ystar_samps[s] &lt;- ystar\n}\n\n### OPTION 2: leverage vectorized language\ntheta_samps &lt;- rbeta(S, a_star, b_star)\nystar_samps &lt;- rbinom(S, size = 1, prob = theta_samps)\n\nYou could run the following code to see how well the simulations approximate the exact distributions:\n\n## Compare simulated vs exact posterior quantities\n# exact posterior mean\na_star / (a_star + b_star)\n# approximate posterior mean\nmean(theta_samps)\n\n# quantile-based 95% posterior credible interval: exact\nqbeta(c(0.025, 0.975), a_star, b_star)\n# quantile-based 95% posterior credible interval: Monte Carlo approximate\nquantile(theta_samps, c(0.025, 0.975))\n\n## Compare simulated PPD to exact\n# mean and var under exact PPD\na_star / (a_star + b_star)\n(a_star / (a_star + b_star)) * (1- (a_star / (a_star + b_star)))\n\n# approximate mean and var under simulated PPD\nmean(ystar_samps)\nvar(ystar_samps)\n\nWe would work with the theta_samps vector to answer questions about \\(\\theta\\). But let’s turn to model checking."
  },
  {
    "objectID": "code/hpd_code.html",
    "href": "code/hpd_code.html",
    "title": "Code for HPD",
    "section": "",
    "text": "The density() function in R estimates the density of a random variable based off an iid sample from the density. It works by chopping up the “x-axis” or support of the density (i.e. the observed range from the iid sample) into small pieces, called coordinates. The function then estimates the density at each one of those coordinates based on the iid sample. The coordinates are the x output from density(), and the estimated density values are the y output from the function. These are accessed accessed via the $ symbol when the returned value from the function is stored as a variable (dens below).\n\n# theta_samps: vector of random samples of theta\n# cred_mass: gamma (e.g. 0.95)\n# grid_size: the bandwidth/number of coordinates used to estimate density\nget_hpd &lt;- function(theta_samps, cred_mass, grid_size = 10000) {\n  # Estimate true posterior density of theta using kernel density estimation.\n  dens &lt;- density(theta_samps, n = grid_size)\n  \n  dx &lt;- diff(dens$x[1:2])  \n  \n  ord &lt;- order(dens$y, decreasing = TRUE)\n  \n  cum_prob &lt;- cumsum(dens$y[ord]) * dx\n  \n  id &lt;- min(which(cum_prob &gt;= cred_mass))\n  dens_cutoff &lt;- dens$y[ord][id]\n  \n  inside &lt;- dens$y &gt;= dens_cutoff\n  \n  hpd_regions &lt;- list() \n  in_region &lt;- FALSE\n  for (i in seq_along(inside)) {\n    if (inside[i] && !in_region) { \n      start &lt;- dens$x[i] \n      in_region &lt;- TRUE \n    } else if ( (!inside[i]) & in_region) { \n      end &lt;- dens$x[i - 1]\n      hpd_regions[[length(hpd_regions) + 1]] &lt;- c(start, end)\n      in_region &lt;- FALSE \n    }\n  }\n  \n  if (in_region) {\n    hpd_regions[[length(hpd_regions) + 1]] &lt;- c(start, dens$x[length(dens$x)])\n  }\n  \n  return(hpd_regions)\n}"
  },
  {
    "objectID": "code/gibbs_normal_model.html",
    "href": "code/gibbs_normal_model.html",
    "title": "Gibbs sampler: Normal Model",
    "section": "",
    "text": "We have data on the amount of time (in hours) a sample of students from a high school spent studying during a two-day reading period before exams. We will model the data as:\n\\[ \\begin{align*}\nY_{i} \\mid \\theta, \\sigma^2 &\\overset{iid}{\\sim} N(\\theta, \\sigma^2) \\qquad i = 1,\\ldots,n \\\\\n\\theta &\\sim N(\\mu_{0}, \\sigma_{0}^2) \\\\\n\\phi &\\sim \\text{Gamma}(a_{0}, b_{0})\n\\end{align*}\\]\nwhere \\(\\phi = \\frac{1}{\\sigma^2}\\). We have to fill out our priors for \\(\\theta\\) and \\(\\phi\\) to fully specify the statistical model before seeing the data. Let’s do that together!\n\nmu_0 &lt;- 5\ns2_0 &lt;- 4\na_0 &lt;- 5\nb_0 &lt;- 15\n\n# demonstrate how I might do prior elicitation\nhist(sqrt(1/(rgamma(10000, a_0, b_0))))"
  },
  {
    "objectID": "code/gibbs_normal_model.html#prior-specification",
    "href": "code/gibbs_normal_model.html#prior-specification",
    "title": "Gibbs sampler: Normal Model",
    "section": "",
    "text": "We have data on the amount of time (in hours) a sample of students from a high school spent studying during a two-day reading period before exams. We will model the data as:\n\\[ \\begin{align*}\nY_{i} \\mid \\theta, \\sigma^2 &\\overset{iid}{\\sim} N(\\theta, \\sigma^2) \\qquad i = 1,\\ldots,n \\\\\n\\theta &\\sim N(\\mu_{0}, \\sigma_{0}^2) \\\\\n\\phi &\\sim \\text{Gamma}(a_{0}, b_{0})\n\\end{align*}\\]\nwhere \\(\\phi = \\frac{1}{\\sigma^2}\\). We have to fill out our priors for \\(\\theta\\) and \\(\\phi\\) to fully specify the statistical model before seeing the data. Let’s do that together!\n\nmu_0 &lt;- 5\ns2_0 &lt;- 4\na_0 &lt;- 5\nb_0 &lt;- 15\n\n# demonstrate how I might do prior elicitation\nhist(sqrt(1/(rgamma(10000, a_0, b_0))))"
  },
  {
    "objectID": "code/gibbs_normal_model.html#data",
    "href": "code/gibbs_normal_model.html#data",
    "title": "Gibbs sampler: Normal Model",
    "section": "Data",
    "text": "Data\nNow we get to see the data:\n\ny &lt;- readRDS(\"~/Desktop/STAT 412/data/school1.Rda\")\ndata.frame(hours = y) |&gt;\n  ggplot(aes(x = hours)) +\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\nybar &lt;- mean(y)\nn &lt;- length(y)"
  },
  {
    "objectID": "code/gibbs_normal_model.html#gibbs-sampler",
    "href": "code/gibbs_normal_model.html#gibbs-sampler",
    "title": "Gibbs sampler: Normal Model",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\nset.seed(412)\n### GIBBS sampler\nS &lt;- 5000\n\n# will generate a bunch of samples, need to store them somewhere!\nTHETA &lt;- rep(NA, S)\nS2 &lt;- rep(NA, S)\n  \n# initialize the sampler \n# since sampling precision first, initialize value of theta\ntheta &lt;- 2\n\nfor (s in 1:S){\n  # sample phi from full conditional\n  b_n &lt;- 0.5*sum( (y - theta)^2) + b_0\n  a_n &lt;- 0.5*n + a_0\n  phi &lt;- rgamma(1, a_n, b_n)\n  \n  # convert phi to s2\n  s2 &lt;- 1/phi\n  \n  # sample theta from full conditional\n  s2_n &lt;- 1/(1/s2_0 + n / s2)\n  mu_n &lt;- s2_n * (n * ybar/s2 + mu_0/s2_0)\n  theta &lt;- rnorm(1, mu_n, sqrt(s2_n))\n\n  # store my samples\n  THETA[s] &lt;- theta\n  S2[s] &lt;- s2\n}\n\n\nSeeing the sampling"
  },
  {
    "objectID": "code/gibbs_normal_model.html#traceplots-and-posterior-quantities",
    "href": "code/gibbs_normal_model.html#traceplots-and-posterior-quantities",
    "title": "Gibbs sampler: Normal Model",
    "section": "Traceplots and posterior quantities",
    "text": "Traceplots and posterior quantities\nTrace plots of markov chains: line plot of simulated draws of parameter against iteration. In theory, the Metropolis and Gibbs sampling algorithms will produce simulated draws that converge to the posterior distribution of interest. But in typical practice, it may take a number of iterations before the simulation values are close to the posterior distribution. So in general it is recommended that one run the algorithm for a number of “burn-in” iterations before one collects iterations for inference.\nWe want nice “caterpillar” looking traceplots. This would suggest convergence!\n\nplot(THETA, xlab = \"Iteration\", ylab = \"theta\", type = \"l\", main = \"Traceplot\")\n\n\n\n\n\n\n\nplot(S2, xlab = \"Iteration\", ylab = \"sigma2\", type = \"l\", main = \"Traceplot\")\n\n\n\n\n\n\n\n\nThat crazy large sampled \\(\\sigma^2\\) at the beginning is a product of intialization and exploring the state space; we almost surely haven’t entered stationary distribution yet. Let’s go ahead and throw away half of our values as burn-in.\n\n# throw away some values as burn-in\nTHETA &lt;- THETA[-c(1:round(S/2))]\nS2 &lt;- S2[-c(1:round(S/2))]\n\nLet’s re-examine the traceplots post burn-in. From experience, these are beautiful looking traceplots!\n\npost_df &lt;- data.frame(theta = THETA, s2 = S2)\npost_df |&gt;\n  mutate(iteration = row_number()) |&gt;\n  pivot_longer(cols = 1:2, names_to = \"param\", values_to = \"value\") |&gt;\n  ggplot(aes(x = iteration, y = value)) +\n  geom_line() +\n  facet_wrap(~param, scales = \"free\") +\n  labs(\"Traceplots after burn-in\")\n\n\n\n\n\n\n\n\nWe should only estimate quantities after we’ve converged (i.e. after burn-in). In this case, we can obtain estimates of posterior quantities like usual:\n\n# estimated posterior mean of theta\nmean(THETA)\n\n[1] 8.967482\n\n# estimated posterior mean of sigma\nmean(sqrt(S2))\n\n[1] 3.493671\n\n## estimated CIs\nquantile(THETA, c(0.025, 0.975))\n\n     2.5%     97.5% \n 7.629652 10.265034 \n\nquantile(sqrt(S2), c(0.025, 0.975))\n\n    2.5%    97.5% \n2.787186 4.464605"
  },
  {
    "objectID": "code/gibbs_normal_model.html#diving-into-joint-density",
    "href": "code/gibbs_normal_model.html#diving-into-joint-density",
    "title": "Gibbs sampler: Normal Model",
    "section": "Diving into joint density",
    "text": "Diving into joint density\nWe can obtain a contour plot of the joint density using the samples obtained from Gibbs:\n\n\n\n\n\n\n\n\n\nWe can see how we “marginalize out” one parameter to get the posterior density of the other. To obtain marginals, we simply grab/isolate the parameter of interest.\n\n\n\n\n\n\n\n\n\n\nhist(THETA, main = \"Marginal posterior histogram\", xlab = \"theta\")\n\n\n\n\n\n\n\nhist(S2, main = \"Marginal posterior histogram\", xlab = \"s2\")"
  },
  {
    "objectID": "code/gibbs_normal_model.html#chains",
    "href": "code/gibbs_normal_model.html#chains",
    "title": "Gibbs sampler: Normal Model",
    "section": "Chains",
    "text": "Chains\nIn this example, we have illustrated running a single “chain” where one has a single starting value and we simulated draws over \\(S\\) iterations. It is possible that the behavior of the MCMC sample will depend on the choice of starting value. So a general recommendation is to run the MCMC algorithm several times using different starting values. In this case, one will have multiple MCMC chains and combine them.\nWhen we code samplers by hand in this course, we will almost always run a single chain to make our lives easier. However, when we turn to software that implements the MCMC for us, it’s very easy to specify multiple chains!"
  },
  {
    "objectID": "code/gibbs_normal_model_sampler_code.html",
    "href": "code/gibbs_normal_model_sampler_code.html",
    "title": "Gibbs Sampler: Normal Model",
    "section": "",
    "text": "set.seed(412)\n### GIBBS sampler\nS &lt;- 5000\n\n# will generate a bunch of samples, need to store them somewhere!\nTHETA &lt;- rep(NA, S)\nS2 &lt;- rep(NA, S)\n  \n# initialize the sampler \n# since sampling precision first, initialize value of theta\ntheta &lt;- 2\n\nfor (s in 1:S){\n  # sample phi from full conditional\n  b_n &lt;- 0.5*sum((y - theta)^2) + b_0\n  a_n &lt;- 0.5*n + a_0\n  phi &lt;- rgamma(1, a_n, b_n)\n  \n  # convert phi to s2\n  s2 &lt;- 1/phi\n  \n  # sample theta from full conditional\n  s2_n &lt;- 1/(1/s2_0 + n / s2)\n  mu_n &lt;- s2_n * (n * ybar/s2 + mu_0/s2_0)\n  theta &lt;- rnorm(1, mu_n, sqrt(s2_n))\n\n  # store my samples\n  THETA[s] &lt;- theta\n  S2[s] &lt;- s2\n}"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html",
    "href": "code/gibbs_normal_model_walkthrough.html",
    "title": "Gibbs sampler: Normal Model",
    "section": "",
    "text": "We have data on the amount of time (in hours) a sample of students from a high school spent studying during a two-day reading period before exams. We will model the data as:\n\\[ \\begin{align*}\nY_{i} \\mid \\theta, \\sigma^2 &\\overset{iid}{\\sim} N(\\theta, \\sigma^2) \\qquad i = 1,\\ldots,n \\\\\n\\theta &\\sim N(\\mu_{0}, \\sigma_{0}^2) \\\\\n\\phi &\\sim \\text{Gamma}(a_{0}, b_{0})\n\\end{align*}\\]\nwhere \\(\\phi = \\frac{1}{\\sigma^2}\\). We have to fill out our priors for \\(\\theta\\) and \\(\\phi\\) to fully specify the statistical model before seeing the data. Let’s do that together!\n\nmu_0 &lt;- 6\ns2_0 &lt;- 2.5\na_0 &lt;- 5\nb_0 &lt;- 100\n\n# demonstrate how I might do prior elicitation\n# hist(sqrt(1/(rgamma(10000, a_0, b_0))))"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html#prior-specification",
    "href": "code/gibbs_normal_model_walkthrough.html#prior-specification",
    "title": "Gibbs sampler: Normal Model",
    "section": "",
    "text": "We have data on the amount of time (in hours) a sample of students from a high school spent studying during a two-day reading period before exams. We will model the data as:\n\\[ \\begin{align*}\nY_{i} \\mid \\theta, \\sigma^2 &\\overset{iid}{\\sim} N(\\theta, \\sigma^2) \\qquad i = 1,\\ldots,n \\\\\n\\theta &\\sim N(\\mu_{0}, \\sigma_{0}^2) \\\\\n\\phi &\\sim \\text{Gamma}(a_{0}, b_{0})\n\\end{align*}\\]\nwhere \\(\\phi = \\frac{1}{\\sigma^2}\\). We have to fill out our priors for \\(\\theta\\) and \\(\\phi\\) to fully specify the statistical model before seeing the data. Let’s do that together!\n\nmu_0 &lt;- 6\ns2_0 &lt;- 2.5\na_0 &lt;- 5\nb_0 &lt;- 100\n\n# demonstrate how I might do prior elicitation\n# hist(sqrt(1/(rgamma(10000, a_0, b_0))))"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html#data",
    "href": "code/gibbs_normal_model_walkthrough.html#data",
    "title": "Gibbs sampler: Normal Model",
    "section": "Data",
    "text": "Data\nNow we get to see the data:\n\ny &lt;- readRDS(\"~/Desktop/STAT 412/data/school1.Rda\")\ndata.frame(hours = y) |&gt;\n  ggplot(aes(x = hours)) +\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\nybar &lt;- mean(y)\nn &lt;- length(y)"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html#gibbs-sampler",
    "href": "code/gibbs_normal_model_walkthrough.html#gibbs-sampler",
    "title": "Gibbs sampler: Normal Model",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\nset.seed(412)\n### GIBBS sampler\nS &lt;- 5000\n\n# will generate a bunch of samples, need to store them somewhere!\nTHETA &lt;- rep(NA, S)\nS2 &lt;- rep(NA, S)\n  \n# initialize the sampler \n# since sampling precision first, initialize value of theta\ntheta &lt;- 2\n\nfor (s in 1:S){\n  # sample phi from full conditional\n  b_n &lt;- 0.5*sum( (y - theta)^2) + b_0\n  a_n &lt;- 0.5*n + a_0\n  phi &lt;- rgamma(1, a_n, b_n)\n  \n  # convert phi to s2\n  s2 &lt;- 1/phi\n  \n  # sample theta from full conditional\n  s2_n &lt;- 1/(1/s2_0 + n / s2)\n  mu_n &lt;- s2_n * (n * ybar/s2 + mu_0/s2_0)\n  theta &lt;- rnorm(1, mu_n, sqrt(s2_n))\n\n  # store my samples\n  THETA[s] &lt;- theta\n  S2[s] &lt;- s2\n}\n\n\nSeeing the sampling"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html#traceplots-and-posterior-quantities",
    "href": "code/gibbs_normal_model_walkthrough.html#traceplots-and-posterior-quantities",
    "title": "Gibbs sampler: Normal Model",
    "section": "Traceplots and posterior quantities",
    "text": "Traceplots and posterior quantities\nTrace plots of markov chains: line plot of simulated draws of parameter against iteration. In theory, the Metropolis and Gibbs sampling algorithms will produce simulated draws that converge to the posterior distribution of interest. But in typical practice, it may take a number of iterations before the simulation values are close to the posterior distribution. So in general it is recommended that one run the algorithm for a number of “burn-in” iterations before one collects iterations for inference.\nWe want nice “caterpillar” looking traceplots. This would suggest convergence!\n\nplot(THETA, xlab = \"Iteration\", ylab = \"theta\", type = \"l\", main = \"Traceplot\")\n\n\n\n\n\n\n\nplot(S2, xlab = \"Iteration\", ylab = \"sigma2\", type = \"l\", main = \"Traceplot\")\n\n\n\n\n\n\n\n\nThat crazy large sampled \\(\\sigma^2\\) at the beginning is a product of intialization and exploring the state space; we almost surely haven’t entered stationary distribution yet. Let’s go ahead and throw away half of our values as burn-in.\n\n# throw away some values as burn-in\nTHETA &lt;- THETA[-c(1:round(S/2))]\nS2 &lt;- S2[-c(1:round(S/2))]\n\nLet’s re-examine the traceplots post burn-in. From experience, these are beautiful looking traceplots!\n\npost_df &lt;- data.frame(theta = THETA, s2 = S2)\npost_df |&gt;\n  mutate(iteration = row_number()) |&gt;\n  pivot_longer(cols = 1:2, names_to = \"param\", values_to = \"value\") |&gt;\n  ggplot(aes(x = iteration, y = value)) +\n  geom_line() +\n  facet_wrap(~param, scales = \"free\") +\n  labs(\"Traceplots after burn-in\")\n\n\n\n\n\n\n\n\nWe should only estimate quantities after we’ve converged (i.e. after burn-in). In this case, we can obtain estimates of posterior quantities like usual:\n\n# estimated posterior mean of theta\nmean(THETA)\n\n[1] 8.693283\n\n# estimated posterior mean of sigma\nmean(sqrt(S2))\n\n[1] 4.19726\n\n## estimated CIs\nquantile(THETA, c(0.025, 0.975))\n\n     2.5%     97.5% \n 7.177723 10.142940 \n\nquantile(sqrt(S2), c(0.025, 0.975))\n\n    2.5%    97.5% \n3.355402 5.368383"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html#diving-into-joint-density",
    "href": "code/gibbs_normal_model_walkthrough.html#diving-into-joint-density",
    "title": "Gibbs sampler: Normal Model",
    "section": "Diving into joint density",
    "text": "Diving into joint density\nWe can obtain a contour plot of the joint density using the samples obtained from Gibbs:\n\n\n\n\n\n\n\n\n\nWe can see how we “marginalize out” one parameter to get the posterior density of the other. To obtain marginals, we simply grab/isolate the parameter of interest.\n\n\n\n\n\n\n\n\n\n\nhist(THETA, main = \"Marginal posterior histogram\", xlab = \"theta\")\n\n\n\n\n\n\n\nhist(S2, main = \"Marginal posterior histogram\", xlab = \"s2\")"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html#chains",
    "href": "code/gibbs_normal_model_walkthrough.html#chains",
    "title": "Gibbs sampler: Normal Model",
    "section": "Chains",
    "text": "Chains\nIn this example, we have illustrated running a single “chain” where one has a single starting value and we simulated draws over \\(S\\) iterations. It is possible that the behavior of the MCMC sample will depend on the choice of starting value. So a general recommendation is to run the MCMC algorithm several times using different starting values. In this case, one will have multiple MCMC chains and combine them.\nWhen we code samplers by hand in this course, we will almost always run a single chain to make our lives easier. However, when we turn to software that implements the MCMC for us, it’s very easy to specify multiple chains!"
  },
  {
    "objectID": "handouts/constrained_normal.html",
    "href": "handouts/constrained_normal.html",
    "title": "Constrained Normal Sampling Code",
    "section": "",
    "text": "# n = number of random samples \n# mu = mean of normal \n# sd = std. dev. normal \n# a = lower constraint (can be -Inf or a real number) \n# b = upper constraint (can be Inf or a real number)\n\nrnorm_constrained &lt;- function(n, mu, sd, a, b){\n  u &lt;- runif(n, pnorm(a, mu,sd), pnorm(b, mu, sd)) \n  qnorm(u, mu, sd) \n}"
  }
]
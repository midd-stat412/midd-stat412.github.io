[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 412: Bayesian Statistics",
    "section": "",
    "text": "Welcome to the website for Middlebury College’s Fall 2025 STAT 412. On this website you will find the course syllabus, schedule, and assignments. The website is frequently updated throughout the semester, so please make a habit of refreshing the page."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#announcements",
    "href": "index.html#announcements",
    "title": "STAT 412: Bayesian Statistics",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 11/03 from 3:30-4:30pm: pre-registration dessert social in WNS 105\n\nModified office hours on 11/03: 2-3pm (instead of 3-4pm)\nYou can also come to the dessert social to ask questions :)\n\nWednesday 10/29 from 7-9 pm: Math & Stat Summer Work Poster Session"
  },
  {
    "objectID": "index.html#course-details",
    "href": "index.html#course-details",
    "title": "STAT 412: Bayesian Statistics",
    "section": "Course details",
    "text": "Course details\nInstructor: Becky Tang (she/her)\n\nOffice: WNS 214\nEmail: btang@middlebury.edu\n\nMeeting times and location: Mondays and Wednesdays 9:45-11am in WNS 011\nOffice hours: Monday 3-4pm, Tuesday 3:30-4:30pm, Friday 11am-12pm\nSyllabus (most recent update 9/1/2025): our syllabus outlines our course policies and a rough schedule. Once classes begin, you should follow the schedule and due dates found on this website."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This page is frequently updated!\n\n\n\nWeek\nDate\nTopic\nAssignments\n\n\n\n\n1\nM 9/08\n\nWelcome!\nWhat is a statistical model?\n\n\nProblem Set 0 (due next class 9/10)\n\n\n\n\nW 9/10\n\nBayes’ rule, Bayes modeling\n\nLikelihood, prior, posterior\nCode and worksheet\n\n\n\nProblem Set 1 (due next class 9/15)\n\n\n\n2\nM 9/15\n\nBeta-Binomial model\n\nProportionality\n\nPosterior predictive distribution\n\n\nProblem Set 2 (due 9/24)\n\n\n\n\nW 9/17\n\nBayes estimators/estimates\n\nSome proofs\n\n\n\nProblem Set 2 (due 9/24)\n\n\n\n3\nM 9/22\n\nConfidence regions\nPosterior quantities and posterior predictive checks via simulation\n\nCode\n\n\n\n\n\n\nW 9/24\n\nConjugacy\nNormal sampling model\n\n\nProblem Set 3 (complete!)\nCode for HW 3\n\n\n\n4\nM 9/29\n\nMarkov chain basics\n\n\n\n\n\nW 10/01\n\nNormal model (cont.) with Gibbs sampling!\n\nData under Handouts\nCode from class\n\n\n\nFor Monday, read the article Explaining the Gibbs Sampler under Handouts and prepare to discuss!\nProblem Set 4\n\n\n\n5\nM 10/06\n\nDiscuss Explaining the Gibbs Sampler\nMCMC diagnostics\n\nCode here\n\n\n\n\n\n\nW 10/06\n\nFinish MCMC diagnostics\nJeffrey’s prior\nIntroduce Case Study 1\n\n\nWork on Case Study 1\n\n\n\n6\nM 10/13\nBecky out of town; Prof. Christian Stratton will be here in my stead!\n\nWork on Case Study 1\nWork on midterm review\n\n\nWork on Case Study 1\n\n\n\n\nW 10/15\n\nPresent Case Study 1\n\n\nStudy for midterm!\n\n\n\n7\nM 10/20\n\nBayesian hierarchical modeling\n\nHandout\n\n\n\n\n\n\nW 10/22\n\nBayesian hierarchical modeling (cont.)\n\nFind data on handouts slide\n\n\n\nProblem Set 5\n\nStarter template and data under handouts\n\n\n\n\n8\nM 10/27\n\nBayesian simple linear regression\n\nHandout\n\n\nFor Wednesday, please read:\n\nSection 7.1 of Hoff (pg. 105-7)\nPg. 337-341 of Blitzstein & Hwang (stop before Ex. 7.5.8. Skip Definition 7.5.6 and proofs if you’d like! But if you miss MGFs, now’s your chance to see them again!)\n\n\n\n\nW 10/29\n\nMultiple linear regression\n\nHandout + code\n\n\n\nProblem Set 6\n\n\n\n9\nM 11/03\n\nMetropolis\n\n\n\n\n\nW 11/05\n\nMetropolis (cont.)\nIntroduce Case Study 2\n\n\n\n\n10\nM 11/10\n\nJAGS\n\n\n\n\n\nW 11/12\n\nCase Study 2 presentations\nJAGS (cont.)\n\n\n\n\n11\nM 11/17\n\nReal-data analysis (ecology!)\n\n\n\n\n\nW 11/19\n\nIntroduce Case Study 3\n\n\n\n\n\n\nTHANKSGIVING\n\n\n\n12\nM 12/01\n\nIntroduce final project\nWork on Case Study 3\n\n\n\n\n\nW 12/03\n\nCase Study 3 presentations\nCRFs"
  },
  {
    "objectID": "handouts.html",
    "href": "handouts.html",
    "title": "Handouts",
    "section": "",
    "text": "Distribution sheet (updated 10/15 with Dirichlet and conjugate priors)\nExplaining the Gibbs Sampler (1992)\n\nDiscussion questions posted here!\n\nConstrained normal sampling code\nProblem Set 5 starter template:  .qmd  or  .Rmd \nData\n\n prussianHorses1 \n prussianHorses2 \n school1  Load code in as readRDS(\"filepath/school1.Rda\")\n divorce  Load code in as readRDS(\"filepath/divorce.Rda\")\n mathscores  Load code in as readRDS(\"filepath/mathscores.Rda\")\n tennis_serves  Load code in as readRDS(\"filepath/tennis_serves.Rda\")\n bbs  Load code in as readRDS(\"filepath/bbs.Rda\")\n hosp_infection \n olympic_butterfly \n ProfessorSalary"
  },
  {
    "objectID": "code/binom_discrete_prior.html",
    "href": "code/binom_discrete_prior.html",
    "title": "Learning about a Binomial Probability",
    "section": "",
    "text": "library(tidyverse)\nknitr::opts_chunk$set(fig.width = 6, fig.height = 3)"
  },
  {
    "objectID": "code/binom_discrete_prior.html#tidyverse-code",
    "href": "code/binom_discrete_prior.html#tidyverse-code",
    "title": "Learning about a Binomial Probability",
    "section": "Tidyverse code",
    "text": "Tidyverse code\n\n# PRIOR\ntheta &lt;- seq(0.1, 0.9, by = 0.1)\nprior1 &lt;- rep(1/9, 9)\nprior2 &lt;- c(0.05, 0.05, 0.05, 0.2, 0.3, 0.2, 0.05, 0.05, 0.05)\n\n# create data frame for visualization and wrangling\nbayes_table &lt;- data.frame(theta, prior1, prior2) \n\n# visualize different priors\nbayes_table |&gt;\n  pivot_longer(cols = 2:3, names_to = \"prior\", values_to = \"prior_probs\") |&gt;\n  mutate(theta = factor(theta)) |&gt; # for prettier visualization\n  ggplot(aes(x = theta, y = prior_probs)) + \n  geom_col() +\n  facet_wrap(~ prior) +\n  labs(x = expression(theta),\n      y = expression(f(theta)),\n      title = \"Prior\")\n\n\n\n\n\n\n\n# LIKELIHOOD\ny &lt;- 4\n## since R is vectorized:\nlike &lt;- dbinom(y, size = 11, prob = theta)\n## equivalently\n## like &lt;- choose(11, y) * theta^y * (1-theta)^(11-y)\nbayes_table &lt;- bayes_table |&gt;\n  add_column(likelihood = like) \n\n\n# MARGINAL LIKELIHOOD\nbayes_table &lt;- bayes_table |&gt;\n  mutate(product = prior2 * likelihood)\nmarg_like &lt;- sum(bayes_table$product)\n\n# POSTERIOR\nbayes_table &lt;- bayes_table |&gt;\n  mutate(posterior = product / marg_like)\nbayes_table |&gt;\n  select(theta, posterior)\n\n  theta      posterior\n1   0.1 0.006168515787\n2   0.2 0.043274594401\n3   0.3 0.086030889543\n4   0.4 0.369693507640\n5   0.5 0.377836937562\n6   0.6 0.109538817078\n7   0.7 0.006772110839\n8   0.8 0.000676165538\n9   0.9 0.000008461613\n\n# visualize prior vs posterior\n# note: could also plot in two separate plots without pivoting\nbayes_table |&gt;\n  pivot_longer(cols = c(\"prior1\", \"posterior\"), names_to = \"dist\", values_to = \"probs\") |&gt;\n  mutate(theta = factor(theta),\n         dist = factor(dist, levels = c(\"prior1\", \"posterior\"))) |&gt; # for prettier visualization\n  ggplot(aes(x = theta, y = probs)) + \n  geom_col() +\n  facet_wrap(~dist) +\n  labs(x = expression(theta),\n      y = \"Probability\",\n      title = expression(\"Comparison of \" * f(theta) * \" and \" * f(theta * \"| y\")))"
  },
  {
    "objectID": "code/binom_discrete_prior.html#base-r-code",
    "href": "code/binom_discrete_prior.html#base-r-code",
    "title": "Learning about a Binomial Probability",
    "section": "Base R code",
    "text": "Base R code\n\n## PRIOR\ntheta &lt;- seq(0.1, 0.9, by = 0.1)\nprior1 &lt;- rep(1/9, 9)\nprior2 &lt;- c(0.05, 0.05, 0.05, 0.2, 0.3, 0.2, 0.05, 0.05, 0.05)\n\nbayes_table &lt;- data.frame(theta, prior1, prior2)\n\n# Visualize prior 1\nbarplot(bayes_table$prior1, names.arg = bayes_table$theta,\n        xlab = expression(theta),\n        ylab = expression(f(theta)),\n        ylim = c(0, max(prior1) + 0.05),\n        main = \"Prior\")\n\n\n\n\n\n\n\n# LIKELIHOOD\ny &lt;- 4\nlike &lt;- dbinom(y, size = 11, prob = theta)\nbayes_table$likelihood &lt;- like\n\n# MARGINAL LIKELIHOOD\nbayes_table$product &lt;- prior1 * like\nmarg_like &lt;- sum(bayes_table$product)\n\n# POSTERIOR\nbayes_table$posterior &lt;-  bayes_table$product / marg_like\n\n# vizualize prior and posterior\npar(mfrow = c(1,2))\ny_max &lt;- max(c(prior1, bayes_table$posterior))\nbarplot(bayes_table$prior1, names.arg = bayes_table$theta,\n        xlab = expression(theta),\n        ylab = expression(f(theta)),\n        ylim = c(0, y_max + 0.05),\n        main = \"Prior\")\nbarplot(bayes_table$posterior, \n        names.arg = theta,\n        beside = T,\n        xlab = expression(theta),\n        ylab = expression(f(theta * \"| y\")),\n        ylim = c(0, y_max + 0.05),\n        main = \"Posterior\")"
  },
  {
    "objectID": "code/binom_discrete_prior.html#if-you-didnt-want-to-plot",
    "href": "code/binom_discrete_prior.html#if-you-didnt-want-to-plot",
    "title": "Learning about a Binomial Probability",
    "section": "If you didn’t want to plot",
    "text": "If you didn’t want to plot\n\nmarg_like &lt;- sum(prior1 * like)\npost &lt;- (prior1 * like)/marg_like\ncbind(theta, post)\n\n      theta          post\n [1,]   0.1 0.01893857939\n [2,]   0.2 0.13286167531\n [3,]   0.3 0.26413206805\n [4,]   0.4 0.28375828506\n [5,]   0.5 0.19333918362\n [6,]   0.6 0.08407652891\n [7,]   0.7 0.02079173713\n [8,]   0.8 0.00207596368\n [9,]   0.9 0.00002597885"
  },
  {
    "objectID": "code/proportionality.html",
    "href": "code/proportionality.html",
    "title": "Prior, likelihood, posterior, proportionality",
    "section": "",
    "text": "We often say the posterior distribution is “proportional to likelihood times prior”, where proportionality is with respect to \\(\\theta\\):\n\\[\\begin{align*}\nf_{\\theta | y}(\\theta | y) &= \\frac{f_{y | \\theta} (y | \\theta) f_{\\theta}(\\theta)}{f_{Y}(y)} \\\\\n& \\overset{\\theta}{\\propto} f_{y | \\theta} (y | \\theta) f_{\\theta}(\\theta)\n\\end{align*}\\]\nThat is, the posterior distribution is a function of \\(\\theta\\), so the marginal likelihood \\(f_{Y}(y)\\) is a constant with respect to \\(\\theta\\). It is just a scaling factor, as illustrated here:"
  },
  {
    "objectID": "handouts/bayes_est.html#theorem",
    "href": "handouts/bayes_est.html#theorem",
    "title": "Bayes estimator proofs",
    "section": "Theorem",
    "text": "Theorem\nUnder squared loss \\(L(\\theta, a) = (\\theta - a)^2\\), the Bayes estimator for \\(\\theta\\) is the posterior mean of \\(\\theta\\), \\(\\mathbb{E}[\\theta | \\mathbf{y}]\\).\n\nWant to show that \\(\\mathbb{E}[\\theta | \\mathbf{y}] = \\underset{a}{\\text{argmin}} \\ \\mathbb{E}[(\\theta - a)^2 | \\mathbf{y}]\\)\nRecall: \\(\\mathbb{E}[\\theta | \\mathbf{y}] = \\int_{\\Theta} \\theta f(\\theta | \\mathbf{y}) d\\theta\\) is a function of \\(\\mathbf{y}\\)! We’ve integrated out \\(\\theta\\)!\nAlso, for any function \\(g\\), \\(\\mathbb{E}[g(\\mathbf{y}) | \\mathbf{y}] = g(\\mathbf{y})\\).\n\nThat is, since we condition on \\(\\mathbf{y}\\) (not random), the expectation of any function of \\(\\mathbf{y}\\) is itself"
  },
  {
    "objectID": "handouts/bayes_est.html#proof-set-up",
    "href": "handouts/bayes_est.html#proof-set-up",
    "title": "Bayes estimator proofs",
    "section": "Proof set-up",
    "text": "Proof set-up\n\nWant to show that \\(m = \\underset{a}{\\text{argmin}} \\ \\mathbb{E}[ |\\theta - a| | \\mathbf{y}]\\)\nNote that the absolute value function is not differentiable, so proving a maximum/minimum cannot rely on derivatives!\nAssume that \\(\\theta\\) is continuous, so that if \\(m\\) is the posterior median, then \\(\\text{Pr}(\\theta \\geq m | \\mathbf{y}) = \\frac{1}{2} = \\text{Pr}(\\theta \\leq m | \\mathbf{y})\\).\nLet \\(a\\) be any other estimator of \\(\\theta\\).\nWe will show that\n\\[\n\\mathbb{E}[L(\\theta , a) | \\mathbf{y}] - \\mathbb{E}[L(\\theta, m) | \\mathbf{y}]  \\geq 0\n\\]\nthus demonstrating that \\(m\\) minimizes the expected loss.\nSome recap from probability: if \\(Y\\) continuous with pdf \\(f\\),\n\n\\(\\text{Pr}(Y \\leq a) = \\int_{-\\infty}^{a} f(y) dy\\)\n\\(\\text{Pr}(a \\leq Y \\leq b) = \\text{Pr}(Y\\leq b) - \\text{Pr}(Y \\leq a)\\) and these inequalities could be \\(\\leq\\) or \\(&lt;\\)"
  },
  {
    "objectID": "handouts/bayes_est.html#proof",
    "href": "handouts/bayes_est.html#proof",
    "title": "Bayes estimator proofs",
    "section": "Proof",
    "text": "Proof\nLet \\(m\\) be the posterior median, and suppose \\(a &lt; m\\) is any other estimator. Remember, \\(a\\) and \\(m\\) are constant w.r.t. \\(\\theta\\).\n\\[\n\\begin{align}\n\\mathbb{E}[L(\\theta , a) | \\mathbf{y}] & - \\mathbb{E}[L(\\theta, m) | \\mathbf{y}] = \\int_{\\Theta} |\\theta - a| f(\\theta | \\mathbf{y}) d\\theta - \\int_{\\Theta} |\\theta - m| f(\\theta | \\mathbf{y})d\\theta \\\\\n&\\class{fragment}{= \\int_{\\Theta} \\left(|\\theta - a| - |\\theta - m|\\right) f(\\theta | \\mathbf{y})d\\theta } \\\\\n&\\class{fragment}{= \\int_{-\\infty}^{a} \\left(|\\theta - a| - |\\theta - m|\\right) f(\\theta | \\mathbf{y})d\\theta + \\int_{a}^{m} \\left(|\\theta - a| - |\\theta - m|\\right) f(\\theta | \\mathbf{y})d\\theta + \\int_{m}^{\\infty} \\left(|\\theta - a| - |\\theta - m|\\right) f(\\theta | \\mathbf{y})d\\theta }\\\\\n& \\class{fragment}{= \\int_{-\\infty}^{a} ((a-\\theta) - ( m - \\theta)) f(\\theta | \\mathbf{y}) d\\theta + \\int_{a}^{m} ((\\theta - a) - (m- \\theta)) f(\\theta | \\mathbf{y}) d\\theta  +\n\\int_{m}^{\\infty} ((\\theta - a) - (\\theta - m)) f(\\theta | \\mathbf{y}) d\\theta} \\\\\n&\\class{fragment}{= \\int_{-\\infty}^{a} (a - m)  f(\\theta | \\mathbf{y}) d\\theta + \\int_{a}^{m} (2\\theta - a- m)  f(\\theta | \\mathbf{y}) d\\theta + \\int_{m}^{\\infty} (m-a) f(\\theta | \\mathbf{y}) d\\theta} \\\\\n&\\class{fragment}{\\color{orange}{\\geq}  \\int_{-\\infty}^{a} (a - m)  f(\\theta | \\mathbf{y}) d\\theta + \\int_{a}^{m} (2\\color{orange}{a} - a- m)  f(\\theta | \\mathbf{y}) d\\theta + \\int_{m}^{\\infty} (m-a) f(\\theta | \\mathbf{y}) d\\theta} \\\\\n&\\class{fragment}{= (a-m)\\text{Pr}(\\theta \\leq a | \\mathbf{y}) + \\color{purple}{(a-m)\\text{Pr}(a  &lt; \\theta \\leq m | \\mathbf{y}) }+ (m-a) \\text{Pr}(\\theta \\geq m | \\mathbf{y})} \\\\\n&\\class{fragment}{ = (a-m)\\text{Pr}(\\theta \\leq a | \\mathbf{y}) + \\color{purple}{(a-m) \\text{Pr}(\\theta \\leq m | \\mathbf{y})  - (a-m) \\text{Pr}(\\theta \\leq a | \\mathbf{y})} + (m-a) \\text{Pr}(\\theta \\geq m | \\mathbf{y}) } \\\\\n&\\class{fragment}{= (a-m) \\text{Pr}(\\theta \\leq m | \\mathbf{y}) +  (m-a) \\text{Pr}(\\theta \\geq m | \\mathbf{y})} \\\\\n&\\class{fragment}{= (a-m)\\left(\\frac{1}{2}\\right) + (m-a)\\left(\\frac{1}{2}\\right)} \\\\\n&\\class{fragment}{= (a-m)\\left(\\frac{1}{2}\\right) - (a-m)\\left(\\frac{1}{2}\\right)}\\\\\n&\\class{fragment}{= 0 \\qquad \\tiny{\\square} }\n\\end{align}\n\\]"
  },
  {
    "objectID": "handouts/bayes_est.html#proof-2",
    "href": "handouts/bayes_est.html#proof-2",
    "title": "Bayes estimator proofs",
    "section": "Proof 2",
    "text": "Proof 2\nApologies; you’ll have to do several “right clicks” until you reach the next slide.\nLet’s begin by manipulating the term we’d like to minimize: \\[\\begin{align}\n\\mathbb{E}[(\\theta - a)^2 | \\mathbf{y}] &= \\mathbb{E}[( \\color{red}{(} \\theta - \\mathbb{E}[\\theta | \\mathbf{y}]\\color{red}{)} + \\color{red}{(}\\mathbb{E}[\\theta | \\mathbf{y}]  - a\\color{red}{)})^2 | \\mathbf{y}] \\\\\n&\\class{fragment}{\\overset{\\text{FOIL}}{=} \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}] + 2\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])(\\mathbb{E}[\\theta | \\mathbf{y}]-a) | \\mathbf{y}] + \\mathbb{E}[(\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2 | \\mathbf{y}]} \\\\\n&\\class{fragment}{=\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}] + 2\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])\\color{orange}{(\\mathbb{E}[\\theta | \\mathbf{y}]-a)} | \\mathbf{y}] + \\mathbb{E}[\\color{orange}{(\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2} | \\mathbf{y}]} \\\\\n& \\class{fragment}{\\text{Note: }  \\color{orange}{\\mathbb{E}[\\theta | \\mathbf{y}]} \\text{ and } \\color{orange}{a} \\text{ are constant w.r.t } \\mathbf{y}}  \\\\\n&\\class{fragment}{=\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}]  + 2 \\color{orange}{(\\mathbb{E}[\\theta | \\mathbf{y}]-a)}\\color{purple}{\\mathbb{E}[\\theta - \\mathbb{E}[\\theta | \\mathbf{y}] | \\mathbf{y}]}+ \\color{orange}{(\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2}} \\\\\n&\\class{fragment}{=\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}]  + 2 (\\mathbb{E}[\\theta | \\mathbf{y}]-a) \\color{purple}{( \\mathbb{E}[\\theta | \\mathbf{y}] - \\mathbb{E}[\\theta | \\mathbf{y}])} + (\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2} \\\\\n&\\class{fragment}{=\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}]  + (\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2}\n\\end{align}\n\\]"
  },
  {
    "objectID": "handouts/bayes_est.html#theorem-1",
    "href": "handouts/bayes_est.html#theorem-1",
    "title": "Bayes estimator proofs",
    "section": "Theorem",
    "text": "Theorem\nUnder absolute loss \\(L(\\theta, a) = |\\theta - a|\\), a Bayes estimator for \\(\\theta\\) is any posterior median of \\(\\theta\\).\n\nThat is, a Bayes estimator is a function \\(\\delta(\\mathbf{y}) \\equiv m\\) such that \\(\\text{Pr}(\\theta \\leq m | \\mathbf{y}) \\geq \\frac{1}{2}\\) and \\(\\text{Pr}(\\theta \\geq m | \\mathbf{y}) \\geq \\frac{1}{2}\\).\nNote that when \\(\\theta\\) is continuous, there exists a single median."
  },
  {
    "objectID": "handouts/bayes_est.html#proof-2-cont.",
    "href": "handouts/bayes_est.html#proof-2-cont.",
    "title": "Bayes estimator proofs",
    "section": "Proof 2 (cont.)",
    "text": "Proof 2 (cont.)\nThus, \\[\n\\underset{a}{\\text{argmin}} \\ \\mathbb{E}[(\\theta - a)^2 | \\mathbf{y}] = \\underset{a}{\\text{argmin}} \\ \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}]  + (\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2\n\\]\n\nThe first term does not depend on \\(a\\), so cannot do anything about minimizing w.r.t \\(a\\)\nThus, focus on minimizing the second term. Minimized when \\(a = \\mathbb{E}[\\theta | \\mathbf{y}]\\).\nThus, the Bayes estimate under squared loss is the posterior mean. \\(\\tiny{\\square}\\)"
  },
  {
    "objectID": "handouts/bayes_est.html#temp",
    "href": "handouts/bayes_est.html#temp",
    "title": "Bayes estimator proofs",
    "section": "temp",
    "text": "temp"
  },
  {
    "objectID": "handouts/bayes_est.html#incremental-derivation",
    "href": "handouts/bayes_est.html#incremental-derivation",
    "title": "Bayes estimator proofs",
    "section": "Incremental Derivation",
    "text": "Incremental Derivation\n\\[\n\\begin{align}\n\\mathbb{E}[(\\theta - a)^2 \\mid \\mathbf{y}] &= \\mathbb{E}[\\big( \\color{red}{(} \\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}]\\color{red}{)} + \\color{red}{(}\\mathbb{E}[\\theta \\mid \\mathbf{y}]  - a\\color{red}{)}\\big)^2 \\mid \\mathbf{y}]\n\\end{align}\n\\]\n.fragment[ \\[\n\\begin{align}\n&= \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])^2 \\mid \\mathbf{y}] \\\\\n&\\quad + 2\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a) \\mid \\mathbf{y}] \\\\\n&\\quad + \\mathbb{E}[(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)^2 \\mid \\mathbf{y}]\n\\end{align}\n\\]]\n.fragment[ \\[\n\\begin{align}\n&= \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])^2 \\mid \\mathbf{y}] \\\\\n&\\quad + 2\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])\\color{orange}{(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)} \\mid \\mathbf{y}] \\\\\n&\\quad + \\mathbb{E}[\\color{orange}{(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)^2} \\mid \\mathbf{y}]\n\\end{align}\n\\]]\n.fragment[ &gt; Note: () and () are constant with respect to ()]\n.fragment[ \\[\n\\begin{align}\n&= \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])^2 \\mid \\mathbf{y}] \\\\\n&\\quad + 2 \\color{orange}{(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)} \\color{purple}{\\mathbb{E}[\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}] \\mid \\mathbf{y}]} \\\\\n&\\quad + \\color{orange}{(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)^2}\n\\end{align}\n\\]]\n.fragment[ \\[\n\\begin{align}\n&= \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])^2 \\mid \\mathbf{y}] \\\\\n&\\quad + 2 (\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a) \\color{purple}{( \\mathbb{E}[\\theta \\mid \\mathbf{y}] - \\mathbb{E}[\\theta \\mid \\mathbf{y}])} \\\\\n&\\quad + (\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)^2\n\\end{align}\n\\]]\n.fragment[ \\[\n\\begin{align}\n&= \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])^2 \\mid \\mathbf{y}] + (\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)^2\n\\end{align}\n\\]]"
  },
  {
    "objectID": "code/post_pred_checks.html",
    "href": "code/post_pred_checks.html",
    "title": "Simulating posterior and posterior predictive distributions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(MASS)\ndata(\"Aids2\")\nFrom the Aids2 dataset from MASS, we have data on patients diagnosed with AIDS in Australia before 1 July 1991. In particular, we are interested in the outcome status of each individual at the end of the observation (alive A or dead D). We also have data on state of origin:\n# A tibble: 4 × 5\n  state     A     D     n death_rate\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n1 NSW     664  1116  1780      0.627\n2 Other   107   142   249      0.570\n3 QLD      78   148   226      0.655\n4 VIC     233   355   588      0.604"
  },
  {
    "objectID": "code/post_pred_checks.html#posterior-predictive-check",
    "href": "code/post_pred_checks.html#posterior-predictive-check",
    "title": "Simulating posterior and posterior predictive distributions",
    "section": "Posterior predictive check",
    "text": "Posterior predictive check\nNow, let’s do some posterior predictive checks (PPC). One thing I might be wondering is if my assumption of iid across the 4 states is reasonble.\nFirst, let me simulate \\(R = 1000\\) new datasets of size 2843 from the PPD. For each dataset \\(r\\), I can assign state values according the observed data because I assumed state doesn’t matter (i.e. state and death status are independent).\n\nset.seed(412)\n# posterior predictive check\nR &lt;- 1000\ndf_ls &lt;- list()\nfor(r in 1:R){\n  theta_samp &lt;- rbeta(1, a + sum_y, b + n - sum_y)\n  ystar_samps &lt;- rbinom(n, size = 1, prob = theta_samp)\n  df_ls[[r]] &lt;- data.frame(status = ystar_samps) |&gt;\n    add_column(state = Aids2$state)\n}\n# Random sample of three rows in first simulated dataset:\ndf_ls[[1]] |&gt;\n  sample_n(3)\n\n  status state\n1      0   NSW\n2      1   NSW\n3      0   NSW\n\n\nVisually, we can look at how the distribution of deaths varies across the states in some of the simulated datasets:\n\n\n\n\n\n\n\n\n\nLooks pretty similar to the plot of the original data above, but humans like to see what they want to see.\nFor a more robust PPC, let’s consider the following test function \\(T(\\mathbf{y})\\): the ratio of the highest death rate and the lowest death rate among the four states.\nIn the observed data, this is 0.655/0.57 = 1.148. Let’s now obtain the values of the test function for the 1000 simualated datasets:\n\nppd_T &lt;- rep(NA, R)\nfor(r in 1:R){\n  temp &lt;- df_ls[[r]] |&gt;\n    group_by(state) |&gt;\n    summarise(death_rate = mean(status))\n  ppd_T[r] &lt;- max(temp$death_rate)/min(temp$death_rate)\n}\n\n\n\n\n\n\n\n\n\n\nThe posterior predictive probability of a ratio as or more extreme than the observed \\(T(\\mathbf{y}) = 1.148\\) is:\n\n# monte carlo approximation\npost_prob_T &lt;- mean(ppd_T &gt;= obs_T)\n\n\\[\\text{Pr}(T(\\mathbf{y}^{*} \\geq T(\\mathbf{y})) | \\mathbf{y}) = 0.088\\] This says that in 88 of the 1000 simulated datasets, I saw a test ratio value as or more extreme that what I observed in real life. To me, that seems quite low. An ideal scenario would have this posterior predictive probability be close to 0.50. Thus, it seems like maybe I should not treat the data as coming from one giant state, and instead allow for variation on the four states. That’s coming in a few weeks…"
  },
  {
    "objectID": "homework/412_hw2_r.html",
    "href": "homework/412_hw2_r.html",
    "title": "STAT 412: Problem Set 2 (R)",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)\nprussian1 &lt;- read_csv(\"../handouts/prussianHorses1.csv\")\nprussian2 &lt;- read_csv(\"../handouts/prussianHorses2.csv\")\n\n\na &lt;- 1\nb &lt;- 2\nqgamma(0.95, a,b)\n\n[1] 1.497866\n\ny &lt;- prussian1$deaths\nn &lt;- length(prussian1$deaths)\ntheta_seq &lt;- seq(0.1, 5, 0.05)\nprior &lt;- dgamma(theta_seq, a, b)\na_post &lt;- a + sum(y)\nb_post &lt;- b + n\npost &lt;- dgamma(theta_seq, a_post, b_post)\nbayes_df &lt;- data.frame(theta = theta_seq, prior = prior, post = post) |&gt;\n  pivot_longer(cols= 2:3, names_to = \"distribution\", values_to = \"density\")\nbayes_df |&gt;\n  ggplot(aes(x = theta, y = density, col = distribution)) + \n  geom_line()\n\n\n\n\n\n\n\npost_prob &lt;-pgamma(0.5, a_post, b_post)\n\nThe posterior probability \\(\\text{Pr}(\\theta  &lt; 0.5 = \\mathbf{y}) = 0.0487284\\). Since this is relatively small, I would say that there is not strong evidence to suggest that the average rate of deaths by horsekick in a given year and cavalary is less than 0.5.\n\nset.seed(2)\ny_new &lt;- rnbinom(220, size = a_post, prob = (b_post)/(b_post + 1))\np1 &lt;- data.frame(y_new) |&gt;\n  ggplot(aes(x = y_new)) + \n  geom_bar() +\n  labs(x = \"Deaths\", title = \"Draws from posterior predictive distribution\")\n\np2 &lt;- prussian2 |&gt;\n  ggplot(aes(x = deaths)) +\n  geom_bar() +\n  labs(x = \"Deaths\", title = \"Held-out observations\")\n\np1 + p2"
  },
  {
    "objectID": "code/post_pred_checks.html#data",
    "href": "code/post_pred_checks.html#data",
    "title": "Simulating posterior and posterior predictive distributions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(MASS)\ndata(\"Aids2\")\n\nFrom the Aids2 dataset from MASS, we have data on patients diagnosed with AIDS in Australia before 1 July 1991. In particular, we are interested in the outcome status of each individual at the end of the observation (alive A or dead D). We also have data on state of origin:\n\n\n# A tibble: 4 × 5\n  state     A     D     n death_rate\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n1 NSW     664  1116  1780      0.627\n2 Other   107   142   249      0.570\n3 QLD      78   148   226      0.655\n4 VIC     233   355   588      0.604"
  },
  {
    "objectID": "code/post_pred_checks.html#model",
    "href": "code/post_pred_checks.html#model",
    "title": "Simulating posterior and posterior predictive distributions",
    "section": "Model",
    "text": "Model\nThe response is binary, so a Bernoulli sampling model could work here. While morbid, let us consider death as success. I’ll assume that individuals are conditionally iid given the probability of death, no matter the state:\n\\[Y_{i} | \\theta \\overset{iid}{\\sim} \\text{Bern}(\\theta) \\qquad i = 1,\\ldots, 2843\\] For my prior, I don’t know much about death rates due to AIDS, so I will use a rather uninformative prior:\n\\[\\theta \\sim \\text{Beta}(2,2)\\] Because \\(n\\) is so large, I expect my posterior to be guided almost fully by the observed data.\nUnder this sampling model and prior, we know the posterior distribution of \\(\\theta\\) and the PPD for a new data point \\(Y^{*}\\) exactly. However, we will obtain samples via simulation from the posterior and PPD instead."
  },
  {
    "objectID": "code/post_pred_checks.html#simulate-posterior-and-posterior-predictive-distributions",
    "href": "code/post_pred_checks.html#simulate-posterior-and-posterior-predictive-distributions",
    "title": "Simulating posterior and posterior predictive distributions",
    "section": "Simulate posterior and posterior predictive distributions",
    "text": "Simulate posterior and posterior predictive distributions\nIn particular, I will obtain \\(S = 5000\\) samples from each distribution:\n\nset.seed(412)\nsum_y &lt;- sum(Aids2$status == \"D\")\nn &lt;- nrow(Aids2)\na &lt;- 2\nb &lt;- 2\n\nS &lt;- 5000\na_star &lt;- a + sum_y\nb_star &lt;- b + n - sum_y\n\n### OPTION 1: for loop\ntheta_samps &lt;- rep(NA, S) \nystar_samps &lt;- rep(NA, S)\nfor(s in 1:S){\n  # sample theta from posterior\n  theta &lt;- rbeta(1, a_star, b_star)\n  \n  # sample y* from data model\n  ystar &lt;- rbinom(1, size = 1, prob = theta)\n  \n  # store\n  theta_samps[s] &lt;- theta\n  ystar_samps[s] &lt;- ystar\n}\n\n### OPTION 2: leverage vectorized language\ntheta_samps &lt;- rbeta(S, a_star, b_star)\nystar_samps &lt;- rbinom(S, size = 1, prob = theta_samps)\n\nYou could run the following code to see how well the simulations approximate the exact distributions:\n\n## Compare simulated vs exact posterior quantities\n# exact posterior mean\na_star / (a_star + b_star)\n# approximate posterior mean\nmean(theta_samps)\n\n# quantile-based 95% posterior credible interval: exact\nqbeta(c(0.025, 0.975), a_star, b_star)\n# quantile-based 95% posterior credible interval: Monte Carlo approximate\nquantile(theta_samps, c(0.025, 0.975))\n\n## Compare simulated PPD to exact\n# mean and var under exact PPD\na_star / (a_star + b_star)\n(a_star / (a_star + b_star)) * (1- (a_star / (a_star + b_star)))\n\n# approximate mean and var under simulated PPD\nmean(ystar_samps)\nvar(ystar_samps)\n\nWe would work with the theta_samps vector to answer questions about \\(\\theta\\). But let’s turn to model checking."
  },
  {
    "objectID": "code/hpd_code.html",
    "href": "code/hpd_code.html",
    "title": "Code for HPD",
    "section": "",
    "text": "The density() function in R estimates the density of a random variable based off an iid sample from the density. It works by chopping up the “x-axis” or support of the density (i.e. the observed range from the iid sample) into small pieces, called coordinates. The function then estimates the density at each one of those coordinates based on the iid sample. The coordinates are the x output from density(), and the estimated density values are the y output from the function. These are accessed accessed via the $ symbol when the returned value from the function is stored as a variable (dens below).\n\n# theta_samps: vector of random samples of theta\n# cred_mass: gamma (e.g. 0.95)\n# grid_size: the bandwidth/number of coordinates used to estimate density\nget_hpd &lt;- function(theta_samps, cred_mass, grid_size = 10000) {\n  # Estimate true posterior density of theta using kernel density estimation.\n  dens &lt;- density(theta_samps, n = grid_size)\n  \n  dx &lt;- diff(dens$x[1:2])  \n  \n  ord &lt;- order(dens$y, decreasing = TRUE)\n  \n  cum_prob &lt;- cumsum(dens$y[ord]) * dx\n  \n  id &lt;- min(which(cum_prob &gt;= cred_mass))\n  dens_cutoff &lt;- dens$y[ord][id]\n  \n  inside &lt;- dens$y &gt;= dens_cutoff\n  \n  hpd_regions &lt;- list() \n  in_region &lt;- FALSE\n  for (i in seq_along(inside)) {\n    if (inside[i] && !in_region) { \n      start &lt;- dens$x[i] \n      in_region &lt;- TRUE \n    } else if ( (!inside[i]) & in_region) { \n      end &lt;- dens$x[i - 1]\n      hpd_regions[[length(hpd_regions) + 1]] &lt;- c(start, end)\n      in_region &lt;- FALSE \n    }\n  }\n  \n  if (in_region) {\n    hpd_regions[[length(hpd_regions) + 1]] &lt;- c(start, dens$x[length(dens$x)])\n  }\n  \n  return(hpd_regions)\n}"
  },
  {
    "objectID": "code/gibbs_normal_model.html",
    "href": "code/gibbs_normal_model.html",
    "title": "Gibbs sampler: Normal Model",
    "section": "",
    "text": "We have data on the amount of time (in hours) a sample of students from a high school spent studying during a two-day reading period before exams. We will model the data as:\n\\[ \\begin{align*}\nY_{i} \\mid \\theta, \\sigma^2 &\\overset{iid}{\\sim} N(\\theta, \\sigma^2) \\qquad i = 1,\\ldots,n \\\\\n\\theta &\\sim N(\\mu_{0}, \\sigma_{0}^2) \\\\\n\\phi &\\sim \\text{Gamma}(a_{0}, b_{0})\n\\end{align*}\\]\nwhere \\(\\phi = \\frac{1}{\\sigma^2}\\). We have to fill out our priors for \\(\\theta\\) and \\(\\phi\\) to fully specify the statistical model before seeing the data. Let’s do that together!\n\nmu_0 &lt;- 5\ns2_0 &lt;- 4\na_0 &lt;- 5\nb_0 &lt;- 15\n\n# demonstrate how I might do prior elicitation\nhist(sqrt(1/(rgamma(10000, a_0, b_0))))"
  },
  {
    "objectID": "code/gibbs_normal_model.html#prior-specification",
    "href": "code/gibbs_normal_model.html#prior-specification",
    "title": "Gibbs sampler: Normal Model",
    "section": "",
    "text": "We have data on the amount of time (in hours) a sample of students from a high school spent studying during a two-day reading period before exams. We will model the data as:\n\\[ \\begin{align*}\nY_{i} \\mid \\theta, \\sigma^2 &\\overset{iid}{\\sim} N(\\theta, \\sigma^2) \\qquad i = 1,\\ldots,n \\\\\n\\theta &\\sim N(\\mu_{0}, \\sigma_{0}^2) \\\\\n\\phi &\\sim \\text{Gamma}(a_{0}, b_{0})\n\\end{align*}\\]\nwhere \\(\\phi = \\frac{1}{\\sigma^2}\\). We have to fill out our priors for \\(\\theta\\) and \\(\\phi\\) to fully specify the statistical model before seeing the data. Let’s do that together!\n\nmu_0 &lt;- 5\ns2_0 &lt;- 4\na_0 &lt;- 5\nb_0 &lt;- 15\n\n# demonstrate how I might do prior elicitation\nhist(sqrt(1/(rgamma(10000, a_0, b_0))))"
  },
  {
    "objectID": "code/gibbs_normal_model.html#data",
    "href": "code/gibbs_normal_model.html#data",
    "title": "Gibbs sampler: Normal Model",
    "section": "Data",
    "text": "Data\nNow we get to see the data:\n\ny &lt;- readRDS(\"~/Desktop/STAT 412/data/school1.Rda\")\ndata.frame(hours = y) |&gt;\n  ggplot(aes(x = hours)) +\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\nybar &lt;- mean(y)\nn &lt;- length(y)"
  },
  {
    "objectID": "code/gibbs_normal_model.html#gibbs-sampler",
    "href": "code/gibbs_normal_model.html#gibbs-sampler",
    "title": "Gibbs sampler: Normal Model",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\nset.seed(412)\n### GIBBS sampler\nS &lt;- 5000\n\n# will generate a bunch of samples, need to store them somewhere!\nTHETA &lt;- rep(NA, S)\nS2 &lt;- rep(NA, S)\n  \n# initialize the sampler \n# since sampling precision first, initialize value of theta\ntheta &lt;- 2\n\nfor (s in 1:S){\n  # sample phi from full conditional\n  b_n &lt;- 0.5*sum( (y - theta)^2) + b_0\n  a_n &lt;- 0.5*n + a_0\n  phi &lt;- rgamma(1, a_n, b_n)\n  \n  # convert phi to s2\n  s2 &lt;- 1/phi\n  \n  # sample theta from full conditional\n  s2_n &lt;- 1/(1/s2_0 + n / s2)\n  mu_n &lt;- s2_n * (n * ybar/s2 + mu_0/s2_0)\n  theta &lt;- rnorm(1, mu_n, sqrt(s2_n))\n\n  # store my samples\n  THETA[s] &lt;- theta\n  S2[s] &lt;- s2\n}\n\n\nSeeing the sampling"
  },
  {
    "objectID": "code/gibbs_normal_model.html#traceplots-and-posterior-quantities",
    "href": "code/gibbs_normal_model.html#traceplots-and-posterior-quantities",
    "title": "Gibbs sampler: Normal Model",
    "section": "Traceplots and posterior quantities",
    "text": "Traceplots and posterior quantities\nTrace plots of markov chains: line plot of simulated draws of parameter against iteration. In theory, the Metropolis and Gibbs sampling algorithms will produce simulated draws that converge to the posterior distribution of interest. But in typical practice, it may take a number of iterations before the simulation values are close to the posterior distribution. So in general it is recommended that one run the algorithm for a number of “burn-in” iterations before one collects iterations for inference.\nWe want nice “caterpillar” looking traceplots. This would suggest convergence!\n\nplot(THETA, xlab = \"Iteration\", ylab = \"theta\", type = \"l\", main = \"Traceplot\")\n\n\n\n\n\n\n\nplot(S2, xlab = \"Iteration\", ylab = \"sigma2\", type = \"l\", main = \"Traceplot\")\n\n\n\n\n\n\n\n\nThat crazy large sampled \\(\\sigma^2\\) at the beginning is a product of intialization and exploring the state space; we almost surely haven’t entered stationary distribution yet. Let’s go ahead and throw away half of our values as burn-in.\n\n# throw away some values as burn-in\nTHETA &lt;- THETA[-c(1:round(S/2))]\nS2 &lt;- S2[-c(1:round(S/2))]\n\nLet’s re-examine the traceplots post burn-in. From experience, these are beautiful looking traceplots!\n\npost_df &lt;- data.frame(theta = THETA, s2 = S2)\npost_df |&gt;\n  mutate(iteration = row_number()) |&gt;\n  pivot_longer(cols = 1:2, names_to = \"param\", values_to = \"value\") |&gt;\n  ggplot(aes(x = iteration, y = value)) +\n  geom_line() +\n  facet_wrap(~param, scales = \"free\") +\n  labs(\"Traceplots after burn-in\")\n\n\n\n\n\n\n\n\nWe should only estimate quantities after we’ve converged (i.e. after burn-in). In this case, we can obtain estimates of posterior quantities like usual:\n\n# estimated posterior mean of theta\nmean(THETA)\n\n[1] 8.967482\n\n# estimated posterior mean of sigma\nmean(sqrt(S2))\n\n[1] 3.493671\n\n## estimated CIs\nquantile(THETA, c(0.025, 0.975))\n\n     2.5%     97.5% \n 7.629652 10.265034 \n\nquantile(sqrt(S2), c(0.025, 0.975))\n\n    2.5%    97.5% \n2.787186 4.464605"
  },
  {
    "objectID": "code/gibbs_normal_model.html#diving-into-joint-density",
    "href": "code/gibbs_normal_model.html#diving-into-joint-density",
    "title": "Gibbs sampler: Normal Model",
    "section": "Diving into joint density",
    "text": "Diving into joint density\nWe can obtain a contour plot of the joint density using the samples obtained from Gibbs:\n\n\n\n\n\n\n\n\n\nWe can see how we “marginalize out” one parameter to get the posterior density of the other. To obtain marginals, we simply grab/isolate the parameter of interest.\n\n\n\n\n\n\n\n\n\n\nhist(THETA, main = \"Marginal posterior histogram\", xlab = \"theta\")\n\n\n\n\n\n\n\nhist(S2, main = \"Marginal posterior histogram\", xlab = \"s2\")"
  },
  {
    "objectID": "code/gibbs_normal_model.html#chains",
    "href": "code/gibbs_normal_model.html#chains",
    "title": "Gibbs sampler: Normal Model",
    "section": "Chains",
    "text": "Chains\nIn this example, we have illustrated running a single “chain” where one has a single starting value and we simulated draws over \\(S\\) iterations. It is possible that the behavior of the MCMC sample will depend on the choice of starting value. So a general recommendation is to run the MCMC algorithm several times using different starting values. In this case, one will have multiple MCMC chains and combine them.\nWhen we code samplers by hand in this course, we will almost always run a single chain to make our lives easier. However, when we turn to software that implements the MCMC for us, it’s very easy to specify multiple chains!"
  },
  {
    "objectID": "code/gibbs_normal_model_sampler_code.html",
    "href": "code/gibbs_normal_model_sampler_code.html",
    "title": "Gibbs Sampler: Normal Model",
    "section": "",
    "text": "set.seed(412)\n### GIBBS sampler\nS &lt;- 5000\n\n# will generate a bunch of samples, need to store them somewhere!\nTHETA &lt;- rep(NA, S)\nS2 &lt;- rep(NA, S)\n  \n# initialize the sampler \n# since sampling precision first, initialize value of theta\ntheta &lt;- 2\n\nfor (s in 1:S){\n  # sample phi from full conditional\n  b_n &lt;- 0.5*sum((y - theta)^2) + b_0\n  a_n &lt;- 0.5*n + a_0\n  phi &lt;- rgamma(1, a_n, b_n)\n  \n  # convert phi to s2\n  s2 &lt;- 1/phi\n  \n  # sample theta from full conditional\n  s2_n &lt;- 1/(1/s2_0 + n / s2)\n  mu_n &lt;- s2_n * (n * ybar/s2 + mu_0/s2_0)\n  theta &lt;- rnorm(1, mu_n, sqrt(s2_n))\n\n  # store my samples\n  THETA[s] &lt;- theta\n  S2[s] &lt;- s2\n}"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html",
    "href": "code/gibbs_normal_model_walkthrough.html",
    "title": "Gibbs sampler: Normal Model",
    "section": "",
    "text": "We have data on the amount of time (in hours) a sample of students from a high school spent studying during a two-day reading period before exams. We will model the data as:\n\\[ \\begin{align*}\nY_{i} \\mid \\theta, \\sigma^2 &\\overset{iid}{\\sim} N(\\theta, \\sigma^2) \\qquad i = 1,\\ldots,n \\\\\n\\theta &\\sim N(\\mu_{0}, \\sigma_{0}^2) \\\\\n\\phi &\\sim \\text{Gamma}(a_{0}, b_{0})\n\\end{align*}\\]\nwhere \\(\\phi = \\frac{1}{\\sigma^2}\\). We have to fill out our priors for \\(\\theta\\) and \\(\\phi\\) to fully specify the statistical model before seeing the data. Let’s do that together!\n\nmu_0 &lt;- 6\ns2_0 &lt;- 2.5\na_0 &lt;- 5\nb_0 &lt;- 100\n\n# demonstrate how I might do prior elicitation\n# hist(sqrt(1/(rgamma(10000, a_0, b_0))))"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html#prior-specification",
    "href": "code/gibbs_normal_model_walkthrough.html#prior-specification",
    "title": "Gibbs sampler: Normal Model",
    "section": "",
    "text": "We have data on the amount of time (in hours) a sample of students from a high school spent studying during a two-day reading period before exams. We will model the data as:\n\\[ \\begin{align*}\nY_{i} \\mid \\theta, \\sigma^2 &\\overset{iid}{\\sim} N(\\theta, \\sigma^2) \\qquad i = 1,\\ldots,n \\\\\n\\theta &\\sim N(\\mu_{0}, \\sigma_{0}^2) \\\\\n\\phi &\\sim \\text{Gamma}(a_{0}, b_{0})\n\\end{align*}\\]\nwhere \\(\\phi = \\frac{1}{\\sigma^2}\\). We have to fill out our priors for \\(\\theta\\) and \\(\\phi\\) to fully specify the statistical model before seeing the data. Let’s do that together!\n\nmu_0 &lt;- 6\ns2_0 &lt;- 2.5\na_0 &lt;- 5\nb_0 &lt;- 100\n\n# demonstrate how I might do prior elicitation\n# hist(sqrt(1/(rgamma(10000, a_0, b_0))))"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html#data",
    "href": "code/gibbs_normal_model_walkthrough.html#data",
    "title": "Gibbs sampler: Normal Model",
    "section": "Data",
    "text": "Data\nNow we get to see the data:\n\ny &lt;- readRDS(\"~/Desktop/STAT 412/data/school1.Rda\")\ndata.frame(hours = y) |&gt;\n  ggplot(aes(x = hours)) +\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\nybar &lt;- mean(y)\nn &lt;- length(y)"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html#gibbs-sampler",
    "href": "code/gibbs_normal_model_walkthrough.html#gibbs-sampler",
    "title": "Gibbs sampler: Normal Model",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\nset.seed(412)\n### GIBBS sampler\nS &lt;- 5000\n\n# will generate a bunch of samples, need to store them somewhere!\nTHETA &lt;- rep(NA, S)\nS2 &lt;- rep(NA, S)\n  \n# initialize the sampler \n# since sampling precision first, initialize value of theta\ntheta &lt;- 2\n\nfor (s in 1:S){\n  # sample phi from full conditional\n  b_n &lt;- 0.5*sum( (y - theta)^2) + b_0\n  a_n &lt;- 0.5*n + a_0\n  phi &lt;- rgamma(1, a_n, b_n)\n  \n  # convert phi to s2\n  s2 &lt;- 1/phi\n  \n  # sample theta from full conditional\n  s2_n &lt;- 1/(1/s2_0 + n / s2)\n  mu_n &lt;- s2_n * (n * ybar/s2 + mu_0/s2_0)\n  theta &lt;- rnorm(1, mu_n, sqrt(s2_n))\n\n  # store my samples\n  THETA[s] &lt;- theta\n  S2[s] &lt;- s2\n}\n\n\nSeeing the sampling"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html#traceplots-and-posterior-quantities",
    "href": "code/gibbs_normal_model_walkthrough.html#traceplots-and-posterior-quantities",
    "title": "Gibbs sampler: Normal Model",
    "section": "Traceplots and posterior quantities",
    "text": "Traceplots and posterior quantities\nTrace plots of markov chains: line plot of simulated draws of parameter against iteration. In theory, the Metropolis and Gibbs sampling algorithms will produce simulated draws that converge to the posterior distribution of interest. But in typical practice, it may take a number of iterations before the simulation values are close to the posterior distribution. So in general it is recommended that one run the algorithm for a number of “burn-in” iterations before one collects iterations for inference.\nWe want nice “caterpillar” looking traceplots. This would suggest convergence!\n\nplot(THETA, xlab = \"Iteration\", ylab = \"theta\", type = \"l\", main = \"Traceplot\")\n\n\n\n\n\n\n\nplot(S2, xlab = \"Iteration\", ylab = \"sigma2\", type = \"l\", main = \"Traceplot\")\n\n\n\n\n\n\n\n\nThat crazy large sampled \\(\\sigma^2\\) at the beginning is a product of intialization and exploring the state space; we almost surely haven’t entered stationary distribution yet. Let’s go ahead and throw away half of our values as burn-in.\n\n# throw away some values as burn-in\nTHETA &lt;- THETA[-c(1:round(S/2))]\nS2 &lt;- S2[-c(1:round(S/2))]\n\nLet’s re-examine the traceplots post burn-in. From experience, these are beautiful looking traceplots!\n\npost_df &lt;- data.frame(theta = THETA, s2 = S2)\npost_df |&gt;\n  mutate(iteration = row_number()) |&gt;\n  pivot_longer(cols = 1:2, names_to = \"param\", values_to = \"value\") |&gt;\n  ggplot(aes(x = iteration, y = value)) +\n  geom_line() +\n  facet_wrap(~param, scales = \"free\") +\n  labs(\"Traceplots after burn-in\")\n\n\n\n\n\n\n\n\nWe should only estimate quantities after we’ve converged (i.e. after burn-in). In this case, we can obtain estimates of posterior quantities like usual:\n\n# estimated posterior mean of theta\nmean(THETA)\n\n[1] 8.693283\n\n# estimated posterior mean of sigma\nmean(sqrt(S2))\n\n[1] 4.19726\n\n## estimated CIs\nquantile(THETA, c(0.025, 0.975))\n\n     2.5%     97.5% \n 7.177723 10.142940 \n\nquantile(sqrt(S2), c(0.025, 0.975))\n\n    2.5%    97.5% \n3.355402 5.368383"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html#diving-into-joint-density",
    "href": "code/gibbs_normal_model_walkthrough.html#diving-into-joint-density",
    "title": "Gibbs sampler: Normal Model",
    "section": "Diving into joint density",
    "text": "Diving into joint density\nWe can obtain a contour plot of the joint density using the samples obtained from Gibbs:\n\n\n\n\n\n\n\n\n\nWe can see how we “marginalize out” one parameter to get the posterior density of the other. To obtain marginals, we simply grab/isolate the parameter of interest.\n\n\n\n\n\n\n\n\n\n\nhist(THETA, main = \"Marginal posterior histogram\", xlab = \"theta\")\n\n\n\n\n\n\n\nhist(S2, main = \"Marginal posterior histogram\", xlab = \"s2\")"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html#chains",
    "href": "code/gibbs_normal_model_walkthrough.html#chains",
    "title": "Gibbs sampler: Normal Model",
    "section": "Chains",
    "text": "Chains\nIn this example, we have illustrated running a single “chain” where one has a single starting value and we simulated draws over \\(S\\) iterations. It is possible that the behavior of the MCMC sample will depend on the choice of starting value. So a general recommendation is to run the MCMC algorithm several times using different starting values. In this case, one will have multiple MCMC chains and combine them.\nWhen we code samplers by hand in this course, we will almost always run a single chain to make our lives easier. However, when we turn to software that implements the MCMC for us, it’s very easy to specify multiple chains!"
  },
  {
    "objectID": "handouts/constrained_normal.html",
    "href": "handouts/constrained_normal.html",
    "title": "Constrained Normal Sampling Code",
    "section": "",
    "text": "# n = number of random samples \n# mu = mean of normal \n# sd = std. dev. normal \n# a = lower constraint (can be -Inf or a real number) \n# b = upper constraint (can be Inf or a real number)\n\nrnorm_constrained &lt;- function(n, mu, sd, a, b){\n  u &lt;- runif(n, pnorm(a, mu,sd), pnorm(b, mu, sd)) \n  qnorm(u, mu, sd) \n}"
  },
  {
    "objectID": "code/mixture_normals.html",
    "href": "code/mixture_normals.html",
    "title": "Mixture of Normals",
    "section": "",
    "text": "The density we’re interest in sampling from is \\[f(\\theta) = 0.45 N\\left(\\theta; -3, \\frac{1}{4}\\right) + 0.10 N\\left(\\theta; 0, \\frac{1}{4}\\right) +0.45 N\\left(\\theta; 3, \\frac{1}{4}\\right)\\] This is a mixutre of three Normals. You’ve seen your first mixture distribution in this class on the previous homework!\nWe can easily evaluate this distribution at a grid of \\(\\theta\\) values.\n\nw_prob &lt;- c(0.45, 0.1, 0.45)\nmu_vec &lt;- c(-3, 0, 3)\nsigma_vec &lt;- c(0.5, 0.5,0.5)\n\n# true density\ntheta_vec &lt;- seq(-10, 10, 0.01)\nmixture_dens &lt;- rep(NA, length(theta_vec))\nfor(i in 1:length(theta_vec)){\n  mixture_dens[i] &lt;- sum(dnorm(theta_vec[i], mu_vec, sigma_vec)*w_prob)\n}"
  },
  {
    "objectID": "code/mixture_normals.html#mixture-of-normals-density",
    "href": "code/mixture_normals.html#mixture-of-normals-density",
    "title": "Mixture of Normals",
    "section": "",
    "text": "The density we’re interest in sampling from is \\[f(\\theta) = 0.45 N\\left(\\theta; -3, \\frac{1}{4}\\right) + 0.10 N\\left(\\theta; 0, \\frac{1}{4}\\right) +0.45 N\\left(\\theta; 3, \\frac{1}{4}\\right)\\] This is a mixutre of three Normals. You’ve seen your first mixture distribution in this class on the previous homework!\nWe can easily evaluate this distribution at a grid of \\(\\theta\\) values.\n\nw_prob &lt;- c(0.45, 0.1, 0.45)\nmu_vec &lt;- c(-3, 0, 3)\nsigma_vec &lt;- c(0.5, 0.5,0.5)\n\n# true density\ntheta_vec &lt;- seq(-10, 10, 0.01)\nmixture_dens &lt;- rep(NA, length(theta_vec))\nfor(i in 1:length(theta_vec)){\n  mixture_dens[i] &lt;- sum(dnorm(theta_vec[i], mu_vec, sigma_vec)*w_prob)\n}"
  },
  {
    "objectID": "code/mixture_normals.html#monte-carlo-sampling",
    "href": "code/mixture_normals.html#monte-carlo-sampling",
    "title": "Mixture of Normals",
    "section": "Monte Carlo sampling",
    "text": "Monte Carlo sampling\nIf we want to sample from this mixture distribution, we can use Monte Calo sampling! For \\(s = 1,\\ldots, S\\):\n\nSample a value \\(w^{(s)} = \\{1,2,3\\}\\) according to mixture weights \\(\\{0.45, 0.10, 0.45\\}\\)\nGiven the value of \\(w\\), sample \\(\\theta^{(s)}\\) from the corresponding Normal. For example:\n\n\\[\\theta^{(s)} | w^{(s)} = 1 \\sim N\\left (-3, \\frac{1}{4}\\right)\\] Note this is indeed Monte Carlo sampling because we are drawing independent random sample!\nLet’s draw 1000 MC samples. A normalized histogram of the samples is shown below, along with the true density overlaid in orange:\n\nset.seed(1)\n# monte carlo\nS &lt;- 1000\nd_samp &lt;- sample(1:3, S, replace = T, prob = w_prob)\nhist(rnorm(S, mu_vec[d_samp], sigma_vec[d_samp]), freq = F, breaks =\n       20, main = \"Mixture of Normals\", xlab = \"theta\")\nlines(theta_vec, mixture_dens, type = \"l\", col = \"orange\")"
  },
  {
    "objectID": "code/mixture_normals.html#mcmc-sampling",
    "href": "code/mixture_normals.html#mcmc-sampling",
    "title": "Mixture of Normals",
    "section": "MCMC sampling",
    "text": "MCMC sampling\nWe can also obtain samples from this distribution via Gibbs sampling (i.e. Markov Chain Monte Carlo)! Note that even though we’re not working with a posterior, we can still perform Gibbs Sampling if we have more than one parameter of interest. In this case, why don’t I iterate between 1) sampling a \\(\\theta\\) value from its full conditional and 2) sampling a mixture component \\(w\\) value from its full conditional? That is, for \\(s =1,\\ldots, S\\):\n\nSample \\(w^{(s)} \\sim f(w | \\theta^{s-1})\\)\nSample \\(\\theta^{(s)} \\sim f(\\theta | w^{(s)})\\)\n\nConvince yourself that this sampling scheme is different from the previous!\nWe already have the distrution for sampling in step 2. So we just need to derive the full conditional for \\(w\\)! Let’s do that together.\n\nGibbs sampler\nLet’s run our sampler for \\(S=5000\\) iterations, then take a look at the traceplot and marginal histogram for \\(\\theta\\). Note: even though we’re sampling \\(w\\)’s along the way, I don’t actually care about them (they are latent variables). So I don’t bother storing them!\n\nseed &lt;- 10\nS &lt;- 5000\nset.seed(seed)\ntheta &lt;- -2\nTHETA &lt;- rep(NA, S)\nfor(s in 1:S){\n  # sample w\n  prob_vec_unnorm &lt;- dnorm(theta, mu_vec, sigma_vec) * w_prob\n  prob_vec_norm &lt;- prob_vec_unnorm/sum(prob_vec_unnorm)\n  w &lt;- sample(1:3, size = 1, prob = prob_vec_norm)\n  \n  # sample theta\n  theta &lt;- rnorm(1, mu_vec[w], sigma_vec[w])\n  THETA[s] &lt;- theta\n}\npar(mfrow = c(1, 2))\nplot(THETA, type = \"l\", main = \"Traceplot\", xlab = \"iteration\")\nhist(THETA, freq = F, main = \"\", xlab = expression(theta))\n\n\n\n\n\n\n\n\nWhat are we noticing?\nWell, we did say that we should throw out the first chunk of iterations to burn-in. So let me throw out the first half and re-visualize:\n\npar(mfrow = c(1, 2))\nTHETA_burned &lt;- THETA[-c(1:(S/2))]\nplot(THETA_burned, type = \"l\",  main = \"Traceplot\", xlab = \"iteration (after burn-in)\")\nhist(THETA_burned, freq = F, main = \"\", xlab = expression(theta))\n\n\n\n\n\n\n\n\nYikes!! That sure is misleading!!!! After burn-in, it seems like my density has converged to a unimodal distribution. What’s happening here?\nLet’s talk about convergence, mixing, and autocorrelation!\n\n\nGibbs, take 2\nLet’s now run the chain a lot longer, this time for \\(S = 20000\\) iterations.\n\n\n\n\n\n\n\n\n\nThat’s much better! Of course, in real life, we don’t know what the desire (posterior) density should look like. But this goes to show that you should, if computationally feasible, run your chain for a long time and perhaps for many different \\(S\\) values."
  },
  {
    "objectID": "code/mixture_normals.html#functions-for-mcmc-diagnostics",
    "href": "code/mixture_normals.html#functions-for-mcmc-diagnostics",
    "title": "Mixture of Normals",
    "section": "Functions for MCMC diagnostics",
    "text": "Functions for MCMC diagnostics\n\nAutocorrelation\nThe R function for examining autocorrelation is acf(), and you pass in a vector of samples:\n\nacf(THETA_burned)\n\n\n\n\n\n\n\n\nThe horizontal blue linee give the values beyond which the autocorrelations are (statistically) significantly different from 0. So you’d like ACF values within the blue bands.\nIf you just want the sample lag \\(t\\) ACF value, you can access it as follows:\n\nacf_out &lt;- acf(THETA_burned, plot = T)\n\n\n\n\n\n\n\n# lag-0 is at index 1\n# lag-1 is at index 2\nacf_out$acf[2]\n\n[1] 0.9685008\n\n\n\n\nEffective sample size\nWe can obtain \\(n_{eff}\\) using the effectiveSize() function from the R package coda (install it first).\n\n# yikes\ncoda::effectiveSize(THETA_burned) \n\n    var1 \n6.235923"
  },
  {
    "objectID": "case_study.html",
    "href": "case_study.html",
    "title": "Case studies",
    "section": "",
    "text": "Your first case study will be performed in pairs! You should work together; don’t just divy up the work. A large part of modeling is bouncing ideas back and forth and checking in to make sure things “make sense”. Each group will submit 1 report in the form of both a .Rmd and a knitted/rendered PDF. These will be submitted to Canvas (not in-person) by beginning of class 10/15.\nThe description of the case study and the data are found here:\n\nDescription\nData:  ItalyMarriageRates \nOptional .Rmd template:  caseStudy1 \n\n\n\n\nEach group will give a brief 5 minute presentation of their work to the class on 10/15. Presentation order will be randomized.\nYour presentation should be accompanied by a brief set of slides. As we are all aware of the data, these slides should focus mostly on describing your statistical model and how you arrived at it, as well as results (with interpretation).\n\nThink about embedding LaTex equations and R plots for your statistical model. These should be presented in an effective manner.\nYour presentation of results should answer the research question in an effective manner. Does that mean you’ll present a table of posterior summary quantities, a plot of a posterior density, or something else?\n\nYou should be prepared to answer questions from the audience. You should also be prepared to ask questions of presenters.\nYou don’t need to submit your slides for this case study! Just be prepared to present from a laptop!\nThis presentation is worth 5 points, in addition to the 35 points of the written portion. However, the presentation will be graded on an individual basis, rather than the entire group. A general rubric is:\n\n1 point: slides effectively share statistical model.\n1 point: slides effectively share results that answer the research question(s)\n1 point: speaker appears practiced and shares speaking time.\n\n“Appearing practiced” also encompasses being as close to the 5-minute duration as possible!\nBecause this presentation is short and you’re so close to your model, you shouldn’t be reading off any sort of notes!\n\n1 point: speaker is able to thoughtfully answer questions from audience\n1 point: asks at least one interesting/thoughtful/relevant question to other speakers\n\n“Effective” is a broad term that I use to cover:\n\nPresenting equations/distributions/slides in a logical order\nHaving font (both text and on figures/tables) be large enough for the audience to see\nSlides have enough text to be generally understood without a speaker, but each slide’s content is elevated/made complete by the speaker’s additions/commentary\nMinimal errors in terms of spelling and typesetting"
  },
  {
    "objectID": "case_study.html#case-study-1",
    "href": "case_study.html#case-study-1",
    "title": "Case studies",
    "section": "",
    "text": "Your first case study will be performed in pairs! You should work together; don’t just divy up the work. A large part of modeling is bouncing ideas back and forth and checking in to make sure things “make sense”. Each group will submit 1 report in the form of both a .Rmd and a knitted/rendered PDF. These will be submitted to Canvas (not in-person) by beginning of class 10/15.\nThe description of the case study and the data are found here:\n\nDescription\nData:  ItalyMarriageRates \nOptional .Rmd template:  caseStudy1 \n\n\n\n\nEach group will give a brief 5 minute presentation of their work to the class on 10/15. Presentation order will be randomized.\nYour presentation should be accompanied by a brief set of slides. As we are all aware of the data, these slides should focus mostly on describing your statistical model and how you arrived at it, as well as results (with interpretation).\n\nThink about embedding LaTex equations and R plots for your statistical model. These should be presented in an effective manner.\nYour presentation of results should answer the research question in an effective manner. Does that mean you’ll present a table of posterior summary quantities, a plot of a posterior density, or something else?\n\nYou should be prepared to answer questions from the audience. You should also be prepared to ask questions of presenters.\nYou don’t need to submit your slides for this case study! Just be prepared to present from a laptop!\nThis presentation is worth 5 points, in addition to the 35 points of the written portion. However, the presentation will be graded on an individual basis, rather than the entire group. A general rubric is:\n\n1 point: slides effectively share statistical model.\n1 point: slides effectively share results that answer the research question(s)\n1 point: speaker appears practiced and shares speaking time.\n\n“Appearing practiced” also encompasses being as close to the 5-minute duration as possible!\nBecause this presentation is short and you’re so close to your model, you shouldn’t be reading off any sort of notes!\n\n1 point: speaker is able to thoughtfully answer questions from audience\n1 point: asks at least one interesting/thoughtful/relevant question to other speakers\n\n“Effective” is a broad term that I use to cover:\n\nPresenting equations/distributions/slides in a logical order\nHaving font (both text and on figures/tables) be large enough for the audience to see\nSlides have enough text to be generally understood without a speaker, but each slide’s content is elevated/made complete by the speaker’s additions/commentary\nMinimal errors in terms of spelling and typesetting"
  },
  {
    "objectID": "case_study/case_study_1_template.html",
    "href": "case_study/case_study_1_template.html",
    "title": "STAT 412: Case Study 1",
    "section": "",
    "text": "DELETE ALL THE TEXT PROVIDED BY BECKY BEFORE SUBMITTING!\n# if you need packages, load them here\n\n# the following line of code sets the size of the figures when knitting for all code chunks. \n# You can also change the figure size in each specific chunk's header\nknitr::opts_chunk$set(fig.height = 3, fig.width = 5)\n\n# load your data here\n# Once you've set your file path, change the R chunk header to eval = TRUE\nmarriage &lt;- read_csv()"
  },
  {
    "objectID": "case_study/case_study_1_template.html#model-description",
    "href": "case_study/case_study_1_template.html#model-description",
    "title": "STAT 412: Case Study 1",
    "section": "Model description",
    "text": "Model description\nClearly state your model (Address questions 1 and 2). I encourage you to type up your model in math notation using Latex. For example:\n\\(Y_{1}, \\ldots, Y_{n} |\\theta \\overset{iid}{\\sim} f(y | \\theta)\\)\nIf you do not know how, that’s okay! Just let me know and I will be happy to help or point you towards some resources."
  },
  {
    "objectID": "case_study/case_study_1_template.html#implementation",
    "href": "case_study/case_study_1_template.html#implementation",
    "title": "STAT 412: Case Study 1",
    "section": "Implementation",
    "text": "Implementation\nClearly state how you will implement your model. (Address question 3.) You should provide enough detail such that someone else could implement your analysis."
  },
  {
    "objectID": "case_study/case_study_1_template.html#gibbs-sampler",
    "href": "case_study/case_study_1_template.html#gibbs-sampler",
    "title": "STAT 412: Case Study 1",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\n# code for your gibbs sampler should go here\n# remember to only keep necessary code for submission!"
  },
  {
    "objectID": "case_study/case_study_1_template.html#results",
    "href": "case_study/case_study_1_template.html#results",
    "title": "STAT 412: Case Study 1",
    "section": "Results",
    "text": "Results\n\nDiagnostics\nFirst do some diagnostics to see if your chain has converged\n\n\nFindings\nProvide results with interpretation (Address question 4). You should alternate between R code and text!"
  },
  {
    "objectID": "case_study/case_study_1_template.html#conclusion",
    "href": "case_study/case_study_1_template.html#conclusion",
    "title": "STAT 412: Case Study 1",
    "section": "Conclusion",
    "text": "Conclusion\nGive a conclusion that answers the research question (Address question 5)."
  },
  {
    "objectID": "handouts/jeffreys_prior_normal.html",
    "href": "handouts/jeffreys_prior_normal.html",
    "title": "Jeffreys Prior",
    "section": "",
    "text": "Suppose I use Jeffrey’s prior for \\(\\theta\\), but I don’t use Jeffrey’s prior for \\(\\gamma = e^{\\theta}\\). Then if I obtain the posterior for \\(\\theta\\) and transform that posterior to \\(\\gamma\\) space, I’m not guaranteed to get the same posterior as if I’d started in \\(\\gamma\\)-space to begin with!\n\nlibrary(pracma)\n\n# Data: single observation\ny &lt;- 2.5\nsigma &lt;- 1\n\n# Grid in mu-space\nmu_vec &lt;- seq(-5, 5, length.out = 10000)\n\n# Likelihood in mu\nlik_mu &lt;- dnorm(y, mean = mu_vec, sd = sigma)\n\n# Prior 1: Jeffreys prior on mu: uniform (constant)\nprior_jeff_mu &lt;- rep(1, length(mu_vec))\n\n# Posterior in mu under Jeffreys prior for mu\npost_mu_jeff &lt;- lik_mu * prior_jeff_mu\npost_mu_jeff &lt;- post_mu_jeff / trapz(mu_vec, post_mu_jeff)\n\n# Transform parameter: gamma = exp(mu)\ngamma_vec &lt;- exp(mu_vec)\ndmu_dgamma &lt;- 1 / gamma_vec  # derivative of mu w.r.t gamma\n\n# Transform posterior to gamma-space\npost_gamma_from_mu &lt;- post_mu_jeff / dmu_dgamma  # divide by |dmu/dgamma| for pdf transform\npost_gamma_from_mu &lt;- post_gamma_from_mu / trapz(gamma_vec, post_gamma_from_mu)\n\n# flat prior on gamma (this is NOT jeffrey's prior for gamma)\nprior_flat_gamma &lt;- rep(1, length(gamma_vec))\n\n# Likelihood in gamma-space (transform mu to gamma)\nlik_gamma &lt;- dnorm(y, mean = log(gamma_vec), sd = sigma)\n\n\n# Posterior in gamma-space under flat prior for gamma\npost_gamma &lt;- lik_gamma * prior_flat_gamma\npost_gamma &lt;- post_gamma / trapz(gamma_vec, post_gamma)\n\n# Sampling function from mu posterior\ncdf_mu_jeff &lt;- cumtrapz(mu_vec, post_mu_jeff)\ncdf_mu_jeff &lt;- cdf_mu_jeff / max(cdf_mu_jeff)\nsample_mu &lt;- function(n) {\n  u &lt;- runif(n)\n  approx(cdf_mu_jeff, mu_vec, xout = u, rule = 2)$y\n}\nmu_samps_jeff &lt;- sample_mu(50000)\n# induced gamma posterior from jeffrey's prior on mu\ngamma_samps_from_mu_jeff &lt;- exp(mu_samps_jeff)\n\n# Sampling function from gamma posterior\ncdf_gamma &lt;- cumtrapz(gamma_vec, post_gamma)\ncdf_gamma &lt;- cdf_gamma / max(cdf_gamma)\nsample_gamma &lt;- function(n) {\n  u &lt;- runif(n)\n  approx(cdf_gamma, gamma_vec, xout = u, rule = 2)$y\n}\n# gamma posterior with flat prior on gamma starting in gamma space\ngamma_samps &lt;- sample_gamma(50000)\n\n\n\n\n\n\n\n\n\n\nNow what happens if I use Jeffrey’s prior for \\(\\theta\\) and also Jeffrey’s prior for \\(\\gamma\\)? The posteriors in \\(\\gamma\\)-space ought to look the same!\n\n# Jeffreys prior on gamma is proportional to 1/gamma\nprior_jeff_gamma &lt;- 1 / gamma_vec\n\n# Posterior in gamma under jeffreys prior\npost_gamma_jeff &lt;- lik_gamma * prior_jeff_gamma\npost_gamma_jeff &lt;- post_gamma_jeff / trapz(gamma_vec, post_gamma_jeff)\n\n# Sampling functions using inverse CDF method\ncdf_gamma_jeff &lt;- cumtrapz(gamma_vec, post_gamma_jeff)\ncdf_gamma_jeff &lt;- cdf_gamma_jeff / max(cdf_gamma_jeff)\nsample_gamma_jeff &lt;- function(n) {\n  u &lt;- runif(n)\n  approx(cdf_gamma_jeff, gamma_vec, xout = u, rule = 2)$y\n}\n# gamma posterior with Jeffreys prior on gamma starting in gamma space\ngamma_samps_jeff &lt;- sample_gamma_jeff(50000)"
  },
  {
    "objectID": "code/hierarchical_normal_schools.html",
    "href": "code/hierarchical_normal_schools.html",
    "title": "Hierarchical modeling: Normal data",
    "section": "",
    "text": "We have data from the 2002 Educational Longitudinal Study (ELS), a survey of students from a large sample of schools across the United States. This dataset includes a population of schools as well as a population of students within each school. Note that this is a nested/hierarchical structure. This survey included 10th grade children from 100 diﬀerent large urban public high schools, all having a 10th grade enrollment of 400 or greater. Note that the number of students sampled from each school differs.\nIn particular, we have data from math scores from a math exam on the ELS, which was standardized to produce a nationwide mean of 50 and a standard deviation of 10.\nThe following displays boxplots of the distribution of math scores by school. The pink line displays the overall sample mean score, averaged across all schools and students. What do we notice?\n\n\n\n\n\n\n\n\n\nHere, we plot the distribution sample mean scores of each school (left), along with the sample mean scores as a function of the sample size from the school (right). What do we notice here?"
  },
  {
    "objectID": "code/hierarchical_normal_schools.html#data-and-eda",
    "href": "code/hierarchical_normal_schools.html#data-and-eda",
    "title": "Hierarchical modeling: Normal data",
    "section": "",
    "text": "We have data from the 2002 Educational Longitudinal Study (ELS), a survey of students from a large sample of schools across the United States. This dataset includes a population of schools as well as a population of students within each school. Note that this is a nested/hierarchical structure. This survey included 10th grade children from 100 diﬀerent large urban public high schools, all having a 10th grade enrollment of 400 or greater. Note that the number of students sampled from each school differs.\nIn particular, we have data from math scores from a math exam on the ELS, which was standardized to produce a nationwide mean of 50 and a standard deviation of 10.\nThe following displays boxplots of the distribution of math scores by school. The pink line displays the overall sample mean score, averaged across all schools and students. What do we notice?\n\n\n\n\n\n\n\n\n\nHere, we plot the distribution sample mean scores of each school (left), along with the sample mean scores as a function of the sample size from the school (right). What do we notice here?"
  },
  {
    "objectID": "code/hierarchical_normal_schools.html#prior-solitication",
    "href": "code/hierarchical_normal_schools.html#prior-solitication",
    "title": "Hierarchical modeling: Normal data",
    "section": "Prior solitication",
    "text": "Prior solitication\nSet priors based on the information provided by ELS: the math exam was designed to give a mean of 50 and a nationwide variance of 100. Note: this variance includes both within-school and between-school variance.\nLet’s brainstorm how to translate these into sensible prior parameter values for the unknown parameters! We need to put things in context. Recall:\n\\[\\theta \\sim N(\\mu_{0}, \\sigma^{2}_{0})\\] \\[1/\\sigma^2 \\sim \\text{Gamma}(a, b)\\] \\[1/\\tau^2 \\sim \\text{Gamma}(c, d)\\]"
  },
  {
    "objectID": "code/hierarchical_normal_schools.html#some-mcmc-diagnostics",
    "href": "code/hierarchical_normal_schools.html#some-mcmc-diagnostics",
    "title": "Hierarchical modeling: Normal data",
    "section": "Some MCMC diagnostics",
    "text": "Some MCMC diagnostics\nRan chain with burn-in of 5000 and then retained 5000 samples. The following plots are another way to determine stationarity/convergence. We can produce boxplots of sequential groups of MCMC samples. In the following, each of the 10 boxplots represents 1/10th of the MCMC samples. If convergence has been achieved, then the distribution of samples in any one boxplot should be the same as that in any other (for a given parameter).\n\n\n\n\n\n\n\n\n\nShowing a subset of the \\(\\theta_{j}\\) (first 9 schools) for illustrative purposes:\n\n\n\n\n\n\n\n\n\nEffective sample sizes from 5000 iterations post burn-in for \\(\\theta\\), \\(\\sigma^2\\), \\(\\tau^2\\) are: 3733.74, 4849.7, 2734.02. The ESS for the \\(\\theta_{j}\\) chains range from 4119.69 to 5526.92."
  },
  {
    "objectID": "code/hierarchical_normal_schools.html#posterior-inference",
    "href": "code/hierarchical_normal_schools.html#posterior-inference",
    "title": "Hierarchical modeling: Normal data",
    "section": "Posterior inference",
    "text": "Posterior inference\nPosterior means and 95% credible intervals of \\(\\theta\\), \\(\\sigma\\), and \\(\\tau\\) are as follows. Let’s think about their interpretation.\n\n\n\n\n\nThe following displays the posterior distributions of the school-specific mean scores \\(\\theta_{j}\\), in order of smallest to largest:\n\n\n\n\n\n\n\n\n\nThis plot illustrates the notion of “shrinkage” or “borrowing information”. Let’s interpret!"
  },
  {
    "objectID": "homework/412_hw5_r_starter.html#part-a",
    "href": "homework/412_hw5_r_starter.html#part-a",
    "title": "STAT 412: Homework 5 (R)",
    "section": "Part a",
    "text": "Part a\n\n# specifying priors"
  },
  {
    "objectID": "homework/412_hw5_r_starter.html#part-b",
    "href": "homework/412_hw5_r_starter.html#part-b",
    "title": "STAT 412: Homework 5 (R)",
    "section": "Part b",
    "text": "Part b\n\n# gibbs sampler here"
  },
  {
    "objectID": "homework/412_hw5_r_starter.html#part-c",
    "href": "homework/412_hw5_r_starter.html#part-c",
    "title": "STAT 412: Homework 5 (R)",
    "section": "Part c",
    "text": "Part c"
  },
  {
    "objectID": "homework/412_hw5_r_starter.html#part-d",
    "href": "homework/412_hw5_r_starter.html#part-d",
    "title": "STAT 412: Homework 5 (R)",
    "section": "Part d",
    "text": "Part d"
  },
  {
    "objectID": "homework/412_hw5_r_starter.html#part-e",
    "href": "homework/412_hw5_r_starter.html#part-e",
    "title": "STAT 412: Homework 5 (R)",
    "section": "Part e",
    "text": "Part e"
  },
  {
    "objectID": "homework/412_hw5_r_starter.html#part-f",
    "href": "homework/412_hw5_r_starter.html#part-f",
    "title": "STAT 412: Homework 5 (R)",
    "section": "Part f",
    "text": "Part f"
  },
  {
    "objectID": "homework/412_hw5_r_starter.html#part-b-1",
    "href": "homework/412_hw5_r_starter.html#part-b-1",
    "title": "STAT 412: Homework 5 (R)",
    "section": "Part b",
    "text": "Part b\n\n# model 1 PPC"
  },
  {
    "objectID": "homework/412_hw5_r_starter.html#part-e-1",
    "href": "homework/412_hw5_r_starter.html#part-e-1",
    "title": "STAT 412: Homework 5 (R)",
    "section": "Part e",
    "text": "Part e\n\n# model 2 Gibbs"
  },
  {
    "objectID": "homework/412_hw5_r_starter.html#part-f-1",
    "href": "homework/412_hw5_r_starter.html#part-f-1",
    "title": "STAT 412: Homework 5 (R)",
    "section": "Part f",
    "text": "Part f\n\n# model 2 posterior summary"
  },
  {
    "objectID": "homework/412_hw5_r_starter.html#part-g",
    "href": "homework/412_hw5_r_starter.html#part-g",
    "title": "STAT 412: Homework 5 (R)",
    "section": "Part g",
    "text": "Part g\n\n# model 2 posterior summary"
  },
  {
    "objectID": "homework/412_hw5_r_starter.html#part-e-optional",
    "href": "homework/412_hw5_r_starter.html#part-e-optional",
    "title": "STAT 412: Homework 5 (R)",
    "section": "Part e (optional)",
    "text": "Part e (optional)"
  },
  {
    "objectID": "code/bayes_slr.html",
    "href": "code/bayes_slr.html",
    "title": "Bayes Simple Linear Regression",
    "section": "",
    "text": "We have data that contains teacher salaries from 2009-2010 for teachers employed by the St. Louis Public School in Michigan. The dataset has been filtered to retain only teachers with at most 15 years of service and who have a full teaching load (FTE). This leaves in total 39 teachers. I have modified the data slightly to make this analysis more interesting.\nWe have the following variables:\n\nid: Identification code for each teacher, assigned randomly\ndegree: Highest educational degree attained: BA (bachelor’s degree) or MA (master’s degree)\nyears: Number of years employed by the school district\nbase: Base annual salary, in dollars.\n\nWe have the following snapshot of the data:\n\n\n\n\n\nid\ndegree\nfte\nyears\nbase\n\n\n\n\n01\nBA\n1\n5\n45231.39\n\n\n02\nMA\n1\n15\n60694.91\n\n\n04\nBA\n1\n10\n54257.09\n\n\n07\nBA\n1\n12\n58495.82\n\n\n11\nBA\n1\n12\n58179.38\n\n\n\n\n\nSuppose I’m a teacher who is moving to Michigan, and I’m interested in learning what kind of salary I could expect to earn. I know that salaries are related in some way to years of experience, so let’s do some EDA to see the empirical relationship between years of experience and base salary at this public school:\n\n\n\n\n\n\n\n\n\nWhat do we notice?"
  },
  {
    "objectID": "code/bayes_slr.html#data-and-eda",
    "href": "code/bayes_slr.html#data-and-eda",
    "title": "Bayes Simple Linear Regression",
    "section": "",
    "text": "We have data that contains teacher salaries from 2009-2010 for teachers employed by the St. Louis Public School in Michigan. The dataset has been filtered to retain only teachers with at most 15 years of service and who have a full teaching load (FTE). This leaves in total 39 teachers. I have modified the data slightly to make this analysis more interesting.\nWe have the following variables:\n\nid: Identification code for each teacher, assigned randomly\ndegree: Highest educational degree attained: BA (bachelor’s degree) or MA (master’s degree)\nyears: Number of years employed by the school district\nbase: Base annual salary, in dollars.\n\nWe have the following snapshot of the data:\n\n\n\n\n\nid\ndegree\nfte\nyears\nbase\n\n\n\n\n01\nBA\n1\n5\n45231.39\n\n\n02\nMA\n1\n15\n60694.91\n\n\n04\nBA\n1\n10\n54257.09\n\n\n07\nBA\n1\n12\n58495.82\n\n\n11\nBA\n1\n12\n58179.38\n\n\n\n\n\nSuppose I’m a teacher who is moving to Michigan, and I’m interested in learning what kind of salary I could expect to earn. I know that salaries are related in some way to years of experience, so let’s do some EDA to see the empirical relationship between years of experience and base salary at this public school:\n\n\n\n\n\n\n\n\n\nWhat do we notice?"
  },
  {
    "objectID": "code/bayes_slr.html#fitting-a-slr",
    "href": "code/bayes_slr.html#fitting-a-slr",
    "title": "Bayes Simple Linear Regression",
    "section": "Fitting a SLR",
    "text": "Fitting a SLR\nI will fit a SLR model to these data, regressing base salary on years of experience (ignoring degree for now). Thus, in context, my SLR looks like the following:\n\\[\\text{base}_{i} = \\beta_{0} + \\beta_{1} \\text{years}_{i} + \\epsilon_{i}\n\\] \\[\\epsilon_{i} \\overset{iid}{\\sim} N(0, \\sigma^2) \\qquad \\quad i = 1,\\ldots, n = `r nrow(teachers)\\]\n\nPrior solicitation 1\nMy priors might be specified as follows:\n\\[\\beta_{0} \\sim N(0, 100^2)\\] \\[\\beta_{1} \\sim N(0, 100^2)\\] \\[\\frac{1}{\\sigma^2} \\sim \\text{Gamma}(1,1)\\]\nWhat’s possibly wrong with this prior specification??\n\n\nPrior solicitation 2\nLet’s choose the following new priors:\n\\[\\beta_{0} \\sim N(50000, 10000^2)\\] \\[\\beta_{1} \\sim N(0, 10000^2)\\]\nTo approximate the joint posterior distribution, I will run a Gibbs sampler for 20000 iterations, throwing the first half away to burn-in.\n\n\nDiagnostics\nHere are some traceplots:\n\n\n\n\n\n\n\n\n\nThe effective sample sizes of \\(\\beta_{0}, \\beta_{1}, \\sigma^2\\) from these 10000 iterations are: 1340.28, 1291.16, 8044.04. Why do we think some of these are so low?"
  },
  {
    "objectID": "code/bayes_slr.html#model-assessment",
    "href": "code/bayes_slr.html#model-assessment",
    "title": "Bayes Simple Linear Regression",
    "section": "Model assessment",
    "text": "Model assessment\n\nPosterior predictive check\nTo assess the fit of a SLR model, one approach is to generate PPDs and compare them (visually) to the original data. Once again, we want the observed response values to be consistent with predicted responses generated from the fitted model. So, as before, let’s generate some PPDs. For \\(k \\in 1:K\\):\n\nObtain/sample posterior values of \\(\\beta_{0}, \\beta_{1}, \\sigma_{2}\\), call these \\(\\beta^*_{0}, \\beta^{*}_{1}, \\sigma^{2*}\\).\nSample \\(\\mathbf{y}^{*} = \\{y^{*}_{1},\\ldots,y_{n}^{*} \\}\\) where \\(n\\) is the sample size \\(n= 39\\) and \\(y_{i}^{*} \\sim N(\\beta^*_{0}+ \\beta^{*}_{1} x_{i}, \\sigma^{2*})\\). Note that these \\(x_{i}\\) are the same as in the original data, as they are assumed fixed!\n\n\nset.seed(1)\nK &lt;- 8\nsamp_ids &lt;- sample(1:G, K) # G is number of iterations post burn-in\nppd_ls &lt;- list() \n# POSTS is matrix with columns [beta0, beta1, s2]\n# y is vector of base salary, x is vector of years of experience\nfor (k in 1:K){\n  ppd_y &lt;- rnorm(n, POSTS[samp_ids[k],1] + POSTS[samp_ids[k],2] * x,\n               sqrt(POSTS[samp_ids[k],3]))\n  ppd_ls[[k]] &lt;- data.frame(base = ppd_y,\n                   years = x, \n                   dist = paste0(\"PPD\", k)) # variable for plotting\n}\n# append on original/true y\nppd_ls[[k+1]] &lt;- data.frame(base = y, years = x, dist = \"Original\")\n\ndo.call(rbind, ppd_ls) |&gt;\n  ggplot(aes(x = years, y = base)) +\n  geom_point() +\n  facet_wrap(~ dist)\n\n\n\n\n\n\n\n\nWhat do we notice?\n\n\nResiduals\nWhen fitting a linear regression model, a very common way of assessing model fit is to look at the residuals: \\(e_{i} = y_{i} - \\hat{y}_{i}\\) where \\(\\hat{y}_{i}\\) is the fitted/estimated value of the \\(i\\)-th response. We should ideally have residuals close to 0. Additionally, residuals shouldn’t “look” very different from different values of \\(x_{i}\\) (i.e. we don’t want to do better at predicting certain observations than others).\nTo do this, as before, for some number \\(K\\) times:\n\nObtain/sample parameter values from the posterior\nSimulate \\(y_{i}^*\\) from its corresponding Normal distribution using the posterior values from 1.\nObtain \\(e_{i} = y_{i} - y_{i}^*\\)\n\nBecause the predicted values are random, so are the residuals \\(e_{i}\\). So, our residual plot will look different from the usual, frequentist residual plot. We can plot credible intervals of the residuals for each observation \\(i\\)! Thus, in this diagnostic, we should choose \\(K\\) large (e.g. the total number of iterations after burn-in) so to get good approximations of the variablity.\n\nset.seed(1)\nresids_ls &lt;- list()\nfor(i in 1:n){\n  y_preds_i &lt;- rnorm(G, POSTS[,1] + POSTS[,2] * x[i], sqrt(POSTS[,3]))\n  resids &lt;- y[i] - y_preds_i\n  resids_ls[[i]] &lt;- data.frame(resid = resids, obs = i, years = x[i])\n}\nresids_df_all &lt;- do.call(rbind, resids_ls)\n\nSnapshot of resids_df_all:\n\n\n\n\n\nresid\nobs\nyears\n\n\n\n\n-254.1278\n1\n5\n\n\n-1330.9272\n1\n5\n\n\n1347.9132\n1\n5\n\n\n\n\n\n\\[\\vdots\\]\n\n\n\n\n\nresid\nobs\nyears\n\n\n\n\n2007.4218\n39\n5.5\n\n\n731.1135\n39\n5.5\n\n\n4374.1961\n39\n5.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do we notice here?"
  },
  {
    "objectID": "code/bayes_slr.html#posterior-inference",
    "href": "code/bayes_slr.html#posterior-inference",
    "title": "Bayes Simple Linear Regression",
    "section": "Posterior inference",
    "text": "Posterior inference\nRegardless of what we determined above about model fit, let’s continue with doing some posterior inference.\n\nPosterior summaries\nHere are some posterior summaries of the three parameters:\n\n\n\n\n\nLet’s give some interpretation!\n\n\nLine of best fit\nLet’s obtain a line of “best fit” (note that a is italicized because there is not a notion of a single best line). One might obtain such a line by taking the posterior mean estimates \\((\\hat{\\beta}_{0}, \\hat{\\beta}_{1})\\) of \\((\\beta_{0}, \\beta_{1})\\), and use them to plot the line \\(\\hat{y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x\\).\nIf we use the posterior means as the estimates, then this “best line” represents the most likely value of the line \\(\\beta_{0} + \\beta_{1} x\\) from the posterior distribution. To include some notion of uncertainty of this “best line”, we might also plot some number of \\(J\\) line estimates where we take samples \\((\\beta_{0}^{(j)}, \\beta_{1}^{(j)})\\) from the posterior (i.e. Gibbs sampler) and plot \\(\\beta_{0}^{(j)} + \\beta_{1}^{(j)} x\\).\n\nset.seed(1)\nJ &lt;- 10\niter_samps &lt;- sample(1:G, J) \npost_means &lt;- colMeans(POSTS)\n\nggplot(teachers, aes(x = years, y = base)) +\n  geom_point(size=2) +\n  geom_abline(data=post_df[iter_samps, ], aes(intercept=beta0, slope=beta1),\n              alpha = 0.2, col = \"magenta\") +\n  geom_abline(intercept = post_means[1],\n              slope = post_means[2], col = \"blue\") +\n  ylab(\"Base salary ($)\") + xlab(\"Years\") +\n  labs(caption = \"Blue line: a line of best fit \\n Pink lines: posterior samples of line\")\n\n\n\n\n\n\n\n\nWhat do we notice?\n\n\nPredicted responses\nI have approximately 3.5 years of teaching experience and my friend has approximately 8.5 years. I might like to know what sort of ranges of base salaries we might expect to earn. To answer this question, why not obtain the posterior density of base salaries for each of \\(x^{(new)} = 3.5\\) and \\(x^{(new)} = 8.5\\)? Do the following for \\(k = 1,\\ldots K\\) (once again, with \\(K\\) large):\n\nUse the posterior samples of \\(\\beta_{0}, \\beta_{1}\\) to simulate the expected earnings at a given level \\(x^{(new)}\\) of years of experience\nRandomly sample the earnings at this mean using the Normal model and a posterior sample of \\(\\sigma^2\\)"
  },
  {
    "objectID": "code/bayes_mlr.html",
    "href": "code/bayes_mlr.html",
    "title": "Bayesian Multiple Linear Regression",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = F, message = F, warning = F, fig.width = 5, fig.height = 3)\nlibrary(mvtnorm) # install if necessary\nlibrary(tidyverse)\nlibrary(kableExtra)\nData from \\(n = 113\\) hospitals are used to assess what factors may influence the likelihood that a patient acquires an infection while hospitalized. We have the following variables:\nInfctRsk\nStay\nXray\n\n\n\n\n4.1\n7.13\n39.6\n\n\n1.6\n8.82\n51.7\n\n\n2.7\n8.34\n74.0\n\n\n5.6\n8.95\n122.8\nPerhaps we’d like to understand how the infection risk is related to the length of stay and number of x-rays given in a hospital, so we fit the following regression model: \\[\\text{InfctRisk}_{i} = \\beta_{0} + \\beta_{1} \\text{Stay}_{i} + \\beta_{2} \\text{Xray}_{i} + \\epsilon_{i}, \\qquad i = 1,\\ldots,n\\] where the \\(\\epsilon_{i} \\overset{iid}{\\sim} N(0,\\sigma^2)\\) are iid.\nLetting \\(Y_{i} = \\text{InfctRisk}_{i}\\), \\(\\mathbf{x}_{i} = (1, \\text{Stay}_{i}, \\text{Xray}_{i})'\\), and \\(\\boldsymbol{\\beta} = (\\beta_{0}, \\beta_{1}, \\beta_{2})'\\) we have the following sampling model:\n\\[\ny_{i}|\\mathbf{x}_{i}, \\boldsymbol{\\beta}, \\sigma^2 \\sim N(\\mathbf{x}_{i}' \\boldsymbol{\\beta}, \\sigma^2), \\qquad i = 1,\\ldots, n\n\\]"
  },
  {
    "objectID": "code/bayes_mlr.html#prior-option-1",
    "href": "code/bayes_mlr.html#prior-option-1",
    "title": "Bayesian Multiple Linear Regression",
    "section": "Prior option 1",
    "text": "Prior option 1\nwe might choose the following (independent) priors:\n\\[\\boldsymbol{\\beta} \\sim MVN_{3}(\\boldsymbol{\\mu}_{0}, \\boldsymbol{\\Sigma}_{0})\\] \\[\\frac{1}{\\sigma^2} \\sim \\text{Gamma}(1,1)\\]\nAnd in particular, we might choose \\(\\boldsymbol{\\mu}_{0} = (0,0,0)'\\) (the scale of the \\(Y_{i}\\) are small, and a priori assume no relationship). We might also assume \\(\\Sigma_{0} = 100^2 \\mathbf{I}_{3}\\) (i.e. the coefficients are independent and have a s.d. of 100).\nWe can approximate the posteriors of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) using a Gibbs sampler.\n\nData set-up\nWe need to do just a small amount of work to specify the data matrix \\(\\mathbf{X}\\), where each row corresponds to each observation \\(i\\)’s predictors, and we have a column of 1’s for the intercept:\n\n\n     [,1]  [,2]  [,3]\n[1,]    1  7.13  39.6\n[2,]    1  8.82  51.7\n[3,]    1  8.34  74.0\n[4,]    1  8.95 122.8\n[5,]    1 11.20  88.9\n[6,]    1  9.76  97.0\n\n\n  (Intercept)  Stay  Xray\n1           1  7.13  39.6\n2           1  8.82  51.7\n3           1  8.34  74.0\n4           1  8.95 122.8\n5           1 11.20  88.9\n6           1  9.76  97.0\n\n\nThen we set-up the rest of the necessary data components:\n\n\nRunning Gibbs sampler\nCheck traceplots to assess convergence:\n\n\n\n\n\n\n\n\n\nLook at effective sample size from \\(G = r G/2\\) iterations:\n\nlibrary(coda)\napply(BETAS, 2, effectiveSize)\n\n[1] 5000 5000 5000\n\neffectiveSize(S2)\n\nvar1 \n5000 \n\n\nHow does this compare to ESS from SLR? Why do we think that is?\nCheck sample lag-2 autocorrelation:\n\n\n          lag-0        lag-1        lag-2\nIntercept     1 -0.014393436  0.012124642\nstay          1 -0.009353404  0.004008842\nxray          1 -0.008098636  0.025658893\ns2            1  0.009352675 -0.023047790\n\n\n\n\nPosterior inference"
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 412: Bayesian Statistics",
    "section": "",
    "text": "Welcome to the website for Middlebury College’s Fall 2025 STAT 412. On this website you will find the course syllabus, schedule, and assignments. The website is frequently updated throughout the semester, so please make a habit of refreshing the page."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#announcements",
    "href": "index.html#announcements",
    "title": "STAT 412: Bayesian Statistics",
    "section": "Announcements",
    "text": "Announcements\n\nPlease ensure R Studio functions on your laptop"
  },
  {
    "objectID": "index.html#course-details",
    "href": "index.html#course-details",
    "title": "STAT 412: Bayesian Statistics",
    "section": "Course details",
    "text": "Course details\nInstructor: Becky Tang (she/her)\n\nOffice: WNS 214\nEmail: btang@middlebury.edu\n\nMeeting times and location: Mondays and Wednesdays 9:45-11am in WNS 011\nOffice hours: Monday 3-4pm, Tuesday 3:30-4:30pm, Friday 11am-12pm\nSyllabus (most recent update 9/1/2025): our syllabus outlines our course policies and a rough schedule. Once classes begin, you should follow the schedule and due dates found on this website."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This page is frequently updated!\n\n\n\nWeek\nDate\nTopic\nAssignments\n\n\n\n\n1\nM 9/08\n\nWelcome!\nWhat is a statistical model?\n\n\nProblem Set 0 (due next class 9/10)\n\n\n\n\nW 9/10\n\nBayes’ rule, Bayes modeling\n\nLikelihood, prior, posterior\nCode and worksheet\n\n\n\nProblem Set 1 (due next class 9/15)\n\n\n\n2\nM 9/15\n\nBeta-Binomial model\n\nProportionality\n\nPosterior predictive distribution\n\n\nProblem Set 2 (due 9/24)\n\n\n\n\nW 9/17\n\nBayes estimators/estimates\n\nSome proofs\n\n\n\nProblem Set 2 (due 9/24)\n\n\n\n3\nM 9/22\n\nConfidence regions\nPosterior quantities and posterior predictive checks via simulation\n\nCode\n\n\n\n\n\n\nW 9/24\n\nConjugacy\nNormal sampling model\n\n\nProblem Set 3 (complete!)\nCode for HW 3"
  },
  {
    "objectID": "handouts.html",
    "href": "handouts.html",
    "title": "Handouts",
    "section": "",
    "text": "Distribution sheet\nData\n\n prussianHorses1 \n prussianHorses2"
  },
  {
    "objectID": "code/binom_discrete_prior.html",
    "href": "code/binom_discrete_prior.html",
    "title": "Learning about a Binomial Probability",
    "section": "",
    "text": "library(tidyverse)\nknitr::opts_chunk$set(fig.width = 6, fig.height = 3)"
  },
  {
    "objectID": "code/binom_discrete_prior.html#tidyverse-code",
    "href": "code/binom_discrete_prior.html#tidyverse-code",
    "title": "Learning about a Binomial Probability",
    "section": "Tidyverse code",
    "text": "Tidyverse code\n\n# PRIOR\ntheta &lt;- seq(0.1, 0.9, by = 0.1)\nprior1 &lt;- rep(1/9, 9)\nprior2 &lt;- c(0.05, 0.05, 0.05, 0.2, 0.3, 0.2, 0.05, 0.05, 0.05)\n\n# create data frame for visualization and wrangling\nbayes_table &lt;- data.frame(theta, prior1, prior2) \n\n# visualize different priors\nbayes_table |&gt;\n  pivot_longer(cols = 2:3, names_to = \"prior\", values_to = \"prior_probs\") |&gt;\n  mutate(theta = factor(theta)) |&gt; # for prettier visualization\n  ggplot(aes(x = theta, y = prior_probs)) + \n  geom_col() +\n  facet_wrap(~ prior) +\n  labs(x = expression(theta),\n      y = expression(f(theta)),\n      title = \"Prior\")\n\n\n\n\n\n\n\n# LIKELIHOOD\ny &lt;- 4\n## since R is vectorized:\nlike &lt;- dbinom(y, size = 11, prob = theta)\n## equivalently\n## like &lt;- choose(11, y) * theta^y * (1-theta)^(11-y)\nbayes_table &lt;- bayes_table |&gt;\n  add_column(likelihood = like) \n\n\n# MARGINAL LIKELIHOOD\nbayes_table &lt;- bayes_table |&gt;\n  mutate(product = prior2 * likelihood)\nmarg_like &lt;- sum(bayes_table$product)\n\n# POSTERIOR\nbayes_table &lt;- bayes_table |&gt;\n  mutate(posterior = product / marg_like)\nbayes_table |&gt;\n  select(theta, posterior)\n\n  theta      posterior\n1   0.1 0.006168515787\n2   0.2 0.043274594401\n3   0.3 0.086030889543\n4   0.4 0.369693507640\n5   0.5 0.377836937562\n6   0.6 0.109538817078\n7   0.7 0.006772110839\n8   0.8 0.000676165538\n9   0.9 0.000008461613\n\n# visualize prior vs posterior\n# note: could also plot in two separate plots without pivoting\nbayes_table |&gt;\n  pivot_longer(cols = c(\"prior1\", \"posterior\"), names_to = \"dist\", values_to = \"probs\") |&gt;\n  mutate(theta = factor(theta),\n         dist = factor(dist, levels = c(\"prior1\", \"posterior\"))) |&gt; # for prettier visualization\n  ggplot(aes(x = theta, y = probs)) + \n  geom_col() +\n  facet_wrap(~dist) +\n  labs(x = expression(theta),\n      y = \"Probability\",\n      title = expression(\"Comparison of \" * f(theta) * \" and \" * f(theta * \"| y\")))"
  },
  {
    "objectID": "code/binom_discrete_prior.html#base-r-code",
    "href": "code/binom_discrete_prior.html#base-r-code",
    "title": "Learning about a Binomial Probability",
    "section": "Base R code",
    "text": "Base R code\n\n## PRIOR\ntheta &lt;- seq(0.1, 0.9, by = 0.1)\nprior1 &lt;- rep(1/9, 9)\nprior2 &lt;- c(0.05, 0.05, 0.05, 0.2, 0.3, 0.2, 0.05, 0.05, 0.05)\n\nbayes_table &lt;- data.frame(theta, prior1, prior2)\n\n# Visualize prior 1\nbarplot(bayes_table$prior1, names.arg = bayes_table$theta,\n        xlab = expression(theta),\n        ylab = expression(f(theta)),\n        ylim = c(0, max(prior1) + 0.05),\n        main = \"Prior\")\n\n\n\n\n\n\n\n# LIKELIHOOD\ny &lt;- 4\nlike &lt;- dbinom(y, size = 11, prob = theta)\nbayes_table$likelihood &lt;- like\n\n# MARGINAL LIKELIHOOD\nbayes_table$product &lt;- prior1 * like\nmarg_like &lt;- sum(bayes_table$product)\n\n# POSTERIOR\nbayes_table$posterior &lt;-  bayes_table$product / marg_like\n\n# vizualize prior and posterior\npar(mfrow = c(1,2))\ny_max &lt;- max(c(prior1, bayes_table$posterior))\nbarplot(bayes_table$prior1, names.arg = bayes_table$theta,\n        xlab = expression(theta),\n        ylab = expression(f(theta)),\n        ylim = c(0, y_max + 0.05),\n        main = \"Prior\")\nbarplot(bayes_table$posterior, \n        names.arg = theta,\n        beside = T,\n        xlab = expression(theta),\n        ylab = expression(f(theta * \"| y\")),\n        ylim = c(0, y_max + 0.05),\n        main = \"Posterior\")"
  },
  {
    "objectID": "code/binom_discrete_prior.html#if-you-didnt-want-to-plot",
    "href": "code/binom_discrete_prior.html#if-you-didnt-want-to-plot",
    "title": "Learning about a Binomial Probability",
    "section": "If you didn’t want to plot",
    "text": "If you didn’t want to plot\n\nmarg_like &lt;- sum(prior1 * like)\npost &lt;- (prior1 * like)/marg_like\ncbind(theta, post)\n\n      theta          post\n [1,]   0.1 0.01893857939\n [2,]   0.2 0.13286167531\n [3,]   0.3 0.26413206805\n [4,]   0.4 0.28375828506\n [5,]   0.5 0.19333918362\n [6,]   0.6 0.08407652891\n [7,]   0.7 0.02079173713\n [8,]   0.8 0.00207596368\n [9,]   0.9 0.00002597885"
  },
  {
    "objectID": "code/proportionality.html",
    "href": "code/proportionality.html",
    "title": "Prior, likelihood, posterior, proportionality",
    "section": "",
    "text": "We often say the posterior distribution is “proportional to likelihood times prior”, where proportionality is with respect to \\(\\theta\\):\n\\[\\begin{align*}\nf_{\\theta | y}(\\theta | y) &= \\frac{f_{y | \\theta} (y | \\theta) f_{\\theta}(\\theta)}{f_{Y}(y)} \\\\\n& \\overset{\\theta}{\\propto} f_{y | \\theta} (y | \\theta) f_{\\theta}(\\theta)\n\\end{align*}\\]\nThat is, the posterior distribution is a function of \\(\\theta\\), so the marginal likelihood \\(f_{Y}(y)\\) is a constant with respect to \\(\\theta\\). It is just a scaling factor, as illustrated here:"
  },
  {
    "objectID": "handouts/bayes_est.html#theorem",
    "href": "handouts/bayes_est.html#theorem",
    "title": "Bayes estimator proofs",
    "section": "Theorem",
    "text": "Theorem\nUnder squared loss \\(L(\\theta, a) = (\\theta - a)^2\\), the Bayes estimator for \\(\\theta\\) is the posterior mean of \\(\\theta\\), \\(\\mathbb{E}[\\theta | \\mathbf{y}]\\).\n\nWant to show that \\(\\mathbb{E}[\\theta | \\mathbf{y}] = \\underset{a}{\\text{argmin}} \\ \\mathbb{E}[(\\theta - a)^2 | \\mathbf{y}]\\)\nRecall: \\(\\mathbb{E}[\\theta | \\mathbf{y}] = \\int_{\\Theta} \\theta f(\\theta | \\mathbf{y}) d\\theta\\) is a function of \\(\\mathbf{y}\\)! We’ve integrated out \\(\\theta\\)!\nAlso, for any function \\(g\\), \\(\\mathbb{E}[g(\\mathbf{y}) | \\mathbf{y}] = g(\\mathbf{y})\\).\n\nThat is, since we condition on \\(\\mathbf{y}\\) (not random), the expectation of any function of \\(\\mathbf{y}\\) is itself"
  },
  {
    "objectID": "handouts/bayes_est.html#proof-set-up",
    "href": "handouts/bayes_est.html#proof-set-up",
    "title": "Bayes estimator proofs",
    "section": "Proof set-up",
    "text": "Proof set-up\n\nWant to show that \\(m = \\underset{a}{\\text{argmin}} \\ \\mathbb{E}[ |\\theta - a| | \\mathbf{y}]\\)\nNote that the absolute value function is not differentiable, so proving a maximum/minimum cannot rely on derivatives!\nAssume that \\(\\theta\\) is continuous, so that if \\(m\\) is the posterior median, then \\(\\text{Pr}(\\theta \\geq m | \\mathbf{y}) = \\frac{1}{2} = \\text{Pr}(\\theta \\leq m | \\mathbf{y})\\).\nLet \\(a\\) be any other estimator of \\(\\theta\\).\nWe will show that\n\\[\n\\mathbb{E}[L(\\theta , a) | \\mathbf{y}] - \\mathbb{E}[L(\\theta, m) | \\mathbf{y}]  \\geq 0\n\\]\nthus demonstrating that \\(m\\) minimizes the expected loss.\nSome recap from probability: if \\(Y\\) continuous with pdf \\(f\\),\n\n\\(\\text{Pr}(Y \\leq a) = \\int_{-\\infty}^{a} f(y) dy\\)\n\\(\\text{Pr}(a \\leq Y \\leq b) = \\text{Pr}(Y\\leq b) - \\text{Pr}(Y \\leq a)\\) and these inequalities could be \\(\\leq\\) or \\(&lt;\\)"
  },
  {
    "objectID": "handouts/bayes_est.html#proof",
    "href": "handouts/bayes_est.html#proof",
    "title": "Bayes estimator proofs",
    "section": "Proof",
    "text": "Proof\nLet \\(m\\) be the posterior median, and suppose \\(a &lt; m\\) is any other estimator. Remember, \\(a\\) and \\(m\\) are constant w.r.t. \\(\\theta\\).\n\\[\n\\begin{align}\n\\mathbb{E}[L(\\theta , a) | \\mathbf{y}] & - \\mathbb{E}[L(\\theta, m) | \\mathbf{y}] = \\int_{\\Theta} |\\theta - a| f(\\theta | \\mathbf{y}) d\\theta - \\int_{\\Theta} |\\theta - m| f(\\theta | \\mathbf{y})d\\theta \\\\\n&\\class{fragment}{= \\int_{\\Theta} \\left(|\\theta - a| - |\\theta - m|\\right) f(\\theta | \\mathbf{y})d\\theta } \\\\\n&\\class{fragment}{= \\int_{-\\infty}^{a} \\left(|\\theta - a| - |\\theta - m|\\right) f(\\theta | \\mathbf{y})d\\theta + \\int_{a}^{m} \\left(|\\theta - a| - |\\theta - m|\\right) f(\\theta | \\mathbf{y})d\\theta + \\int_{m}^{\\infty} \\left(|\\theta - a| - |\\theta - m|\\right) f(\\theta | \\mathbf{y})d\\theta }\\\\\n& \\class{fragment}{= \\int_{-\\infty}^{a} ((a-\\theta) - ( m - \\theta)) f(\\theta | \\mathbf{y}) d\\theta + \\int_{a}^{m} ((\\theta - a) - (m- \\theta)) f(\\theta | \\mathbf{y}) d\\theta  +\n\\int_{m}^{\\infty} ((\\theta - a) - (\\theta - m)) f(\\theta | \\mathbf{y}) d\\theta} \\\\\n&\\class{fragment}{= \\int_{-\\infty}^{a} (a - m)  f(\\theta | \\mathbf{y}) d\\theta + \\int_{a}^{m} (2\\theta - a- m)  f(\\theta | \\mathbf{y}) d\\theta + \\int_{m}^{\\infty} (m-a) f(\\theta | \\mathbf{y}) d\\theta} \\\\\n&\\class{fragment}{\\color{orange}{\\geq}  \\int_{-\\infty}^{a} (a - m)  f(\\theta | \\mathbf{y}) d\\theta + \\int_{a}^{m} (2\\color{orange}{a} - a- m)  f(\\theta | \\mathbf{y}) d\\theta + \\int_{m}^{\\infty} (m-a) f(\\theta | \\mathbf{y}) d\\theta} \\\\\n&\\class{fragment}{= (a-m)\\text{Pr}(\\theta \\leq a | \\mathbf{y}) + \\color{purple}{(a-m)\\text{Pr}(a  &lt; \\theta \\leq m | \\mathbf{y}) }+ (m-a) \\text{Pr}(\\theta \\geq m | \\mathbf{y})} \\\\\n&\\class{fragment}{ = (a-m)\\text{Pr}(\\theta \\leq a | \\mathbf{y}) + \\color{purple}{(a-m) \\text{Pr}(\\theta \\leq m | \\mathbf{y})  - (a-m) \\text{Pr}(\\theta \\leq a | \\mathbf{y})} + (m-a) \\text{Pr}(\\theta \\geq m | \\mathbf{y}) } \\\\\n&\\class{fragment}{= (a-m) \\text{Pr}(\\theta \\leq m | \\mathbf{y}) +  (m-a) \\text{Pr}(\\theta \\geq m | \\mathbf{y})} \\\\\n&\\class{fragment}{= (a-m)\\left(\\frac{1}{2}\\right) + (m-a)\\left(\\frac{1}{2}\\right)} \\\\\n&\\class{fragment}{= (a-m)\\left(\\frac{1}{2}\\right) - (a-m)\\left(\\frac{1}{2}\\right)}\\\\\n&\\class{fragment}{= 0 \\qquad \\tiny{\\square} }\n\\end{align}\n\\]"
  },
  {
    "objectID": "handouts/bayes_est.html#proof-2",
    "href": "handouts/bayes_est.html#proof-2",
    "title": "Bayes estimator proofs",
    "section": "Proof 2",
    "text": "Proof 2\nApologies; you’ll have to do several “right clicks” until you reach the next slide.\nLet’s begin by manipulating the term we’d like to minimize: \\[\\begin{align}\n\\mathbb{E}[(\\theta - a)^2 | \\mathbf{y}] &= \\mathbb{E}[( \\color{red}{(} \\theta - \\mathbb{E}[\\theta | \\mathbf{y}]\\color{red}{)} + \\color{red}{(}\\mathbb{E}[\\theta | \\mathbf{y}]  - a\\color{red}{)})^2 | \\mathbf{y}] \\\\\n&\\class{fragment}{\\overset{\\text{FOIL}}{=} \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}] + 2\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])(\\mathbb{E}[\\theta | \\mathbf{y}]-a) | \\mathbf{y}] + \\mathbb{E}[(\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2 | \\mathbf{y}]} \\\\\n&\\class{fragment}{=\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}] + 2\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])\\color{orange}{(\\mathbb{E}[\\theta | \\mathbf{y}]-a)} | \\mathbf{y}] + \\mathbb{E}[\\color{orange}{(\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2} | \\mathbf{y}]} \\\\\n& \\class{fragment}{\\text{Note: }  \\color{orange}{\\mathbb{E}[\\theta | \\mathbf{y}]} \\text{ and } \\color{orange}{a} \\text{ are constant w.r.t } \\mathbf{y}}  \\\\\n&\\class{fragment}{=\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}]  + 2 \\color{orange}{(\\mathbb{E}[\\theta | \\mathbf{y}]-a)}\\color{purple}{\\mathbb{E}[\\theta - \\mathbb{E}[\\theta | \\mathbf{y}] | \\mathbf{y}]}+ \\color{orange}{(\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2}} \\\\\n&\\class{fragment}{=\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}]  + 2 (\\mathbb{E}[\\theta | \\mathbf{y}]-a) \\color{purple}{( \\mathbb{E}[\\theta | \\mathbf{y}] - \\mathbb{E}[\\theta | \\mathbf{y}])} + (\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2} \\\\\n&\\class{fragment}{=\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}]  + (\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2}\n\\end{align}\n\\]"
  },
  {
    "objectID": "handouts/bayes_est.html#theorem-1",
    "href": "handouts/bayes_est.html#theorem-1",
    "title": "Bayes estimator proofs",
    "section": "Theorem",
    "text": "Theorem\nUnder absolute loss \\(L(\\theta, a) = |\\theta - a|\\), a Bayes estimator for \\(\\theta\\) is any posterior median of \\(\\theta\\).\n\nThat is, a Bayes estimator is a function \\(\\delta(\\mathbf{y}) \\equiv m\\) such that \\(\\text{Pr}(\\theta \\leq m | \\mathbf{y}) \\geq \\frac{1}{2}\\) and \\(\\text{Pr}(\\theta \\geq m | \\mathbf{y}) \\geq \\frac{1}{2}\\).\nNote that when \\(\\theta\\) is continuous, there exists a single median."
  },
  {
    "objectID": "handouts/bayes_est.html#proof-2-cont.",
    "href": "handouts/bayes_est.html#proof-2-cont.",
    "title": "Bayes estimator proofs",
    "section": "Proof 2 (cont.)",
    "text": "Proof 2 (cont.)\nThus, \\[\n\\underset{a}{\\text{argmin}} \\ \\mathbb{E}[(\\theta - a)^2 | \\mathbf{y}] = \\underset{a}{\\text{argmin}} \\ \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}]  + (\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2\n\\]\n\nThe first term does not depend on \\(a\\), so cannot do anything about minimizing w.r.t \\(a\\)\nThus, focus on minimizing the second term. Minimized when \\(a = \\mathbb{E}[\\theta | \\mathbf{y}]\\).\nThus, the Bayes estimate under squared loss is the posterior mean. \\(\\tiny{\\square}\\)"
  },
  {
    "objectID": "handouts/bayes_est.html#temp",
    "href": "handouts/bayes_est.html#temp",
    "title": "Bayes estimator proofs",
    "section": "temp",
    "text": "temp"
  },
  {
    "objectID": "handouts/bayes_est.html#incremental-derivation",
    "href": "handouts/bayes_est.html#incremental-derivation",
    "title": "Bayes estimator proofs",
    "section": "Incremental Derivation",
    "text": "Incremental Derivation\n\\[\n\\begin{align}\n\\mathbb{E}[(\\theta - a)^2 \\mid \\mathbf{y}] &= \\mathbb{E}[\\big( \\color{red}{(} \\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}]\\color{red}{)} + \\color{red}{(}\\mathbb{E}[\\theta \\mid \\mathbf{y}]  - a\\color{red}{)}\\big)^2 \\mid \\mathbf{y}]\n\\end{align}\n\\]\n.fragment[ \\[\n\\begin{align}\n&= \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])^2 \\mid \\mathbf{y}] \\\\\n&\\quad + 2\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a) \\mid \\mathbf{y}] \\\\\n&\\quad + \\mathbb{E}[(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)^2 \\mid \\mathbf{y}]\n\\end{align}\n\\]]\n.fragment[ \\[\n\\begin{align}\n&= \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])^2 \\mid \\mathbf{y}] \\\\\n&\\quad + 2\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])\\color{orange}{(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)} \\mid \\mathbf{y}] \\\\\n&\\quad + \\mathbb{E}[\\color{orange}{(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)^2} \\mid \\mathbf{y}]\n\\end{align}\n\\]]\n.fragment[ &gt; Note: () and () are constant with respect to ()]\n.fragment[ \\[\n\\begin{align}\n&= \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])^2 \\mid \\mathbf{y}] \\\\\n&\\quad + 2 \\color{orange}{(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)} \\color{purple}{\\mathbb{E}[\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}] \\mid \\mathbf{y}]} \\\\\n&\\quad + \\color{orange}{(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)^2}\n\\end{align}\n\\]]\n.fragment[ \\[\n\\begin{align}\n&= \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])^2 \\mid \\mathbf{y}] \\\\\n&\\quad + 2 (\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a) \\color{purple}{( \\mathbb{E}[\\theta \\mid \\mathbf{y}] - \\mathbb{E}[\\theta \\mid \\mathbf{y}])} \\\\\n&\\quad + (\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)^2\n\\end{align}\n\\]]\n.fragment[ \\[\n\\begin{align}\n&= \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])^2 \\mid \\mathbf{y}] + (\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)^2\n\\end{align}\n\\]]"
  },
  {
    "objectID": "code/post_pred_checks.html",
    "href": "code/post_pred_checks.html",
    "title": "Simulating posterior and posterior predictive distributions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(MASS)\ndata(\"Aids2\")\nFrom the Aids2 dataset from MASS, we have data on patients diagnosed with AIDS in Australia before 1 July 1991. In particular, we are interested in the outcome status of each individual at the end of the observation (alive A or dead D). We also have data on state of origin:\n# A tibble: 4 × 5\n  state     A     D     n death_rate\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n1 NSW     664  1116  1780      0.627\n2 Other   107   142   249      0.570\n3 QLD      78   148   226      0.655\n4 VIC     233   355   588      0.604"
  },
  {
    "objectID": "code/post_pred_checks.html#posterior-predictive-check",
    "href": "code/post_pred_checks.html#posterior-predictive-check",
    "title": "Simulating posterior and posterior predictive distributions",
    "section": "Posterior predictive check",
    "text": "Posterior predictive check\nNow, let’s do some posterior predictive checks (PPC). One thing I might be wondering is if my assumption of iid across the 4 states is reasonble.\nFirst, let me simulate \\(R = 1000\\) new datasets of size 2843 from the PPD. For each dataset \\(r\\), I can assign state values according the observed data because I assumed state doesn’t matter (i.e. state and death status are independent).\n\nset.seed(412)\n# posterior predictive check\nR &lt;- 1000\ndf_ls &lt;- list()\nfor(r in 1:R){\n  theta_samp &lt;- rbeta(1, a + sum_y, b + n - sum_y)\n  ystar_samps &lt;- rbinom(n, size = 1, prob = theta_samp)\n  df_ls[[r]] &lt;- data.frame(status = ystar_samps) |&gt;\n    add_column(state = Aids2$state)\n}\n# Random sample of three rows in first simulated dataset:\ndf_ls[[1]] |&gt;\n  sample_n(3)\n\n  status state\n1      0   NSW\n2      1   NSW\n3      0   NSW\n\n\nVisually, we can look at how the distribution of deaths varies across the states in some of the simulated datasets:\n\n\n\n\n\n\n\n\n\nLooks pretty similar to the plot of the original data above, but humans like to see what they want to see.\nFor a more robust PPC, let’s consider the following test function \\(T(\\mathbf{y})\\): the ratio of the highest death rate and the lowest death rate among the four states.\nIn the observed data, this is 0.655/0.57 = 1.148. Let’s now obtain the values of the test function for the 1000 simualated datasets:\n\nppd_T &lt;- rep(NA, R)\nfor(r in 1:R){\n  temp &lt;- df_ls[[r]] |&gt;\n    group_by(state) |&gt;\n    summarise(death_rate = mean(status))\n  ppd_T[r] &lt;- max(temp$death_rate)/min(temp$death_rate)\n}\n\n\n\n\n\n\n\n\n\n\nThe posterior predictive probability of a ratio as or more extreme than the observed \\(T(\\mathbf{y}) = 1.148\\) is:\n\n# monte carlo approximation\npost_prob_T &lt;- mean(ppd_T &gt;= obs_T)\n\n\\[\\text{Pr}(T(\\mathbf{y}^{*} \\geq T(\\mathbf{y})) | \\mathbf{y}) = 0.088\\] This says that in 88 of the 1000 simulated datasets, I saw a test ratio value as or more extreme that what I observed in real life. To me, that seems quite low. An ideal scenario would have this posterior predictive probability be close to 0.50. Thus, it seems like maybe I should not treat the data as coming from one giant state, and instead allow for variation on the four states. That’s coming in a few weeks…"
  },
  {
    "objectID": "homework/412_hw2_r.html",
    "href": "homework/412_hw2_r.html",
    "title": "STAT 412: Problem Set 2 (R)",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)\nprussian1 &lt;- read_csv(\"../handouts/prussianHorses1.csv\")\nprussian2 &lt;- read_csv(\"../handouts/prussianHorses2.csv\")\n\n\na &lt;- 1\nb &lt;- 2\nqgamma(0.95, a,b)\n\n[1] 1.497866\n\ny &lt;- prussian1$deaths\nn &lt;- length(prussian1$deaths)\ntheta_seq &lt;- seq(0.1, 5, 0.05)\nprior &lt;- dgamma(theta_seq, a, b)\na_post &lt;- a + sum(y)\nb_post &lt;- b + n\npost &lt;- dgamma(theta_seq, a_post, b_post)\nbayes_df &lt;- data.frame(theta = theta_seq, prior = prior, post = post) |&gt;\n  pivot_longer(cols= 2:3, names_to = \"distribution\", values_to = \"density\")\nbayes_df |&gt;\n  ggplot(aes(x = theta, y = density, col = distribution)) + \n  geom_line()\n\n\n\n\n\n\n\npost_prob &lt;-pgamma(0.5, a_post, b_post)\n\nThe posterior probability \\(\\text{Pr}(\\theta  &lt; 0.5 = \\mathbf{y}) = 0.0487284\\). Since this is relatively small, I would say that there is not strong evidence to suggest that the average rate of deaths by horsekick in a given year and cavalary is less than 0.5.\n\nset.seed(2)\ny_new &lt;- rnbinom(220, size = a_post, prob = (b_post)/(b_post + 1))\np1 &lt;- data.frame(y_new) |&gt;\n  ggplot(aes(x = y_new)) + \n  geom_bar() +\n  labs(x = \"Deaths\", title = \"Draws from posterior predictive distribution\")\n\np2 &lt;- prussian2 |&gt;\n  ggplot(aes(x = deaths)) +\n  geom_bar() +\n  labs(x = \"Deaths\", title = \"Held-out observations\")\n\np1 + p2"
  },
  {
    "objectID": "code/post_pred_checks.html#data",
    "href": "code/post_pred_checks.html#data",
    "title": "Simulating posterior and posterior predictive distributions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(MASS)\ndata(\"Aids2\")\n\nFrom the Aids2 dataset from MASS, we have data on patients diagnosed with AIDS in Australia before 1 July 1991. In particular, we are interested in the outcome status of each individual at the end of the observation (alive A or dead D). We also have data on state of origin:\n\n\n# A tibble: 4 × 5\n  state     A     D     n death_rate\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n1 NSW     664  1116  1780      0.627\n2 Other   107   142   249      0.570\n3 QLD      78   148   226      0.655\n4 VIC     233   355   588      0.604"
  },
  {
    "objectID": "code/post_pred_checks.html#model",
    "href": "code/post_pred_checks.html#model",
    "title": "Simulating posterior and posterior predictive distributions",
    "section": "Model",
    "text": "Model\nThe response is binary, so a Bernoulli sampling model could work here. While morbid, let us consider death as success. I’ll assume that individuals are conditionally iid given the probability of death, no matter the state:\n\\[Y_{i} | \\theta \\overset{iid}{\\sim} \\text{Bern}(\\theta) \\qquad i = 1,\\ldots, 2843\\] For my prior, I don’t know much about death rates due to AIDS, so I will use a rather uninformative prior:\n\\[\\theta \\sim \\text{Beta}(2,2)\\] Because \\(n\\) is so large, I expect my posterior to be guided almost fully by the observed data.\nUnder this sampling model and prior, we know the posterior distribution of \\(\\theta\\) and the PPD for a new data point \\(Y^{*}\\) exactly. However, we will obtain samples via simulation from the posterior and PPD instead."
  },
  {
    "objectID": "code/post_pred_checks.html#simulate-posterior-and-posterior-predictive-distributions",
    "href": "code/post_pred_checks.html#simulate-posterior-and-posterior-predictive-distributions",
    "title": "Simulating posterior and posterior predictive distributions",
    "section": "Simulate posterior and posterior predictive distributions",
    "text": "Simulate posterior and posterior predictive distributions\nIn particular, I will obtain \\(S = 5000\\) samples from each distribution:\n\nset.seed(412)\nsum_y &lt;- sum(Aids2$status == \"D\")\nn &lt;- nrow(Aids2)\na &lt;- 2\nb &lt;- 2\n\nS &lt;- 5000\na_star &lt;- a + sum_y\nb_star &lt;- b + n - sum_y\n\n### OPTION 1: for loop\ntheta_samps &lt;- rep(NA, S) \nystar_samps &lt;- rep(NA, S)\nfor(s in 1:S){\n  # sample theta from posterior\n  theta &lt;- rbeta(1, a_star, b_star)\n  \n  # sample y* from data model\n  ystar &lt;- rbinom(1, size = 1, prob = theta)\n  \n  # store\n  theta_samps[s] &lt;- theta\n  ystar_samps[s] &lt;- ystar\n}\n\n### OPTION 2: leverage vectorized language\ntheta_samps &lt;- rbeta(S, a_star, b_star)\nystar_samps &lt;- rbinom(S, size = 1, prob = theta_samps)\n\nYou could run the following code to see how well the simulations approximate the exact distributions:\n\n## Compare simulated vs exact posterior quantities\n# exact posterior mean\na_star / (a_star + b_star)\n# approximate posterior mean\nmean(theta_samps)\n\n# quantile-based 95% posterior credible interval: exact\nqbeta(c(0.025, 0.975), a_star, b_star)\n# quantile-based 95% posterior credible interval: Monte Carlo approximate\nquantile(theta_samps, c(0.025, 0.975))\n\n## Compare simulated PPD to exact\n# mean and var under exact PPD\na_star / (a_star + b_star)\n(a_star / (a_star + b_star)) * (1- (a_star / (a_star + b_star)))\n\n# approximate mean and var under simulated PPD\nmean(ystar_samps)\nvar(ystar_samps)\n\nWe would work with the theta_samps vector to answer questions about \\(\\theta\\). But let’s turn to model checking."
  },
  {
    "objectID": "code/hpd_code.html",
    "href": "code/hpd_code.html",
    "title": "Code for HPD",
    "section": "",
    "text": "# theta_samps: vector of random samples of theta\n# cred_mass: gamma (e.g. 0.95)\n# grid_size: the discretization/bandwidth we use to estimate the posterior density of theta\nget_hpd &lt;- function(theta_samps, cred_mass, grid_size = 10000) {\n  # Estimate true posterior density of theta using kernel density estimation.\n  dens &lt;- density(theta_samps, n = grid_size)\n  \n  dx &lt;- diff(dens$x[1:2])  \n  \n  ord &lt;- order(dens$y, decreasing = TRUE)\n  \n  cum_prob &lt;- cumsum(dens$y[ord]) * dx\n  \n  id &lt;- min(which(cum_prob &gt;= cred_mass))\n  dens_cutoff &lt;- dens$y[ord][id]\n  \n  inside &lt;- dens$y &gt;= dens_cutoff\n  \n  hpd_regions &lt;- list() \n  in_region &lt;- FALSE\n  for (i in seq_along(inside)) {\n    if (inside[i] && !in_region) { \n      start &lt;- dens$x[i] \n      in_region &lt;- TRUE \n    } else if ( (!inside[i]) & in_region) { \n      end &lt;- dens$x[i - 1]\n      hpd_regions[[length(hpd_regions) + 1]] &lt;- c(start, end)\n      in_region &lt;- FALSE \n    }\n  }\n  \n  if (in_region) {\n    hpd_regions[[length(hpd_regions) + 1]] &lt;- c(start, dens$x[length(dens$x)])\n  }\n  \n  return(hpd_regions)\n}"
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 412: Bayesian Statistics",
    "section": "",
    "text": "Welcome to the website for Middlebury College’s Fall 2025 STAT 412. On this website you will find the course syllabus, schedule, and assignments. The website is frequently updated throughout the semester, so please make a habit of refreshing the page."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#announcements",
    "href": "index.html#announcements",
    "title": "STAT 412: Bayesian Statistics",
    "section": "Announcements",
    "text": "Announcements\n\nPlease ensure R Studio functions on your laptop"
  },
  {
    "objectID": "index.html#course-details",
    "href": "index.html#course-details",
    "title": "STAT 412: Bayesian Statistics",
    "section": "Course details",
    "text": "Course details\nInstructor: Becky Tang (she/her)\n\nOffice: WNS 214\nEmail: btang@middlebury.edu\n\nMeeting times and location: Mondays and Wednesdays 9:45-11am in WNS 011\nOffice hours: Monday 3-4pm, Tuesday 3:30-4:30pm, Friday 11am-12pm\nSyllabus (most recent update 9/1/2025): our syllabus outlines our course policies and a rough schedule. Once classes begin, you should follow the schedule and due dates found on this website."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This page is frequently updated!\n\n\n\nWeek\nDate\nTopic\nAssignments\n\n\n\n\n1\nM 9/08\n\nWelcome!\nWhat is a statistical model?\n\n\nProblem Set 0 (due next class 9/10)\n\n\n\n\nW 9/10\n\nBayes’ rule, Bayes modeling\n\nLikelihood, prior, posterior\nCode and worksheet\n\n\n\nProblem Set 1 (due next class 9/15)\n\n\n\n2\nM 9/15\n\nBeta-Binomial model\n\nProportionality\n\nPosterior predictive distribution\n\n\nProblem Set 2 (due 9/25)\n\nAs of 9/15: Not all problems have been assigned yet!\n\n\n\n\n\nW 9/17\n\nEstimators\nConfidence regions"
  },
  {
    "objectID": "handouts.html",
    "href": "handouts.html",
    "title": "Handouts",
    "section": "",
    "text": "Distribution sheet"
  },
  {
    "objectID": "code/binom_discrete_prior.html",
    "href": "code/binom_discrete_prior.html",
    "title": "Learning about a Binomial Probability",
    "section": "",
    "text": "library(tidyverse)\nknitr::opts_chunk$set(fig.width = 6, fig.height = 3)"
  },
  {
    "objectID": "code/binom_discrete_prior.html#tidyverse-code",
    "href": "code/binom_discrete_prior.html#tidyverse-code",
    "title": "Learning about a Binomial Probability",
    "section": "Tidyverse code",
    "text": "Tidyverse code\n\n# PRIOR\ntheta &lt;- seq(0.1, 0.9, by = 0.1)\nprior1 &lt;- rep(1/9, 9)\nprior2 &lt;- c(0.05, 0.05, 0.05, 0.2, 0.3, 0.2, 0.05, 0.05, 0.05)\n\n# create data frame for visualization and wrangling\nbayes_table &lt;- data.frame(theta, prior1, prior2) \n\n# visualize different priors\nbayes_table |&gt;\n  pivot_longer(cols = 2:3, names_to = \"prior\", values_to = \"prior_probs\") |&gt;\n  mutate(theta = factor(theta)) |&gt; # for prettier visualization\n  ggplot(aes(x = theta, y = prior_probs)) + \n  geom_col() +\n  facet_wrap(~ prior) +\n  labs(x = expression(theta),\n      y = expression(f(theta)),\n      title = \"Prior\")\n\n\n\n\n\n\n\n# LIKELIHOOD\ny &lt;- 4\n## since R is vectorized:\nlike &lt;- dbinom(y, size = 11, prob = theta)\n## equivalently\n## like &lt;- choose(11, y) * theta^y * (1-theta)^(11-y)\nbayes_table &lt;- bayes_table |&gt;\n  add_column(likelihood = like) \n\n\n# MARGINAL LIKELIHOOD\nbayes_table &lt;- bayes_table |&gt;\n  mutate(product = prior2 * likelihood)\nmarg_like &lt;- sum(bayes_table$product)\n\n# POSTERIOR\nbayes_table &lt;- bayes_table |&gt;\n  mutate(posterior = product / marg_like)\nbayes_table |&gt;\n  select(theta, posterior)\n\n  theta      posterior\n1   0.1 0.006168515787\n2   0.2 0.043274594401\n3   0.3 0.086030889543\n4   0.4 0.369693507640\n5   0.5 0.377836937562\n6   0.6 0.109538817078\n7   0.7 0.006772110839\n8   0.8 0.000676165538\n9   0.9 0.000008461613\n\n# visualize prior vs posterior\n# note: could also plot in two separate plots without pivoting\nbayes_table |&gt;\n  pivot_longer(cols = c(\"prior1\", \"posterior\"), names_to = \"dist\", values_to = \"probs\") |&gt;\n  mutate(theta = factor(theta),\n         dist = factor(dist, levels = c(\"prior1\", \"posterior\"))) |&gt; # for prettier visualization\n  ggplot(aes(x = theta, y = probs)) + \n  geom_col() +\n  facet_wrap(~dist) +\n  labs(x = expression(theta),\n      y = \"Probability\",\n      title = expression(\"Comparison of \" * f(theta) * \" and \" * f(theta * \"| y\")))"
  },
  {
    "objectID": "code/binom_discrete_prior.html#base-r-code",
    "href": "code/binom_discrete_prior.html#base-r-code",
    "title": "Learning about a Binomial Probability",
    "section": "Base R code",
    "text": "Base R code\n\n## PRIOR\ntheta &lt;- seq(0.1, 0.9, by = 0.1)\nprior1 &lt;- rep(1/9, 9)\nprior2 &lt;- c(0.05, 0.05, 0.05, 0.2, 0.3, 0.2, 0.05, 0.05, 0.05)\n\nbayes_table &lt;- data.frame(theta, prior1, prior2)\n\n# Visualize prior 1\nbarplot(bayes_table$prior1, names.arg = bayes_table$theta,\n        xlab = expression(theta),\n        ylab = expression(f(theta)),\n        ylim = c(0, max(prior1) + 0.05),\n        main = \"Prior\")\n\n\n\n\n\n\n\n# LIKELIHOOD\ny &lt;- 4\nlike &lt;- dbinom(y, size = 11, prob = theta)\nbayes_table$likelihood &lt;- like\n\n# MARGINAL LIKELIHOOD\nbayes_table$product &lt;- prior1 * like\nmarg_like &lt;- sum(bayes_table$product)\n\n# POSTERIOR\nbayes_table$posterior &lt;-  bayes_table$product / marg_like\n\n# vizualize prior and posterior\npar(mfrow = c(1,2))\ny_max &lt;- max(c(prior1, bayes_table$posterior))\nbarplot(bayes_table$prior1, names.arg = bayes_table$theta,\n        xlab = expression(theta),\n        ylab = expression(f(theta)),\n        ylim = c(0, y_max + 0.05),\n        main = \"Prior\")\nbarplot(bayes_table$posterior, \n        names.arg = theta,\n        beside = T,\n        xlab = expression(theta),\n        ylab = expression(f(theta * \"| y\")),\n        ylim = c(0, y_max + 0.05),\n        main = \"Posterior\")"
  },
  {
    "objectID": "code/binom_discrete_prior.html#if-you-didnt-want-to-plot",
    "href": "code/binom_discrete_prior.html#if-you-didnt-want-to-plot",
    "title": "Learning about a Binomial Probability",
    "section": "If you didn’t want to plot",
    "text": "If you didn’t want to plot\n\nmarg_like &lt;- sum(prior1 * like)\npost &lt;- (prior1 * like)/marg_like\ncbind(theta, post)\n\n      theta          post\n [1,]   0.1 0.01893857939\n [2,]   0.2 0.13286167531\n [3,]   0.3 0.26413206805\n [4,]   0.4 0.28375828506\n [5,]   0.5 0.19333918362\n [6,]   0.6 0.08407652891\n [7,]   0.7 0.02079173713\n [8,]   0.8 0.00207596368\n [9,]   0.9 0.00002597885"
  },
  {
    "objectID": "code/proportionality.html",
    "href": "code/proportionality.html",
    "title": "Prior, likelihood, posterior, proportionality",
    "section": "",
    "text": "We often say the posterior distribution is “proportional to likelihood times prior”, where proportionality is with respect to \\(\\theta\\):\n\\[\\begin{align*}\nf_{\\theta | y}(\\theta | y) &= \\frac{f_{y | \\theta} (y | \\theta) f_{\\theta}(\\theta)}{f_{Y}(y)} \\\\\n& \\overset{\\theta}{\\propto} f_{y | \\theta} (y | \\theta) f_{\\theta}(\\theta)\n\\end{align*}\\]\nThat is, the posterior distribution is a function of \\(\\theta\\), so the marginal likelihood \\(f_{Y}(y)\\) is a constant with respect to \\(\\theta\\). It is just a scaling factor, as illustrated here:"
  }
]
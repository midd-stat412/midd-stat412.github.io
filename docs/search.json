[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 412: Bayesian Statistics",
    "section": "",
    "text": "Welcome to the website for Middlebury College’s Fall 2025 STAT 412. On this website you will find the course syllabus, schedule, and assignments. The website is frequently updated throughout the semester, so please make a habit of refreshing the page."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#announcements",
    "href": "index.html#announcements",
    "title": "STAT 412: Bayesian Statistics",
    "section": "Announcements",
    "text": "Announcements\n\nWeek 6 office hours:\n\nMonday 10/13 and Tuesday 10/14 cancelled\nWednesday 10/15 from 3-4pm\nThursday 10/16 from 10-11am\nFriday 10/17 from 11am-12pm (usual)\n\nMidterm on Friday 10/17 from 2:30-4:30pm in WNS 011 (our usual classroom)\n\nSome practice problems here"
  },
  {
    "objectID": "index.html#course-details",
    "href": "index.html#course-details",
    "title": "STAT 412: Bayesian Statistics",
    "section": "Course details",
    "text": "Course details\nInstructor: Becky Tang (she/her)\n\nOffice: WNS 214\nEmail: btang@middlebury.edu\n\nMeeting times and location: Mondays and Wednesdays 9:45-11am in WNS 011\nOffice hours: Monday 3-4pm, Tuesday 3:30-4:30pm, Friday 11am-12pm\nSyllabus (most recent update 9/1/2025): our syllabus outlines our course policies and a rough schedule. Once classes begin, you should follow the schedule and due dates found on this website."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This page is frequently updated!\n\n\n\nWeek\nDate\nTopic\nAssignments\n\n\n\n\n1\nM 9/08\n\nWelcome!\nWhat is a statistical model?\n\n\nProblem Set 0 (due next class 9/10)\n\n\n\n\nW 9/10\n\nBayes’ rule, Bayes modeling\n\nLikelihood, prior, posterior\nCode and worksheet\n\n\n\nProblem Set 1 (due next class 9/15)\n\n\n\n2\nM 9/15\n\nBeta-Binomial model\n\nProportionality\n\nPosterior predictive distribution\n\n\nProblem Set 2 (due 9/24)\n\n\n\n\nW 9/17\n\nBayes estimators/estimates\n\nSome proofs\n\n\n\nProblem Set 2 (due 9/24)\n\n\n\n3\nM 9/22\n\nConfidence regions\nPosterior quantities and posterior predictive checks via simulation\n\nCode\n\n\n\n\n\n\nW 9/24\n\nConjugacy\nNormal sampling model\n\n\nProblem Set 3 (complete!)\nCode for HW 3\n\n\n\n4\nM 9/29\n\nMarkov chain basics\n\n\n\n\n\nW 10/01\n\nNormal model (cont.) with Gibbs sampling!\n\nData under Handouts\nCode from class\n\n\n\nFor Monday, read the article Explaining the Gibbs Sampler under Handouts and prepare to discuss!\nProblem Set 4\n\n\n\n5\nM 10/06\n\nDiscuss Explaining the Gibbs Sampler\nMCMC diagnostics\n\nCode here\n\n\n\n\n\n\nW 10/06\n\nFinish MCMC diagnostics\nJeffrey’s prior\nIntroduce Case Study 1\n\n\nWork on Case Study 1\n\n\n\n6\nM 10/13\nBecky out of town; Prof. Christian Stratton will be here in my stead!\n\nWork on Case Study 1\nWork on midterm review\n\n\nWork on Case Study 1\n\n\n\n\nW 10/15\n\nPresent Case Study 1\n\n\nStudy for midterm!"
  },
  {
    "objectID": "handouts.html",
    "href": "handouts.html",
    "title": "Handouts",
    "section": "",
    "text": "Distribution sheet (updated 10/11 with Inverse Gamma dist.)\nExplaining the Gibbs Sampler (1992)\n\nDiscussion questions posted here!\n\nConstrained normal sampling code\nData\n\n prussianHorses1 \n prussianHorses2 \n school1  Load code in as readRDS(\"filepath/school1.Rda\")\n divorce  Load code in as readRDS(\"filepath/divorce.Rda\")"
  },
  {
    "objectID": "code/binom_discrete_prior.html",
    "href": "code/binom_discrete_prior.html",
    "title": "Learning about a Binomial Probability",
    "section": "",
    "text": "library(tidyverse)\nknitr::opts_chunk$set(fig.width = 6, fig.height = 3)"
  },
  {
    "objectID": "code/binom_discrete_prior.html#tidyverse-code",
    "href": "code/binom_discrete_prior.html#tidyverse-code",
    "title": "Learning about a Binomial Probability",
    "section": "Tidyverse code",
    "text": "Tidyverse code\n\n# PRIOR\ntheta &lt;- seq(0.1, 0.9, by = 0.1)\nprior1 &lt;- rep(1/9, 9)\nprior2 &lt;- c(0.05, 0.05, 0.05, 0.2, 0.3, 0.2, 0.05, 0.05, 0.05)\n\n# create data frame for visualization and wrangling\nbayes_table &lt;- data.frame(theta, prior1, prior2) \n\n# visualize different priors\nbayes_table |&gt;\n  pivot_longer(cols = 2:3, names_to = \"prior\", values_to = \"prior_probs\") |&gt;\n  mutate(theta = factor(theta)) |&gt; # for prettier visualization\n  ggplot(aes(x = theta, y = prior_probs)) + \n  geom_col() +\n  facet_wrap(~ prior) +\n  labs(x = expression(theta),\n      y = expression(f(theta)),\n      title = \"Prior\")\n\n\n\n\n\n\n\n# LIKELIHOOD\ny &lt;- 4\n## since R is vectorized:\nlike &lt;- dbinom(y, size = 11, prob = theta)\n## equivalently\n## like &lt;- choose(11, y) * theta^y * (1-theta)^(11-y)\nbayes_table &lt;- bayes_table |&gt;\n  add_column(likelihood = like) \n\n\n# MARGINAL LIKELIHOOD\nbayes_table &lt;- bayes_table |&gt;\n  mutate(product = prior2 * likelihood)\nmarg_like &lt;- sum(bayes_table$product)\n\n# POSTERIOR\nbayes_table &lt;- bayes_table |&gt;\n  mutate(posterior = product / marg_like)\nbayes_table |&gt;\n  select(theta, posterior)\n\n  theta      posterior\n1   0.1 0.006168515787\n2   0.2 0.043274594401\n3   0.3 0.086030889543\n4   0.4 0.369693507640\n5   0.5 0.377836937562\n6   0.6 0.109538817078\n7   0.7 0.006772110839\n8   0.8 0.000676165538\n9   0.9 0.000008461613\n\n# visualize prior vs posterior\n# note: could also plot in two separate plots without pivoting\nbayes_table |&gt;\n  pivot_longer(cols = c(\"prior1\", \"posterior\"), names_to = \"dist\", values_to = \"probs\") |&gt;\n  mutate(theta = factor(theta),\n         dist = factor(dist, levels = c(\"prior1\", \"posterior\"))) |&gt; # for prettier visualization\n  ggplot(aes(x = theta, y = probs)) + \n  geom_col() +\n  facet_wrap(~dist) +\n  labs(x = expression(theta),\n      y = \"Probability\",\n      title = expression(\"Comparison of \" * f(theta) * \" and \" * f(theta * \"| y\")))"
  },
  {
    "objectID": "code/binom_discrete_prior.html#base-r-code",
    "href": "code/binom_discrete_prior.html#base-r-code",
    "title": "Learning about a Binomial Probability",
    "section": "Base R code",
    "text": "Base R code\n\n## PRIOR\ntheta &lt;- seq(0.1, 0.9, by = 0.1)\nprior1 &lt;- rep(1/9, 9)\nprior2 &lt;- c(0.05, 0.05, 0.05, 0.2, 0.3, 0.2, 0.05, 0.05, 0.05)\n\nbayes_table &lt;- data.frame(theta, prior1, prior2)\n\n# Visualize prior 1\nbarplot(bayes_table$prior1, names.arg = bayes_table$theta,\n        xlab = expression(theta),\n        ylab = expression(f(theta)),\n        ylim = c(0, max(prior1) + 0.05),\n        main = \"Prior\")\n\n\n\n\n\n\n\n# LIKELIHOOD\ny &lt;- 4\nlike &lt;- dbinom(y, size = 11, prob = theta)\nbayes_table$likelihood &lt;- like\n\n# MARGINAL LIKELIHOOD\nbayes_table$product &lt;- prior1 * like\nmarg_like &lt;- sum(bayes_table$product)\n\n# POSTERIOR\nbayes_table$posterior &lt;-  bayes_table$product / marg_like\n\n# vizualize prior and posterior\npar(mfrow = c(1,2))\ny_max &lt;- max(c(prior1, bayes_table$posterior))\nbarplot(bayes_table$prior1, names.arg = bayes_table$theta,\n        xlab = expression(theta),\n        ylab = expression(f(theta)),\n        ylim = c(0, y_max + 0.05),\n        main = \"Prior\")\nbarplot(bayes_table$posterior, \n        names.arg = theta,\n        beside = T,\n        xlab = expression(theta),\n        ylab = expression(f(theta * \"| y\")),\n        ylim = c(0, y_max + 0.05),\n        main = \"Posterior\")"
  },
  {
    "objectID": "code/binom_discrete_prior.html#if-you-didnt-want-to-plot",
    "href": "code/binom_discrete_prior.html#if-you-didnt-want-to-plot",
    "title": "Learning about a Binomial Probability",
    "section": "If you didn’t want to plot",
    "text": "If you didn’t want to plot\n\nmarg_like &lt;- sum(prior1 * like)\npost &lt;- (prior1 * like)/marg_like\ncbind(theta, post)\n\n      theta          post\n [1,]   0.1 0.01893857939\n [2,]   0.2 0.13286167531\n [3,]   0.3 0.26413206805\n [4,]   0.4 0.28375828506\n [5,]   0.5 0.19333918362\n [6,]   0.6 0.08407652891\n [7,]   0.7 0.02079173713\n [8,]   0.8 0.00207596368\n [9,]   0.9 0.00002597885"
  },
  {
    "objectID": "code/proportionality.html",
    "href": "code/proportionality.html",
    "title": "Prior, likelihood, posterior, proportionality",
    "section": "",
    "text": "We often say the posterior distribution is “proportional to likelihood times prior”, where proportionality is with respect to \\(\\theta\\):\n\\[\\begin{align*}\nf_{\\theta | y}(\\theta | y) &= \\frac{f_{y | \\theta} (y | \\theta) f_{\\theta}(\\theta)}{f_{Y}(y)} \\\\\n& \\overset{\\theta}{\\propto} f_{y | \\theta} (y | \\theta) f_{\\theta}(\\theta)\n\\end{align*}\\]\nThat is, the posterior distribution is a function of \\(\\theta\\), so the marginal likelihood \\(f_{Y}(y)\\) is a constant with respect to \\(\\theta\\). It is just a scaling factor, as illustrated here:"
  },
  {
    "objectID": "handouts/bayes_est.html#theorem",
    "href": "handouts/bayes_est.html#theorem",
    "title": "Bayes estimator proofs",
    "section": "Theorem",
    "text": "Theorem\nUnder squared loss \\(L(\\theta, a) = (\\theta - a)^2\\), the Bayes estimator for \\(\\theta\\) is the posterior mean of \\(\\theta\\), \\(\\mathbb{E}[\\theta | \\mathbf{y}]\\).\n\nWant to show that \\(\\mathbb{E}[\\theta | \\mathbf{y}] = \\underset{a}{\\text{argmin}} \\ \\mathbb{E}[(\\theta - a)^2 | \\mathbf{y}]\\)\nRecall: \\(\\mathbb{E}[\\theta | \\mathbf{y}] = \\int_{\\Theta} \\theta f(\\theta | \\mathbf{y}) d\\theta\\) is a function of \\(\\mathbf{y}\\)! We’ve integrated out \\(\\theta\\)!\nAlso, for any function \\(g\\), \\(\\mathbb{E}[g(\\mathbf{y}) | \\mathbf{y}] = g(\\mathbf{y})\\).\n\nThat is, since we condition on \\(\\mathbf{y}\\) (not random), the expectation of any function of \\(\\mathbf{y}\\) is itself"
  },
  {
    "objectID": "handouts/bayes_est.html#proof-set-up",
    "href": "handouts/bayes_est.html#proof-set-up",
    "title": "Bayes estimator proofs",
    "section": "Proof set-up",
    "text": "Proof set-up\n\nWant to show that \\(m = \\underset{a}{\\text{argmin}} \\ \\mathbb{E}[ |\\theta - a| | \\mathbf{y}]\\)\nNote that the absolute value function is not differentiable, so proving a maximum/minimum cannot rely on derivatives!\nAssume that \\(\\theta\\) is continuous, so that if \\(m\\) is the posterior median, then \\(\\text{Pr}(\\theta \\geq m | \\mathbf{y}) = \\frac{1}{2} = \\text{Pr}(\\theta \\leq m | \\mathbf{y})\\).\nLet \\(a\\) be any other estimator of \\(\\theta\\).\nWe will show that\n\\[\n\\mathbb{E}[L(\\theta , a) | \\mathbf{y}] - \\mathbb{E}[L(\\theta, m) | \\mathbf{y}]  \\geq 0\n\\]\nthus demonstrating that \\(m\\) minimizes the expected loss.\nSome recap from probability: if \\(Y\\) continuous with pdf \\(f\\),\n\n\\(\\text{Pr}(Y \\leq a) = \\int_{-\\infty}^{a} f(y) dy\\)\n\\(\\text{Pr}(a \\leq Y \\leq b) = \\text{Pr}(Y\\leq b) - \\text{Pr}(Y \\leq a)\\) and these inequalities could be \\(\\leq\\) or \\(&lt;\\)"
  },
  {
    "objectID": "handouts/bayes_est.html#proof",
    "href": "handouts/bayes_est.html#proof",
    "title": "Bayes estimator proofs",
    "section": "Proof",
    "text": "Proof\nLet \\(m\\) be the posterior median, and suppose \\(a &lt; m\\) is any other estimator. Remember, \\(a\\) and \\(m\\) are constant w.r.t. \\(\\theta\\).\n\\[\n\\begin{align}\n\\mathbb{E}[L(\\theta , a) | \\mathbf{y}] & - \\mathbb{E}[L(\\theta, m) | \\mathbf{y}] = \\int_{\\Theta} |\\theta - a| f(\\theta | \\mathbf{y}) d\\theta - \\int_{\\Theta} |\\theta - m| f(\\theta | \\mathbf{y})d\\theta \\\\\n&\\class{fragment}{= \\int_{\\Theta} \\left(|\\theta - a| - |\\theta - m|\\right) f(\\theta | \\mathbf{y})d\\theta } \\\\\n&\\class{fragment}{= \\int_{-\\infty}^{a} \\left(|\\theta - a| - |\\theta - m|\\right) f(\\theta | \\mathbf{y})d\\theta + \\int_{a}^{m} \\left(|\\theta - a| - |\\theta - m|\\right) f(\\theta | \\mathbf{y})d\\theta + \\int_{m}^{\\infty} \\left(|\\theta - a| - |\\theta - m|\\right) f(\\theta | \\mathbf{y})d\\theta }\\\\\n& \\class{fragment}{= \\int_{-\\infty}^{a} ((a-\\theta) - ( m - \\theta)) f(\\theta | \\mathbf{y}) d\\theta + \\int_{a}^{m} ((\\theta - a) - (m- \\theta)) f(\\theta | \\mathbf{y}) d\\theta  +\n\\int_{m}^{\\infty} ((\\theta - a) - (\\theta - m)) f(\\theta | \\mathbf{y}) d\\theta} \\\\\n&\\class{fragment}{= \\int_{-\\infty}^{a} (a - m)  f(\\theta | \\mathbf{y}) d\\theta + \\int_{a}^{m} (2\\theta - a- m)  f(\\theta | \\mathbf{y}) d\\theta + \\int_{m}^{\\infty} (m-a) f(\\theta | \\mathbf{y}) d\\theta} \\\\\n&\\class{fragment}{\\color{orange}{\\geq}  \\int_{-\\infty}^{a} (a - m)  f(\\theta | \\mathbf{y}) d\\theta + \\int_{a}^{m} (2\\color{orange}{a} - a- m)  f(\\theta | \\mathbf{y}) d\\theta + \\int_{m}^{\\infty} (m-a) f(\\theta | \\mathbf{y}) d\\theta} \\\\\n&\\class{fragment}{= (a-m)\\text{Pr}(\\theta \\leq a | \\mathbf{y}) + \\color{purple}{(a-m)\\text{Pr}(a  &lt; \\theta \\leq m | \\mathbf{y}) }+ (m-a) \\text{Pr}(\\theta \\geq m | \\mathbf{y})} \\\\\n&\\class{fragment}{ = (a-m)\\text{Pr}(\\theta \\leq a | \\mathbf{y}) + \\color{purple}{(a-m) \\text{Pr}(\\theta \\leq m | \\mathbf{y})  - (a-m) \\text{Pr}(\\theta \\leq a | \\mathbf{y})} + (m-a) \\text{Pr}(\\theta \\geq m | \\mathbf{y}) } \\\\\n&\\class{fragment}{= (a-m) \\text{Pr}(\\theta \\leq m | \\mathbf{y}) +  (m-a) \\text{Pr}(\\theta \\geq m | \\mathbf{y})} \\\\\n&\\class{fragment}{= (a-m)\\left(\\frac{1}{2}\\right) + (m-a)\\left(\\frac{1}{2}\\right)} \\\\\n&\\class{fragment}{= (a-m)\\left(\\frac{1}{2}\\right) - (a-m)\\left(\\frac{1}{2}\\right)}\\\\\n&\\class{fragment}{= 0 \\qquad \\tiny{\\square} }\n\\end{align}\n\\]"
  },
  {
    "objectID": "handouts/bayes_est.html#proof-2",
    "href": "handouts/bayes_est.html#proof-2",
    "title": "Bayes estimator proofs",
    "section": "Proof 2",
    "text": "Proof 2\nApologies; you’ll have to do several “right clicks” until you reach the next slide.\nLet’s begin by manipulating the term we’d like to minimize: \\[\\begin{align}\n\\mathbb{E}[(\\theta - a)^2 | \\mathbf{y}] &= \\mathbb{E}[( \\color{red}{(} \\theta - \\mathbb{E}[\\theta | \\mathbf{y}]\\color{red}{)} + \\color{red}{(}\\mathbb{E}[\\theta | \\mathbf{y}]  - a\\color{red}{)})^2 | \\mathbf{y}] \\\\\n&\\class{fragment}{\\overset{\\text{FOIL}}{=} \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}] + 2\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])(\\mathbb{E}[\\theta | \\mathbf{y}]-a) | \\mathbf{y}] + \\mathbb{E}[(\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2 | \\mathbf{y}]} \\\\\n&\\class{fragment}{=\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}] + 2\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])\\color{orange}{(\\mathbb{E}[\\theta | \\mathbf{y}]-a)} | \\mathbf{y}] + \\mathbb{E}[\\color{orange}{(\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2} | \\mathbf{y}]} \\\\\n& \\class{fragment}{\\text{Note: }  \\color{orange}{\\mathbb{E}[\\theta | \\mathbf{y}]} \\text{ and } \\color{orange}{a} \\text{ are constant w.r.t } \\mathbf{y}}  \\\\\n&\\class{fragment}{=\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}]  + 2 \\color{orange}{(\\mathbb{E}[\\theta | \\mathbf{y}]-a)}\\color{purple}{\\mathbb{E}[\\theta - \\mathbb{E}[\\theta | \\mathbf{y}] | \\mathbf{y}]}+ \\color{orange}{(\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2}} \\\\\n&\\class{fragment}{=\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}]  + 2 (\\mathbb{E}[\\theta | \\mathbf{y}]-a) \\color{purple}{( \\mathbb{E}[\\theta | \\mathbf{y}] - \\mathbb{E}[\\theta | \\mathbf{y}])} + (\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2} \\\\\n&\\class{fragment}{=\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}]  + (\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2}\n\\end{align}\n\\]"
  },
  {
    "objectID": "handouts/bayes_est.html#theorem-1",
    "href": "handouts/bayes_est.html#theorem-1",
    "title": "Bayes estimator proofs",
    "section": "Theorem",
    "text": "Theorem\nUnder absolute loss \\(L(\\theta, a) = |\\theta - a|\\), a Bayes estimator for \\(\\theta\\) is any posterior median of \\(\\theta\\).\n\nThat is, a Bayes estimator is a function \\(\\delta(\\mathbf{y}) \\equiv m\\) such that \\(\\text{Pr}(\\theta \\leq m | \\mathbf{y}) \\geq \\frac{1}{2}\\) and \\(\\text{Pr}(\\theta \\geq m | \\mathbf{y}) \\geq \\frac{1}{2}\\).\nNote that when \\(\\theta\\) is continuous, there exists a single median."
  },
  {
    "objectID": "handouts/bayes_est.html#proof-2-cont.",
    "href": "handouts/bayes_est.html#proof-2-cont.",
    "title": "Bayes estimator proofs",
    "section": "Proof 2 (cont.)",
    "text": "Proof 2 (cont.)\nThus, \\[\n\\underset{a}{\\text{argmin}} \\ \\mathbb{E}[(\\theta - a)^2 | \\mathbf{y}] = \\underset{a}{\\text{argmin}} \\ \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta | \\mathbf{y}])^2 | \\mathbf{y}]  + (\\mathbb{E}[\\theta | \\mathbf{y}]-a)^2\n\\]\n\nThe first term does not depend on \\(a\\), so cannot do anything about minimizing w.r.t \\(a\\)\nThus, focus on minimizing the second term. Minimized when \\(a = \\mathbb{E}[\\theta | \\mathbf{y}]\\).\nThus, the Bayes estimate under squared loss is the posterior mean. \\(\\tiny{\\square}\\)"
  },
  {
    "objectID": "handouts/bayes_est.html#temp",
    "href": "handouts/bayes_est.html#temp",
    "title": "Bayes estimator proofs",
    "section": "temp",
    "text": "temp"
  },
  {
    "objectID": "handouts/bayes_est.html#incremental-derivation",
    "href": "handouts/bayes_est.html#incremental-derivation",
    "title": "Bayes estimator proofs",
    "section": "Incremental Derivation",
    "text": "Incremental Derivation\n\\[\n\\begin{align}\n\\mathbb{E}[(\\theta - a)^2 \\mid \\mathbf{y}] &= \\mathbb{E}[\\big( \\color{red}{(} \\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}]\\color{red}{)} + \\color{red}{(}\\mathbb{E}[\\theta \\mid \\mathbf{y}]  - a\\color{red}{)}\\big)^2 \\mid \\mathbf{y}]\n\\end{align}\n\\]\n.fragment[ \\[\n\\begin{align}\n&= \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])^2 \\mid \\mathbf{y}] \\\\\n&\\quad + 2\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a) \\mid \\mathbf{y}] \\\\\n&\\quad + \\mathbb{E}[(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)^2 \\mid \\mathbf{y}]\n\\end{align}\n\\]]\n.fragment[ \\[\n\\begin{align}\n&= \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])^2 \\mid \\mathbf{y}] \\\\\n&\\quad + 2\\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])\\color{orange}{(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)} \\mid \\mathbf{y}] \\\\\n&\\quad + \\mathbb{E}[\\color{orange}{(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)^2} \\mid \\mathbf{y}]\n\\end{align}\n\\]]\n.fragment[ &gt; Note: () and () are constant with respect to ()]\n.fragment[ \\[\n\\begin{align}\n&= \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])^2 \\mid \\mathbf{y}] \\\\\n&\\quad + 2 \\color{orange}{(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)} \\color{purple}{\\mathbb{E}[\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}] \\mid \\mathbf{y}]} \\\\\n&\\quad + \\color{orange}{(\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)^2}\n\\end{align}\n\\]]\n.fragment[ \\[\n\\begin{align}\n&= \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])^2 \\mid \\mathbf{y}] \\\\\n&\\quad + 2 (\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a) \\color{purple}{( \\mathbb{E}[\\theta \\mid \\mathbf{y}] - \\mathbb{E}[\\theta \\mid \\mathbf{y}])} \\\\\n&\\quad + (\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)^2\n\\end{align}\n\\]]\n.fragment[ \\[\n\\begin{align}\n&= \\mathbb{E}[(\\theta - \\mathbb{E}[\\theta \\mid \\mathbf{y}])^2 \\mid \\mathbf{y}] + (\\mathbb{E}[\\theta \\mid \\mathbf{y}]-a)^2\n\\end{align}\n\\]]"
  },
  {
    "objectID": "code/post_pred_checks.html",
    "href": "code/post_pred_checks.html",
    "title": "Simulating posterior and posterior predictive distributions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(MASS)\ndata(\"Aids2\")\nFrom the Aids2 dataset from MASS, we have data on patients diagnosed with AIDS in Australia before 1 July 1991. In particular, we are interested in the outcome status of each individual at the end of the observation (alive A or dead D). We also have data on state of origin:\n# A tibble: 4 × 5\n  state     A     D     n death_rate\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n1 NSW     664  1116  1780      0.627\n2 Other   107   142   249      0.570\n3 QLD      78   148   226      0.655\n4 VIC     233   355   588      0.604"
  },
  {
    "objectID": "code/post_pred_checks.html#posterior-predictive-check",
    "href": "code/post_pred_checks.html#posterior-predictive-check",
    "title": "Simulating posterior and posterior predictive distributions",
    "section": "Posterior predictive check",
    "text": "Posterior predictive check\nNow, let’s do some posterior predictive checks (PPC). One thing I might be wondering is if my assumption of iid across the 4 states is reasonble.\nFirst, let me simulate \\(R = 1000\\) new datasets of size 2843 from the PPD. For each dataset \\(r\\), I can assign state values according the observed data because I assumed state doesn’t matter (i.e. state and death status are independent).\n\nset.seed(412)\n# posterior predictive check\nR &lt;- 1000\ndf_ls &lt;- list()\nfor(r in 1:R){\n  theta_samp &lt;- rbeta(1, a + sum_y, b + n - sum_y)\n  ystar_samps &lt;- rbinom(n, size = 1, prob = theta_samp)\n  df_ls[[r]] &lt;- data.frame(status = ystar_samps) |&gt;\n    add_column(state = Aids2$state)\n}\n# Random sample of three rows in first simulated dataset:\ndf_ls[[1]] |&gt;\n  sample_n(3)\n\n  status state\n1      0   NSW\n2      1   NSW\n3      0   NSW\n\n\nVisually, we can look at how the distribution of deaths varies across the states in some of the simulated datasets:\n\n\n\n\n\n\n\n\n\nLooks pretty similar to the plot of the original data above, but humans like to see what they want to see.\nFor a more robust PPC, let’s consider the following test function \\(T(\\mathbf{y})\\): the ratio of the highest death rate and the lowest death rate among the four states.\nIn the observed data, this is 0.655/0.57 = 1.148. Let’s now obtain the values of the test function for the 1000 simualated datasets:\n\nppd_T &lt;- rep(NA, R)\nfor(r in 1:R){\n  temp &lt;- df_ls[[r]] |&gt;\n    group_by(state) |&gt;\n    summarise(death_rate = mean(status))\n  ppd_T[r] &lt;- max(temp$death_rate)/min(temp$death_rate)\n}\n\n\n\n\n\n\n\n\n\n\nThe posterior predictive probability of a ratio as or more extreme than the observed \\(T(\\mathbf{y}) = 1.148\\) is:\n\n# monte carlo approximation\npost_prob_T &lt;- mean(ppd_T &gt;= obs_T)\n\n\\[\\text{Pr}(T(\\mathbf{y}^{*} \\geq T(\\mathbf{y})) | \\mathbf{y}) = 0.088\\] This says that in 88 of the 1000 simulated datasets, I saw a test ratio value as or more extreme that what I observed in real life. To me, that seems quite low. An ideal scenario would have this posterior predictive probability be close to 0.50. Thus, it seems like maybe I should not treat the data as coming from one giant state, and instead allow for variation on the four states. That’s coming in a few weeks…"
  },
  {
    "objectID": "homework/412_hw2_r.html",
    "href": "homework/412_hw2_r.html",
    "title": "STAT 412: Problem Set 2 (R)",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)\nprussian1 &lt;- read_csv(\"../handouts/prussianHorses1.csv\")\nprussian2 &lt;- read_csv(\"../handouts/prussianHorses2.csv\")\n\n\na &lt;- 1\nb &lt;- 2\nqgamma(0.95, a,b)\n\n[1] 1.497866\n\ny &lt;- prussian1$deaths\nn &lt;- length(prussian1$deaths)\ntheta_seq &lt;- seq(0.1, 5, 0.05)\nprior &lt;- dgamma(theta_seq, a, b)\na_post &lt;- a + sum(y)\nb_post &lt;- b + n\npost &lt;- dgamma(theta_seq, a_post, b_post)\nbayes_df &lt;- data.frame(theta = theta_seq, prior = prior, post = post) |&gt;\n  pivot_longer(cols= 2:3, names_to = \"distribution\", values_to = \"density\")\nbayes_df |&gt;\n  ggplot(aes(x = theta, y = density, col = distribution)) + \n  geom_line()\n\n\n\n\n\n\n\npost_prob &lt;-pgamma(0.5, a_post, b_post)\n\nThe posterior probability \\(\\text{Pr}(\\theta  &lt; 0.5 = \\mathbf{y}) = 0.0487284\\). Since this is relatively small, I would say that there is not strong evidence to suggest that the average rate of deaths by horsekick in a given year and cavalary is less than 0.5.\n\nset.seed(2)\ny_new &lt;- rnbinom(220, size = a_post, prob = (b_post)/(b_post + 1))\np1 &lt;- data.frame(y_new) |&gt;\n  ggplot(aes(x = y_new)) + \n  geom_bar() +\n  labs(x = \"Deaths\", title = \"Draws from posterior predictive distribution\")\n\np2 &lt;- prussian2 |&gt;\n  ggplot(aes(x = deaths)) +\n  geom_bar() +\n  labs(x = \"Deaths\", title = \"Held-out observations\")\n\np1 + p2"
  },
  {
    "objectID": "code/post_pred_checks.html#data",
    "href": "code/post_pred_checks.html#data",
    "title": "Simulating posterior and posterior predictive distributions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(MASS)\ndata(\"Aids2\")\n\nFrom the Aids2 dataset from MASS, we have data on patients diagnosed with AIDS in Australia before 1 July 1991. In particular, we are interested in the outcome status of each individual at the end of the observation (alive A or dead D). We also have data on state of origin:\n\n\n# A tibble: 4 × 5\n  state     A     D     n death_rate\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n1 NSW     664  1116  1780      0.627\n2 Other   107   142   249      0.570\n3 QLD      78   148   226      0.655\n4 VIC     233   355   588      0.604"
  },
  {
    "objectID": "code/post_pred_checks.html#model",
    "href": "code/post_pred_checks.html#model",
    "title": "Simulating posterior and posterior predictive distributions",
    "section": "Model",
    "text": "Model\nThe response is binary, so a Bernoulli sampling model could work here. While morbid, let us consider death as success. I’ll assume that individuals are conditionally iid given the probability of death, no matter the state:\n\\[Y_{i} | \\theta \\overset{iid}{\\sim} \\text{Bern}(\\theta) \\qquad i = 1,\\ldots, 2843\\] For my prior, I don’t know much about death rates due to AIDS, so I will use a rather uninformative prior:\n\\[\\theta \\sim \\text{Beta}(2,2)\\] Because \\(n\\) is so large, I expect my posterior to be guided almost fully by the observed data.\nUnder this sampling model and prior, we know the posterior distribution of \\(\\theta\\) and the PPD for a new data point \\(Y^{*}\\) exactly. However, we will obtain samples via simulation from the posterior and PPD instead."
  },
  {
    "objectID": "code/post_pred_checks.html#simulate-posterior-and-posterior-predictive-distributions",
    "href": "code/post_pred_checks.html#simulate-posterior-and-posterior-predictive-distributions",
    "title": "Simulating posterior and posterior predictive distributions",
    "section": "Simulate posterior and posterior predictive distributions",
    "text": "Simulate posterior and posterior predictive distributions\nIn particular, I will obtain \\(S = 5000\\) samples from each distribution:\n\nset.seed(412)\nsum_y &lt;- sum(Aids2$status == \"D\")\nn &lt;- nrow(Aids2)\na &lt;- 2\nb &lt;- 2\n\nS &lt;- 5000\na_star &lt;- a + sum_y\nb_star &lt;- b + n - sum_y\n\n### OPTION 1: for loop\ntheta_samps &lt;- rep(NA, S) \nystar_samps &lt;- rep(NA, S)\nfor(s in 1:S){\n  # sample theta from posterior\n  theta &lt;- rbeta(1, a_star, b_star)\n  \n  # sample y* from data model\n  ystar &lt;- rbinom(1, size = 1, prob = theta)\n  \n  # store\n  theta_samps[s] &lt;- theta\n  ystar_samps[s] &lt;- ystar\n}\n\n### OPTION 2: leverage vectorized language\ntheta_samps &lt;- rbeta(S, a_star, b_star)\nystar_samps &lt;- rbinom(S, size = 1, prob = theta_samps)\n\nYou could run the following code to see how well the simulations approximate the exact distributions:\n\n## Compare simulated vs exact posterior quantities\n# exact posterior mean\na_star / (a_star + b_star)\n# approximate posterior mean\nmean(theta_samps)\n\n# quantile-based 95% posterior credible interval: exact\nqbeta(c(0.025, 0.975), a_star, b_star)\n# quantile-based 95% posterior credible interval: Monte Carlo approximate\nquantile(theta_samps, c(0.025, 0.975))\n\n## Compare simulated PPD to exact\n# mean and var under exact PPD\na_star / (a_star + b_star)\n(a_star / (a_star + b_star)) * (1- (a_star / (a_star + b_star)))\n\n# approximate mean and var under simulated PPD\nmean(ystar_samps)\nvar(ystar_samps)\n\nWe would work with the theta_samps vector to answer questions about \\(\\theta\\). But let’s turn to model checking."
  },
  {
    "objectID": "code/hpd_code.html",
    "href": "code/hpd_code.html",
    "title": "Code for HPD",
    "section": "",
    "text": "The density() function in R estimates the density of a random variable based off an iid sample from the density. It works by chopping up the “x-axis” or support of the density (i.e. the observed range from the iid sample) into small pieces, called coordinates. The function then estimates the density at each one of those coordinates based on the iid sample. The coordinates are the x output from density(), and the estimated density values are the y output from the function. These are accessed accessed via the $ symbol when the returned value from the function is stored as a variable (dens below).\n\n# theta_samps: vector of random samples of theta\n# cred_mass: gamma (e.g. 0.95)\n# grid_size: the bandwidth/number of coordinates used to estimate density\nget_hpd &lt;- function(theta_samps, cred_mass, grid_size = 10000) {\n  # Estimate true posterior density of theta using kernel density estimation.\n  dens &lt;- density(theta_samps, n = grid_size)\n  \n  dx &lt;- diff(dens$x[1:2])  \n  \n  ord &lt;- order(dens$y, decreasing = TRUE)\n  \n  cum_prob &lt;- cumsum(dens$y[ord]) * dx\n  \n  id &lt;- min(which(cum_prob &gt;= cred_mass))\n  dens_cutoff &lt;- dens$y[ord][id]\n  \n  inside &lt;- dens$y &gt;= dens_cutoff\n  \n  hpd_regions &lt;- list() \n  in_region &lt;- FALSE\n  for (i in seq_along(inside)) {\n    if (inside[i] && !in_region) { \n      start &lt;- dens$x[i] \n      in_region &lt;- TRUE \n    } else if ( (!inside[i]) & in_region) { \n      end &lt;- dens$x[i - 1]\n      hpd_regions[[length(hpd_regions) + 1]] &lt;- c(start, end)\n      in_region &lt;- FALSE \n    }\n  }\n  \n  if (in_region) {\n    hpd_regions[[length(hpd_regions) + 1]] &lt;- c(start, dens$x[length(dens$x)])\n  }\n  \n  return(hpd_regions)\n}"
  },
  {
    "objectID": "code/gibbs_normal_model.html",
    "href": "code/gibbs_normal_model.html",
    "title": "Gibbs sampler: Normal Model",
    "section": "",
    "text": "We have data on the amount of time (in hours) a sample of students from a high school spent studying during a two-day reading period before exams. We will model the data as:\n\\[ \\begin{align*}\nY_{i} \\mid \\theta, \\sigma^2 &\\overset{iid}{\\sim} N(\\theta, \\sigma^2) \\qquad i = 1,\\ldots,n \\\\\n\\theta &\\sim N(\\mu_{0}, \\sigma_{0}^2) \\\\\n\\phi &\\sim \\text{Gamma}(a_{0}, b_{0})\n\\end{align*}\\]\nwhere \\(\\phi = \\frac{1}{\\sigma^2}\\). We have to fill out our priors for \\(\\theta\\) and \\(\\phi\\) to fully specify the statistical model before seeing the data. Let’s do that together!\n\nmu_0 &lt;- 5\ns2_0 &lt;- 4\na_0 &lt;- 5\nb_0 &lt;- 15\n\n# demonstrate how I might do prior elicitation\nhist(sqrt(1/(rgamma(10000, a_0, b_0))))"
  },
  {
    "objectID": "code/gibbs_normal_model.html#prior-specification",
    "href": "code/gibbs_normal_model.html#prior-specification",
    "title": "Gibbs sampler: Normal Model",
    "section": "",
    "text": "We have data on the amount of time (in hours) a sample of students from a high school spent studying during a two-day reading period before exams. We will model the data as:\n\\[ \\begin{align*}\nY_{i} \\mid \\theta, \\sigma^2 &\\overset{iid}{\\sim} N(\\theta, \\sigma^2) \\qquad i = 1,\\ldots,n \\\\\n\\theta &\\sim N(\\mu_{0}, \\sigma_{0}^2) \\\\\n\\phi &\\sim \\text{Gamma}(a_{0}, b_{0})\n\\end{align*}\\]\nwhere \\(\\phi = \\frac{1}{\\sigma^2}\\). We have to fill out our priors for \\(\\theta\\) and \\(\\phi\\) to fully specify the statistical model before seeing the data. Let’s do that together!\n\nmu_0 &lt;- 5\ns2_0 &lt;- 4\na_0 &lt;- 5\nb_0 &lt;- 15\n\n# demonstrate how I might do prior elicitation\nhist(sqrt(1/(rgamma(10000, a_0, b_0))))"
  },
  {
    "objectID": "code/gibbs_normal_model.html#data",
    "href": "code/gibbs_normal_model.html#data",
    "title": "Gibbs sampler: Normal Model",
    "section": "Data",
    "text": "Data\nNow we get to see the data:\n\ny &lt;- readRDS(\"~/Desktop/STAT 412/data/school1.Rda\")\ndata.frame(hours = y) |&gt;\n  ggplot(aes(x = hours)) +\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\nybar &lt;- mean(y)\nn &lt;- length(y)"
  },
  {
    "objectID": "code/gibbs_normal_model.html#gibbs-sampler",
    "href": "code/gibbs_normal_model.html#gibbs-sampler",
    "title": "Gibbs sampler: Normal Model",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\nset.seed(412)\n### GIBBS sampler\nS &lt;- 5000\n\n# will generate a bunch of samples, need to store them somewhere!\nTHETA &lt;- rep(NA, S)\nS2 &lt;- rep(NA, S)\n  \n# initialize the sampler \n# since sampling precision first, initialize value of theta\ntheta &lt;- 2\n\nfor (s in 1:S){\n  # sample phi from full conditional\n  b_n &lt;- 0.5*sum( (y - theta)^2) + b_0\n  a_n &lt;- 0.5*n + a_0\n  phi &lt;- rgamma(1, a_n, b_n)\n  \n  # convert phi to s2\n  s2 &lt;- 1/phi\n  \n  # sample theta from full conditional\n  s2_n &lt;- 1/(1/s2_0 + n / s2)\n  mu_n &lt;- s2_n * (n * ybar/s2 + mu_0/s2_0)\n  theta &lt;- rnorm(1, mu_n, sqrt(s2_n))\n\n  # store my samples\n  THETA[s] &lt;- theta\n  S2[s] &lt;- s2\n}\n\n\nSeeing the sampling"
  },
  {
    "objectID": "code/gibbs_normal_model.html#traceplots-and-posterior-quantities",
    "href": "code/gibbs_normal_model.html#traceplots-and-posterior-quantities",
    "title": "Gibbs sampler: Normal Model",
    "section": "Traceplots and posterior quantities",
    "text": "Traceplots and posterior quantities\nTrace plots of markov chains: line plot of simulated draws of parameter against iteration. In theory, the Metropolis and Gibbs sampling algorithms will produce simulated draws that converge to the posterior distribution of interest. But in typical practice, it may take a number of iterations before the simulation values are close to the posterior distribution. So in general it is recommended that one run the algorithm for a number of “burn-in” iterations before one collects iterations for inference.\nWe want nice “caterpillar” looking traceplots. This would suggest convergence!\n\nplot(THETA, xlab = \"Iteration\", ylab = \"theta\", type = \"l\", main = \"Traceplot\")\n\n\n\n\n\n\n\nplot(S2, xlab = \"Iteration\", ylab = \"sigma2\", type = \"l\", main = \"Traceplot\")\n\n\n\n\n\n\n\n\nThat crazy large sampled \\(\\sigma^2\\) at the beginning is a product of intialization and exploring the state space; we almost surely haven’t entered stationary distribution yet. Let’s go ahead and throw away half of our values as burn-in.\n\n# throw away some values as burn-in\nTHETA &lt;- THETA[-c(1:round(S/2))]\nS2 &lt;- S2[-c(1:round(S/2))]\n\nLet’s re-examine the traceplots post burn-in. From experience, these are beautiful looking traceplots!\n\npost_df &lt;- data.frame(theta = THETA, s2 = S2)\npost_df |&gt;\n  mutate(iteration = row_number()) |&gt;\n  pivot_longer(cols = 1:2, names_to = \"param\", values_to = \"value\") |&gt;\n  ggplot(aes(x = iteration, y = value)) +\n  geom_line() +\n  facet_wrap(~param, scales = \"free\") +\n  labs(\"Traceplots after burn-in\")\n\n\n\n\n\n\n\n\nWe should only estimate quantities after we’ve converged (i.e. after burn-in). In this case, we can obtain estimates of posterior quantities like usual:\n\n# estimated posterior mean of theta\nmean(THETA)\n\n[1] 8.967482\n\n# estimated posterior mean of sigma\nmean(sqrt(S2))\n\n[1] 3.493671\n\n## estimated CIs\nquantile(THETA, c(0.025, 0.975))\n\n     2.5%     97.5% \n 7.629652 10.265034 \n\nquantile(sqrt(S2), c(0.025, 0.975))\n\n    2.5%    97.5% \n2.787186 4.464605"
  },
  {
    "objectID": "code/gibbs_normal_model.html#diving-into-joint-density",
    "href": "code/gibbs_normal_model.html#diving-into-joint-density",
    "title": "Gibbs sampler: Normal Model",
    "section": "Diving into joint density",
    "text": "Diving into joint density\nWe can obtain a contour plot of the joint density using the samples obtained from Gibbs:\n\n\n\n\n\n\n\n\n\nWe can see how we “marginalize out” one parameter to get the posterior density of the other. To obtain marginals, we simply grab/isolate the parameter of interest.\n\n\n\n\n\n\n\n\n\n\nhist(THETA, main = \"Marginal posterior histogram\", xlab = \"theta\")\n\n\n\n\n\n\n\nhist(S2, main = \"Marginal posterior histogram\", xlab = \"s2\")"
  },
  {
    "objectID": "code/gibbs_normal_model.html#chains",
    "href": "code/gibbs_normal_model.html#chains",
    "title": "Gibbs sampler: Normal Model",
    "section": "Chains",
    "text": "Chains\nIn this example, we have illustrated running a single “chain” where one has a single starting value and we simulated draws over \\(S\\) iterations. It is possible that the behavior of the MCMC sample will depend on the choice of starting value. So a general recommendation is to run the MCMC algorithm several times using different starting values. In this case, one will have multiple MCMC chains and combine them.\nWhen we code samplers by hand in this course, we will almost always run a single chain to make our lives easier. However, when we turn to software that implements the MCMC for us, it’s very easy to specify multiple chains!"
  },
  {
    "objectID": "code/gibbs_normal_model_sampler_code.html",
    "href": "code/gibbs_normal_model_sampler_code.html",
    "title": "Gibbs Sampler: Normal Model",
    "section": "",
    "text": "set.seed(412)\n### GIBBS sampler\nS &lt;- 5000\n\n# will generate a bunch of samples, need to store them somewhere!\nTHETA &lt;- rep(NA, S)\nS2 &lt;- rep(NA, S)\n  \n# initialize the sampler \n# since sampling precision first, initialize value of theta\ntheta &lt;- 2\n\nfor (s in 1:S){\n  # sample phi from full conditional\n  b_n &lt;- 0.5*sum((y - theta)^2) + b_0\n  a_n &lt;- 0.5*n + a_0\n  phi &lt;- rgamma(1, a_n, b_n)\n  \n  # convert phi to s2\n  s2 &lt;- 1/phi\n  \n  # sample theta from full conditional\n  s2_n &lt;- 1/(1/s2_0 + n / s2)\n  mu_n &lt;- s2_n * (n * ybar/s2 + mu_0/s2_0)\n  theta &lt;- rnorm(1, mu_n, sqrt(s2_n))\n\n  # store my samples\n  THETA[s] &lt;- theta\n  S2[s] &lt;- s2\n}"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html",
    "href": "code/gibbs_normal_model_walkthrough.html",
    "title": "Gibbs sampler: Normal Model",
    "section": "",
    "text": "We have data on the amount of time (in hours) a sample of students from a high school spent studying during a two-day reading period before exams. We will model the data as:\n\\[ \\begin{align*}\nY_{i} \\mid \\theta, \\sigma^2 &\\overset{iid}{\\sim} N(\\theta, \\sigma^2) \\qquad i = 1,\\ldots,n \\\\\n\\theta &\\sim N(\\mu_{0}, \\sigma_{0}^2) \\\\\n\\phi &\\sim \\text{Gamma}(a_{0}, b_{0})\n\\end{align*}\\]\nwhere \\(\\phi = \\frac{1}{\\sigma^2}\\). We have to fill out our priors for \\(\\theta\\) and \\(\\phi\\) to fully specify the statistical model before seeing the data. Let’s do that together!\n\nmu_0 &lt;- 6\ns2_0 &lt;- 2.5\na_0 &lt;- 5\nb_0 &lt;- 100\n\n# demonstrate how I might do prior elicitation\n# hist(sqrt(1/(rgamma(10000, a_0, b_0))))"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html#prior-specification",
    "href": "code/gibbs_normal_model_walkthrough.html#prior-specification",
    "title": "Gibbs sampler: Normal Model",
    "section": "",
    "text": "We have data on the amount of time (in hours) a sample of students from a high school spent studying during a two-day reading period before exams. We will model the data as:\n\\[ \\begin{align*}\nY_{i} \\mid \\theta, \\sigma^2 &\\overset{iid}{\\sim} N(\\theta, \\sigma^2) \\qquad i = 1,\\ldots,n \\\\\n\\theta &\\sim N(\\mu_{0}, \\sigma_{0}^2) \\\\\n\\phi &\\sim \\text{Gamma}(a_{0}, b_{0})\n\\end{align*}\\]\nwhere \\(\\phi = \\frac{1}{\\sigma^2}\\). We have to fill out our priors for \\(\\theta\\) and \\(\\phi\\) to fully specify the statistical model before seeing the data. Let’s do that together!\n\nmu_0 &lt;- 6\ns2_0 &lt;- 2.5\na_0 &lt;- 5\nb_0 &lt;- 100\n\n# demonstrate how I might do prior elicitation\n# hist(sqrt(1/(rgamma(10000, a_0, b_0))))"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html#data",
    "href": "code/gibbs_normal_model_walkthrough.html#data",
    "title": "Gibbs sampler: Normal Model",
    "section": "Data",
    "text": "Data\nNow we get to see the data:\n\ny &lt;- readRDS(\"~/Desktop/STAT 412/data/school1.Rda\")\ndata.frame(hours = y) |&gt;\n  ggplot(aes(x = hours)) +\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\nybar &lt;- mean(y)\nn &lt;- length(y)"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html#gibbs-sampler",
    "href": "code/gibbs_normal_model_walkthrough.html#gibbs-sampler",
    "title": "Gibbs sampler: Normal Model",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\nset.seed(412)\n### GIBBS sampler\nS &lt;- 5000\n\n# will generate a bunch of samples, need to store them somewhere!\nTHETA &lt;- rep(NA, S)\nS2 &lt;- rep(NA, S)\n  \n# initialize the sampler \n# since sampling precision first, initialize value of theta\ntheta &lt;- 2\n\nfor (s in 1:S){\n  # sample phi from full conditional\n  b_n &lt;- 0.5*sum( (y - theta)^2) + b_0\n  a_n &lt;- 0.5*n + a_0\n  phi &lt;- rgamma(1, a_n, b_n)\n  \n  # convert phi to s2\n  s2 &lt;- 1/phi\n  \n  # sample theta from full conditional\n  s2_n &lt;- 1/(1/s2_0 + n / s2)\n  mu_n &lt;- s2_n * (n * ybar/s2 + mu_0/s2_0)\n  theta &lt;- rnorm(1, mu_n, sqrt(s2_n))\n\n  # store my samples\n  THETA[s] &lt;- theta\n  S2[s] &lt;- s2\n}\n\n\nSeeing the sampling"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html#traceplots-and-posterior-quantities",
    "href": "code/gibbs_normal_model_walkthrough.html#traceplots-and-posterior-quantities",
    "title": "Gibbs sampler: Normal Model",
    "section": "Traceplots and posterior quantities",
    "text": "Traceplots and posterior quantities\nTrace plots of markov chains: line plot of simulated draws of parameter against iteration. In theory, the Metropolis and Gibbs sampling algorithms will produce simulated draws that converge to the posterior distribution of interest. But in typical practice, it may take a number of iterations before the simulation values are close to the posterior distribution. So in general it is recommended that one run the algorithm for a number of “burn-in” iterations before one collects iterations for inference.\nWe want nice “caterpillar” looking traceplots. This would suggest convergence!\n\nplot(THETA, xlab = \"Iteration\", ylab = \"theta\", type = \"l\", main = \"Traceplot\")\n\n\n\n\n\n\n\nplot(S2, xlab = \"Iteration\", ylab = \"sigma2\", type = \"l\", main = \"Traceplot\")\n\n\n\n\n\n\n\n\nThat crazy large sampled \\(\\sigma^2\\) at the beginning is a product of intialization and exploring the state space; we almost surely haven’t entered stationary distribution yet. Let’s go ahead and throw away half of our values as burn-in.\n\n# throw away some values as burn-in\nTHETA &lt;- THETA[-c(1:round(S/2))]\nS2 &lt;- S2[-c(1:round(S/2))]\n\nLet’s re-examine the traceplots post burn-in. From experience, these are beautiful looking traceplots!\n\npost_df &lt;- data.frame(theta = THETA, s2 = S2)\npost_df |&gt;\n  mutate(iteration = row_number()) |&gt;\n  pivot_longer(cols = 1:2, names_to = \"param\", values_to = \"value\") |&gt;\n  ggplot(aes(x = iteration, y = value)) +\n  geom_line() +\n  facet_wrap(~param, scales = \"free\") +\n  labs(\"Traceplots after burn-in\")\n\n\n\n\n\n\n\n\nWe should only estimate quantities after we’ve converged (i.e. after burn-in). In this case, we can obtain estimates of posterior quantities like usual:\n\n# estimated posterior mean of theta\nmean(THETA)\n\n[1] 8.693283\n\n# estimated posterior mean of sigma\nmean(sqrt(S2))\n\n[1] 4.19726\n\n## estimated CIs\nquantile(THETA, c(0.025, 0.975))\n\n     2.5%     97.5% \n 7.177723 10.142940 \n\nquantile(sqrt(S2), c(0.025, 0.975))\n\n    2.5%    97.5% \n3.355402 5.368383"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html#diving-into-joint-density",
    "href": "code/gibbs_normal_model_walkthrough.html#diving-into-joint-density",
    "title": "Gibbs sampler: Normal Model",
    "section": "Diving into joint density",
    "text": "Diving into joint density\nWe can obtain a contour plot of the joint density using the samples obtained from Gibbs:\n\n\n\n\n\n\n\n\n\nWe can see how we “marginalize out” one parameter to get the posterior density of the other. To obtain marginals, we simply grab/isolate the parameter of interest.\n\n\n\n\n\n\n\n\n\n\nhist(THETA, main = \"Marginal posterior histogram\", xlab = \"theta\")\n\n\n\n\n\n\n\nhist(S2, main = \"Marginal posterior histogram\", xlab = \"s2\")"
  },
  {
    "objectID": "code/gibbs_normal_model_walkthrough.html#chains",
    "href": "code/gibbs_normal_model_walkthrough.html#chains",
    "title": "Gibbs sampler: Normal Model",
    "section": "Chains",
    "text": "Chains\nIn this example, we have illustrated running a single “chain” where one has a single starting value and we simulated draws over \\(S\\) iterations. It is possible that the behavior of the MCMC sample will depend on the choice of starting value. So a general recommendation is to run the MCMC algorithm several times using different starting values. In this case, one will have multiple MCMC chains and combine them.\nWhen we code samplers by hand in this course, we will almost always run a single chain to make our lives easier. However, when we turn to software that implements the MCMC for us, it’s very easy to specify multiple chains!"
  },
  {
    "objectID": "handouts/constrained_normal.html",
    "href": "handouts/constrained_normal.html",
    "title": "Constrained Normal Sampling Code",
    "section": "",
    "text": "# n = number of random samples \n# mu = mean of normal \n# sd = std. dev. normal \n# a = lower constraint (can be -Inf or a real number) \n# b = upper constraint (can be Inf or a real number)\n\nrnorm_constrained &lt;- function(n, mu, sd, a, b){\n  u &lt;- runif(n, pnorm(a, mu,sd), pnorm(b, mu, sd)) \n  qnorm(u, mu, sd) \n}"
  },
  {
    "objectID": "code/mixture_normals.html",
    "href": "code/mixture_normals.html",
    "title": "Mixture of Normals",
    "section": "",
    "text": "The density we’re interest in sampling from is \\[f(\\theta) = 0.45 N\\left(\\theta; -3, \\frac{1}{4}\\right) + 0.10 N\\left(\\theta; 0, \\frac{1}{4}\\right) +0.45 N\\left(\\theta; 3, \\frac{1}{4}\\right)\\] This is a mixutre of three Normals. You’ve seen your first mixture distribution in this class on the previous homework!\nWe can easily evaluate this distribution at a grid of \\(\\theta\\) values.\n\nw_prob &lt;- c(0.45, 0.1, 0.45)\nmu_vec &lt;- c(-3, 0, 3)\nsigma_vec &lt;- c(0.5, 0.5,0.5)\n\n# true density\ntheta_vec &lt;- seq(-10, 10, 0.01)\nmixture_dens &lt;- rep(NA, length(theta_vec))\nfor(i in 1:length(theta_vec)){\n  mixture_dens[i] &lt;- sum(dnorm(theta_vec[i], mu_vec, sigma_vec)*w_prob)\n}"
  },
  {
    "objectID": "code/mixture_normals.html#mixture-of-normals-density",
    "href": "code/mixture_normals.html#mixture-of-normals-density",
    "title": "Mixture of Normals",
    "section": "",
    "text": "The density we’re interest in sampling from is \\[f(\\theta) = 0.45 N\\left(\\theta; -3, \\frac{1}{4}\\right) + 0.10 N\\left(\\theta; 0, \\frac{1}{4}\\right) +0.45 N\\left(\\theta; 3, \\frac{1}{4}\\right)\\] This is a mixutre of three Normals. You’ve seen your first mixture distribution in this class on the previous homework!\nWe can easily evaluate this distribution at a grid of \\(\\theta\\) values.\n\nw_prob &lt;- c(0.45, 0.1, 0.45)\nmu_vec &lt;- c(-3, 0, 3)\nsigma_vec &lt;- c(0.5, 0.5,0.5)\n\n# true density\ntheta_vec &lt;- seq(-10, 10, 0.01)\nmixture_dens &lt;- rep(NA, length(theta_vec))\nfor(i in 1:length(theta_vec)){\n  mixture_dens[i] &lt;- sum(dnorm(theta_vec[i], mu_vec, sigma_vec)*w_prob)\n}"
  },
  {
    "objectID": "code/mixture_normals.html#monte-carlo-sampling",
    "href": "code/mixture_normals.html#monte-carlo-sampling",
    "title": "Mixture of Normals",
    "section": "Monte Carlo sampling",
    "text": "Monte Carlo sampling\nIf we want to sample from this mixture distribution, we can use Monte Calo sampling! For \\(s = 1,\\ldots, S\\):\n\nSample a value \\(w^{(s)} = \\{1,2,3\\}\\) according to mixture weights \\(\\{0.45, 0.10, 0.45\\}\\)\nGiven the value of \\(w\\), sample \\(\\theta^{(s)}\\) from the corresponding Normal. For example:\n\n\\[\\theta^{(s)} | w^{(s)} = 1 \\sim N\\left (-3, \\frac{1}{4}\\right)\\] Note this is indeed Monte Carlo sampling because we are drawing independent random sample!\nLet’s draw 1000 MC samples. A normalized histogram of the samples is shown below, along with the true density overlaid in orange:\n\nset.seed(1)\n# monte carlo\nS &lt;- 1000\nd_samp &lt;- sample(1:3, S, replace = T, prob = w_prob)\nhist(rnorm(S, mu_vec[d_samp], sigma_vec[d_samp]), freq = F, breaks =\n       20, main = \"Mixture of Normals\", xlab = \"theta\")\nlines(theta_vec, mixture_dens, type = \"l\", col = \"orange\")"
  },
  {
    "objectID": "code/mixture_normals.html#mcmc-sampling",
    "href": "code/mixture_normals.html#mcmc-sampling",
    "title": "Mixture of Normals",
    "section": "MCMC sampling",
    "text": "MCMC sampling\nWe can also obtain samples from this distribution via Gibbs sampling (i.e. Markov Chain Monte Carlo)! Note that even though we’re not working with a posterior, we can still perform Gibbs Sampling if we have more than one parameter of interest. In this case, why don’t I iterate between 1) sampling a \\(\\theta\\) value from its full conditional and 2) sampling a mixture component \\(w\\) value from its full conditional? That is, for \\(s =1,\\ldots, S\\):\n\nSample \\(w^{(s)} \\sim f(w | \\theta^{s-1})\\)\nSample \\(\\theta^{(s)} \\sim f(\\theta | w^{(s)})\\)\n\nConvince yourself that this sampling scheme is different from the previous!\nWe already have the distrution for sampling in step 2. So we just need to derive the full conditional for \\(w\\)! Let’s do that together.\n\nGibbs sampler\nLet’s run our sampler for \\(S=5000\\) iterations, then take a look at the traceplot and marginal histogram for \\(\\theta\\). Note: even though we’re sampling \\(w\\)’s along the way, I don’t actually care about them (they are latent variables). So I don’t bother storing them!\n\nseed &lt;- 10\nS &lt;- 5000\nset.seed(seed)\ntheta &lt;- -2\nTHETA &lt;- rep(NA, S)\nfor(s in 1:S){\n  # sample w\n  prob_vec_unnorm &lt;- dnorm(theta, mu_vec, sigma_vec) * w_prob\n  prob_vec_norm &lt;- prob_vec_unnorm/sum(prob_vec_unnorm)\n  w &lt;- sample(1:3, size = 1, prob = prob_vec_norm)\n  \n  # sample theta\n  theta &lt;- rnorm(1, mu_vec[w], sigma_vec[w])\n  THETA[s] &lt;- theta\n}\npar(mfrow = c(1, 2))\nplot(THETA, type = \"l\", main = \"Traceplot\", xlab = \"iteration\")\nhist(THETA, freq = F, main = \"\", xlab = expression(theta))\n\n\n\n\n\n\n\n\nWhat are we noticing?\nWell, we did say that we should throw out the first chunk of iterations to burn-in. So let me throw out the first half and re-visualize:\n\npar(mfrow = c(1, 2))\nTHETA_burned &lt;- THETA[-c(1:(S/2))]\nplot(THETA_burned, type = \"l\",  main = \"Traceplot\", xlab = \"iteration (after burn-in)\")\nhist(THETA_burned, freq = F, main = \"\", xlab = expression(theta))\n\n\n\n\n\n\n\n\nYikes!! That sure is misleading!!!! After burn-in, it seems like my density has converged to a unimodal distribution. What’s happening here?\nLet’s talk about convergence, mixing, and autocorrelation!\n\n\nGibbs, take 2\nLet’s now run the chain a lot longer, this time for \\(S = 20000\\) iterations.\n\n\n\n\n\n\n\n\n\nThat’s much better! Of course, in real life, we don’t know what the desire (posterior) density should look like. But this goes to show that you should, if computationally feasible, run your chain for a long time and perhaps for many different \\(S\\) values."
  },
  {
    "objectID": "code/mixture_normals.html#functions-for-mcmc-diagnostics",
    "href": "code/mixture_normals.html#functions-for-mcmc-diagnostics",
    "title": "Mixture of Normals",
    "section": "Functions for MCMC diagnostics",
    "text": "Functions for MCMC diagnostics\n\nAutocorrelation\nThe R function for examining autocorrelation is acf(), and you pass in a vector of samples:\n\nacf(THETA_burned)\n\n\n\n\n\n\n\n\nThe horizontal blue linee give the values beyond which the autocorrelations are (statistically) significantly different from 0. So you’d like ACF values within the blue bands.\nIf you just want the sample lag \\(t\\) ACF value, you can access it as follows:\n\nacf_out &lt;- acf(THETA_burned, plot = T)\n\n\n\n\n\n\n\n# lag-0 is at index 1\n# lag-1 is at index 2\nacf_out$acf[2]\n\n[1] 0.9685008\n\n\n\n\nEffective sample size\nWe can obtain \\(n_{eff}\\) using the effectiveSize() function from the R package coda (install it first).\n\n# yikes\ncoda::effectiveSize(THETA_burned) \n\n    var1 \n6.235923"
  },
  {
    "objectID": "case_study.html",
    "href": "case_study.html",
    "title": "Case studies",
    "section": "",
    "text": "Your first case study will be performed in pairs! You should work together; don’t just divy up the work. A large part of modeling is bouncing ideas back and forth and checking in to make sure things “make sense”. Each group will submit 1 report in the form of both a .Rmd and a knitted/rendered PDF. These will be submitted to Canvas (not in-person) by beginning of class 10/15.\nThe description of the case study and the data are found here:\n\nDescription\nData:  ItalyMarriageRates \nOptional .Rmd template:  caseStudy1 \n\nAdditionally, each group will give a brief 5 minute presentation of their work to the class on 10/15. Your presentation should be accompanied by a brief set of slides. These slides should focus mostly on describing your statistical model and how you arrived at it, as well as results (with interpretation). You should be prepared to answer questions from the audience."
  },
  {
    "objectID": "case_study.html#case-study-1",
    "href": "case_study.html#case-study-1",
    "title": "Case studies",
    "section": "",
    "text": "Your first case study will be performed in pairs! You should work together; don’t just divy up the work. A large part of modeling is bouncing ideas back and forth and checking in to make sure things “make sense”. Each group will submit 1 report in the form of both a .Rmd and a knitted/rendered PDF. These will be submitted to Canvas (not in-person) by beginning of class 10/15.\nThe description of the case study and the data are found here:\n\nDescription\nData:  ItalyMarriageRates \nOptional .Rmd template:  caseStudy1 \n\nAdditionally, each group will give a brief 5 minute presentation of their work to the class on 10/15. Your presentation should be accompanied by a brief set of slides. These slides should focus mostly on describing your statistical model and how you arrived at it, as well as results (with interpretation). You should be prepared to answer questions from the audience."
  },
  {
    "objectID": "case_study/case_study_1_template.html",
    "href": "case_study/case_study_1_template.html",
    "title": "STAT 412: Case Study 1",
    "section": "",
    "text": "DELETE ALL THE TEXT PROVIDED BY BECKY BEFORE SUBMITTING!\n# if you need packages, load them here\n\n# the following line of code sets the size of the figures when knitting for all code chunks. \n# You can also change the figure size in each specific chunk's header\nknitr::opts_chunk$set(fig.height = 3, fig.width = 5)\n\n# load your data here\n# Once you've set your file path, change the R chunk header to eval = TRUE\nmarriage &lt;- read_csv()"
  },
  {
    "objectID": "case_study/case_study_1_template.html#model-description",
    "href": "case_study/case_study_1_template.html#model-description",
    "title": "STAT 412: Case Study 1",
    "section": "Model description",
    "text": "Model description\nClearly state your model (Address questions 1 and 2). I encourage you to type up your model in math notation using Latex. For example:\n\\(Y_{1}, \\ldots, Y_{n} |\\theta \\overset{iid}{\\sim} f(y | \\theta)\\)\nIf you do not know how, that’s okay! Just let me know and I will be happy to help or point you towards some resources."
  },
  {
    "objectID": "case_study/case_study_1_template.html#implementation",
    "href": "case_study/case_study_1_template.html#implementation",
    "title": "STAT 412: Case Study 1",
    "section": "Implementation",
    "text": "Implementation\nClearly state how you will implement your model. (Address question 3.) You should provide enough detail such that someone else could implement your analysis."
  },
  {
    "objectID": "case_study/case_study_1_template.html#gibbs-sampler",
    "href": "case_study/case_study_1_template.html#gibbs-sampler",
    "title": "STAT 412: Case Study 1",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\n# code for your gibbs sampler should go here\n# remember to only keep necessary code for submission!"
  },
  {
    "objectID": "case_study/case_study_1_template.html#results",
    "href": "case_study/case_study_1_template.html#results",
    "title": "STAT 412: Case Study 1",
    "section": "Results",
    "text": "Results\n\nDiagnostics\nFirst do some diagnostics to see if your chain has converged\n\n\nFindings\nProvide results with interpretation (Address question 4). You should alternate between R code and text!"
  },
  {
    "objectID": "case_study/case_study_1_template.html#conclusion",
    "href": "case_study/case_study_1_template.html#conclusion",
    "title": "STAT 412: Case Study 1",
    "section": "Conclusion",
    "text": "Conclusion\nGive a conclusion that answers the research question (Address question 5)."
  },
  {
    "objectID": "handouts/jeffreys_prior_normal.html",
    "href": "handouts/jeffreys_prior_normal.html",
    "title": "Jeffreys Prior",
    "section": "",
    "text": "Suppose I use Jeffrey’s prior for \\(\\theta\\), but I don’t use Jeffrey’s prior for \\(\\gamma = e^{\\theta}\\). Then if I obtain the posterior for \\(\\theta\\) and transform that posterior to \\(\\gamma\\) space, I’m not guaranteed to get the same posterior as if I’d started in \\(\\gamma\\)-space to begin with!\n\nlibrary(pracma)\n\n# Data: single observation\ny &lt;- 2.5\nsigma &lt;- 1\n\n# Grid in mu-space\nmu_vec &lt;- seq(-5, 5, length.out = 10000)\n\n# Likelihood in mu\nlik_mu &lt;- dnorm(y, mean = mu_vec, sd = sigma)\n\n# Prior 1: Jeffreys prior on mu: uniform (constant)\nprior_jeff_mu &lt;- rep(1, length(mu_vec))\n\n# Posterior in mu under Jeffreys prior for mu\npost_mu_jeff &lt;- lik_mu * prior_jeff_mu\npost_mu_jeff &lt;- post_mu_jeff / trapz(mu_vec, post_mu_jeff)\n\n# Transform parameter: gamma = exp(mu)\ngamma_vec &lt;- exp(mu_vec)\ndmu_dgamma &lt;- 1 / gamma_vec  # derivative of mu w.r.t gamma\n\n# Transform posterior to gamma-space\npost_gamma_from_mu &lt;- post_mu_jeff / dmu_dgamma  # divide by |dmu/dgamma| for pdf transform\npost_gamma_from_mu &lt;- post_gamma_from_mu / trapz(gamma_vec, post_gamma_from_mu)\n\n# flat prior on gamma (this is NOT jeffrey's prior for gamma)\nprior_flat_gamma &lt;- rep(1, length(gamma_vec))\n\n# Likelihood in gamma-space (transform mu to gamma)\nlik_gamma &lt;- dnorm(y, mean = log(gamma_vec), sd = sigma)\n\n\n# Posterior in gamma-space under flat prior for gamma\npost_gamma &lt;- lik_gamma * prior_flat_gamma\npost_gamma &lt;- post_gamma / trapz(gamma_vec, post_gamma)\n\n# Sampling function from mu posterior\ncdf_mu_jeff &lt;- cumtrapz(mu_vec, post_mu_jeff)\ncdf_mu_jeff &lt;- cdf_mu_jeff / max(cdf_mu_jeff)\nsample_mu &lt;- function(n) {\n  u &lt;- runif(n)\n  approx(cdf_mu_jeff, mu_vec, xout = u, rule = 2)$y\n}\nmu_samps_jeff &lt;- sample_mu(50000)\n# induced gamma posterior from jeffrey's prior on mu\ngamma_samps_from_mu_jeff &lt;- exp(mu_samps_jeff)\n\n# Sampling function from gamma posterior\ncdf_gamma &lt;- cumtrapz(gamma_vec, post_gamma)\ncdf_gamma &lt;- cdf_gamma / max(cdf_gamma)\nsample_gamma &lt;- function(n) {\n  u &lt;- runif(n)\n  approx(cdf_gamma, gamma_vec, xout = u, rule = 2)$y\n}\n# gamma posterior with flat prior on gamma starting in gamma space\ngamma_samps &lt;- sample_gamma(50000)\n\n\n\n\n\n\n\n\n\n\nNow what happens if I use Jeffrey’s prior for \\(\\theta\\) and also Jeffrey’s prior for \\(\\gamma\\)? The posteriors in \\(\\gamma\\)-space ought to look the same!\n\n# Jeffreys prior on gamma is proportional to 1/gamma\nprior_jeff_gamma &lt;- 1 / gamma_vec\n\n# Posterior in gamma under jeffreys prior\npost_gamma_jeff &lt;- lik_gamma * prior_jeff_gamma\npost_gamma_jeff &lt;- post_gamma_jeff / trapz(gamma_vec, post_gamma_jeff)\n\n# Sampling functions using inverse CDF method\ncdf_gamma_jeff &lt;- cumtrapz(gamma_vec, post_gamma_jeff)\ncdf_gamma_jeff &lt;- cdf_gamma_jeff / max(cdf_gamma_jeff)\nsample_gamma_jeff &lt;- function(n) {\n  u &lt;- runif(n)\n  approx(cdf_gamma_jeff, gamma_vec, xout = u, rule = 2)$y\n}\n# gamma posterior with Jeffreys prior on gamma starting in gamma space\ngamma_samps_jeff &lt;- sample_gamma_jeff(50000)"
  }
]